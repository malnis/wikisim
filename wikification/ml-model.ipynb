{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model-create.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model-create.py \n",
    "\n",
    "\"\"\"\n",
    "Train model and everything here in a script because ssh and jupyter are failing me.\n",
    "\"\"\"\n",
    "\n",
    "allX = []\n",
    "allY = []\n",
    "allMId = []\n",
    "\n",
    "trainX = []\n",
    "trainY = []\n",
    "trainMId = []\n",
    "valiX = []\n",
    "valiY = []\n",
    "valiMId = []\n",
    "testX = []\n",
    "testY = []\n",
    "testMId = []\n",
    "\n",
    "linesToUse = 1000000 # limit amount of total data\n",
    "totalLines = 0\n",
    "# first try with just getting all data\n",
    "with open('/users/cs/amaral/wikisim/wikification/learning-data/el-5000-hybridgen.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        totalLines += 1\n",
    "        if totalLines > linesToUse:\n",
    "            break\n",
    "        data = line.split(',')\n",
    "        allX.append([float(data[2]), float(data[3]), float(data[4]), float(data[5]), float(data[6])])\n",
    "        allY.append(int(data[1]))\n",
    "        allMId.append(long(data[7]))\n",
    "        \n",
    "# split 75, 25\n",
    "trainLines = int(totalLines * 0.75)\n",
    "valiLines = int(totalLines * 0.0)\n",
    "testLines = int(totalLines * 0.25)\n",
    "\n",
    "for i in range(0, trainLines):\n",
    "    trainX.append(allX[i])\n",
    "    trainY.append(allY[i])\n",
    "    trainMId.append(allMId[i])\n",
    "\n",
    "for i in range(trainLines, trainLines + valiLines):\n",
    "    valiX.append(allX[i])\n",
    "    valiY.append(allY[i])\n",
    "    valiMId.append(allMId[i])\n",
    "    \n",
    "for i in range(trainLines + valiLines, trainLines + valiLines + testLines):\n",
    "    testX.append(allX[i])\n",
    "    testY.append(allY[i])\n",
    "    testMId.append(allMId[i])\n",
    "    \n",
    "print len(trainX)\n",
    "print len(testX)\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import sys\n",
    "sys.path.append('./pyltr/')\n",
    "import pyltr\n",
    "\n",
    "etr = ExtraTreesRegressor(n_estimators=200, min_samples_split=5, random_state=1, n_jobs=-1)\n",
    "etr.fit(trainX, trainY)\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators=200, min_samples_split=5, random_state=1, n_jobs=-1)\n",
    "rfr.fit(trainX, trainY)\n",
    "\n",
    "gbr = GradientBoostingRegressor(n_estimators=300, max_depth=3, learning_rate=0.1, loss='ls', random_state=1)\n",
    "gbr.fit(trainX, trainY)\n",
    "\n",
    "lmart = pyltr.models.LambdaMART(n_estimators=300, learning_rate=0.1, verbose = 1)\n",
    "lmart.fit(trainX, trainY, trainMId)\n",
    "\n",
    "\"\"\"\n",
    "Save the model.\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "\n",
    "pickle.dump(etr, open('/users/cs/amaral/wikisim/wikification/ml-models/model-etr-2.pkl', 'wb'))\n",
    "pickle.dump(rfr, open('/users/cs/amaral/wikisim/wikification/ml-models/model-rfr-2.pkl', 'wb'))\n",
    "pickle.dump(gbr, open('/users/cs/amaral/wikisim/wikification/ml-models/model-gbr-2.pkl', 'wb'))\n",
    "pickle.dump(lmart, open('/users/cs/amaral/wikisim/wikification/ml-models/model-lmart-2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 170097\n",
      "56699\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell is to get all the data for the ml model\n",
    "\"\"\"\n",
    "\n",
    "allX = []\n",
    "allY = []\n",
    "allMId = []\n",
    "\n",
    "trainX = []\n",
    "trainY = []\n",
    "trainMId = []\n",
    "valiX = []\n",
    "valiY = []\n",
    "valiMId = []\n",
    "testX = []\n",
    "testY = []\n",
    "testMId = []\n",
    "\n",
    "linesToUse = 1000000 # limit amount of total data\n",
    "totalLines = 0\n",
    "# first try with just getting all data\n",
    "with open('/users/cs/amaral/wikisim/wikification/learning-data/el-5000.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        totalLines += 1\n",
    "        if totalLines > linesToUse:\n",
    "            break\n",
    "        data = line.split(',')\n",
    "        allX.append([float(data[2]), float(data[3]), float(data[4]), float(data[5]), float(data[6])])\n",
    "        allY.append(int(data[1]))\n",
    "        allMId.append(long(data[7]))\n",
    "        \n",
    "# split 75, 25\n",
    "trainLines = int(totalLines * 0.75)\n",
    "valiLines = int(totalLines * 0.0)\n",
    "testLines = int(totalLines * 0.25)\n",
    "\n",
    "for i in range(0, trainLines):\n",
    "    trainX.append(allX[i])\n",
    "    trainY.append(allY[i])\n",
    "    trainMId.append(allMId[i])\n",
    "\n",
    "for i in range(trainLines, trainLines + valiLines):\n",
    "    valiX.append(allX[i])\n",
    "    valiY.append(allY[i])\n",
    "    valiMId.append(allMId[i])\n",
    "    \n",
    "for i in range(trainLines + valiLines, trainLines + valiLines + testLines):\n",
    "    testX.append(allX[i])\n",
    "    testY.append(allY[i])\n",
    "    testMId.append(allMId[i])\n",
    "    \n",
    "print len(trainX)\n",
    "print len(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter  Train score    Remaining                           Monitor Output \n",
      "    1       0.8987       88.16m                                         \n",
      "    2       0.9178       86.97m                                         \n",
      "    3       0.9288       86.27m                                         \n",
      "    4       0.9290       85.70m                                         \n",
      "    5       0.9356       85.35m                                         \n",
      "    6       0.9362       84.96m                                         \n",
      "    7       0.9387       84.65m                                         \n",
      "    8       0.9411       84.34m                                         \n",
      "    9       0.9412       84.02m                                         \n",
      "   10       0.9414       83.73m                                         \n",
      "   15       0.9439       82.20m                                         \n",
      "   20       0.9479       80.73m                                         \n",
      "   25       0.9519       79.28m                                         \n",
      "   30       0.9546       77.82m                                         \n",
      "   35       0.9556       76.38m                                         \n",
      "   40       0.9571       74.97m                                         \n",
      "   45       0.9580       73.55m                                         \n",
      "   50       0.9585       72.14m                                         \n"
     ]
    }
   ],
   "source": [
    "\"\"\" This and other cells helped by: https://github.com/ogrisel/notebooks/blob/master/Learning%20to%20Rank.ipynb\n",
    "This cell is to train the model.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import sys\n",
    "sys.path.append('./pyltr/')\n",
    "import pyltr\n",
    "\n",
    "#etr = ExtraTreesRegressor(n_estimators=200, min_samples_split=5, random_state=1, n_jobs=-1)\n",
    "#etr.fit(trainX, trainY)\n",
    "\n",
    "#rfr = RandomForestRegressor(n_estimators=200, min_samples_split=5, random_state=1, n_jobs=-1)\n",
    "#rfr.fit(trainX, trainY)\n",
    "\n",
    "#gbr = GradientBoostingRegressor(n_estimators=300, max_depth=3, learning_rate=0.1, loss='ls', random_state=1)\n",
    "#gbr.fit(trainX, trainY)\n",
    "\n",
    "#gbc = GradientBoostingClassifier(n_estimators=200, min_samples_split=5, random_state=1)\n",
    "#gbc.fit(trainX, trainY)\n",
    "\n",
    "lmart = pyltr.models.LambdaMART(n_estimators=300, learning_rate=0.1, verbose = 1)\n",
    "lmart.fit(trainX, trainY, trainMId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees Regressor:\n",
      "R^2 Score: 0.843993888891\n",
      "\n",
      "BDB =  0.0 \n",
      "TP: 6134 0.993199481865 \n",
      "FP: 8991 0.17795855353 \n",
      "TN: 41532 0.82204144647 \n",
      "FN: 42 0.00680051813472\n",
      "\n",
      "BDB =  0.1 \n",
      "TP: 5993 0.970369170984 \n",
      "FP: 1909 0.0377847712923 \n",
      "TN: 48614 0.962215228708 \n",
      "FN: 183 0.0296308290155\n",
      "\n",
      "BDB =  0.2 \n",
      "TP: 5897 0.954825129534 \n",
      "FP: 1204 0.0238307305584 \n",
      "TN: 49319 0.976169269442 \n",
      "FN: 279 0.0451748704663\n",
      "\n",
      "BDB =  0.3 \n",
      "TP: 5788 0.937176165803 \n",
      "FP: 878 0.0173782237793 \n",
      "TN: 49645 0.982621776221 \n",
      "FN: 388 0.0628238341969\n",
      "\n",
      "BDB =  0.4 \n",
      "TP: 5679 0.919527202073 \n",
      "FP: 667 0.0132019080419 \n",
      "TN: 49856 0.986798091958 \n",
      "FN: 497 0.0804727979275\n",
      "\n",
      "BDB =  0.5 \n",
      "TP: 5533 0.895887305699 \n",
      "FP: 496 0.0098173109277 \n",
      "TN: 50027 0.990182689072 \n",
      "FN: 643 0.104112694301\n",
      "\n",
      "BDB =  0.6 \n",
      "TP: 5372 0.86981865285 \n",
      "FP: 374 0.00740256912693 \n",
      "TN: 50149 0.992597430873 \n",
      "FN: 804 0.13018134715\n",
      "\n",
      "BDB =  0.7 \n",
      "TP: 5177 0.838244818653 \n",
      "FP: 270 0.00534410070661 \n",
      "TN: 50253 0.994655899293 \n",
      "FN: 999 0.161755181347\n",
      "\n",
      "BDB =  0.8 \n",
      "TP: 4894 0.792422279793 \n",
      "FP: 174 0.00344397601093 \n",
      "TN: 50349 0.996556023989 \n",
      "FN: 1282 0.207577720207\n",
      "\n",
      "BDB =  0.9 \n",
      "TP: 4462 0.722474093264 \n",
      "FP: 91 0.00180115986778 \n",
      "TN: 50432 0.998198840132 \n",
      "FN: 1714 0.277525906736\n",
      "\n",
      "\n",
      "Random Forest Regressor:\n",
      "R^2 Score: 0.84456202897\n",
      "\n",
      "BDB =  0.0 \n",
      "TP: 6122 0.991256476684 \n",
      "FP: 6381 0.126298913366 \n",
      "TN: 44142 0.873701086634 \n",
      "FN: 54 0.00874352331606\n",
      "\n",
      "BDB =  0.1 \n",
      "TP: 5994 0.970531088083 \n",
      "FP: 1866 0.0369336737723 \n",
      "TN: 48657 0.963066326228 \n",
      "FN: 182 0.0294689119171\n",
      "\n",
      "BDB =  0.2 \n",
      "TP: 5895 0.954501295337 \n",
      "FP: 1227 0.0242859687667 \n",
      "TN: 49296 0.975714031233 \n",
      "FN: 281 0.0454987046632\n",
      "\n",
      "BDB =  0.3 \n",
      "TP: 5796 0.938471502591 \n",
      "FP: 883 0.0174771886072 \n",
      "TN: 49640 0.982522811393 \n",
      "FN: 380 0.0615284974093\n",
      "\n",
      "BDB =  0.4 \n",
      "TP: 5683 0.920174870466 \n",
      "FP: 659 0.0130435643172 \n",
      "TN: 49864 0.986956435683 \n",
      "FN: 493 0.0798251295337\n",
      "\n",
      "BDB =  0.5 \n",
      "TP: 5540 0.897020725389 \n",
      "FP: 503 0.00995586168676 \n",
      "TN: 50020 0.990044138313 \n",
      "FN: 636 0.102979274611\n",
      "\n",
      "BDB =  0.6 \n",
      "TP: 5393 0.873218911917 \n",
      "FP: 381 0.00754111988599 \n",
      "TN: 50142 0.992458880114 \n",
      "FN: 783 0.126781088083\n",
      "\n",
      "BDB =  0.7 \n",
      "TP: 5185 0.83954015544 \n",
      "FP: 271 0.00536389367219 \n",
      "TN: 50252 0.994636106328 \n",
      "FN: 991 0.16045984456\n",
      "\n",
      "BDB =  0.8 \n",
      "TP: 4902 0.79371761658 \n",
      "FP: 166 0.00328563228629 \n",
      "TN: 50357 0.996714367714 \n",
      "FN: 1274 0.20628238342\n",
      "\n",
      "BDB =  0.9 \n",
      "TP: 4490 0.727007772021 \n",
      "FP: 101 0.00199908952358 \n",
      "TN: 50422 0.998000910476 \n",
      "FN: 1686 0.272992227979\n",
      "\n",
      "\n",
      "Gradient Boosting Regressor:\n",
      "R^2 Score: 0.848231559571\n",
      "\n",
      "BDB =  0.0 \n",
      "TP: 6157 0.99692357513 \n",
      "FP: 26752 0.529501415197 \n",
      "TN: 23771 0.470498584803 \n",
      "FN: 19 0.00307642487047\n",
      "\n",
      "BDB =  0.1 \n",
      "TP: 6012 0.973445595855 \n",
      "FP: 1951 0.0386160758466 \n",
      "TN: 48572 0.961383924153 \n",
      "FN: 164 0.0265544041451\n",
      "\n",
      "BDB =  0.2 \n",
      "TP: 5894 0.954339378238 \n",
      "FP: 1210 0.0239494883518 \n",
      "TN: 49313 0.976050511648 \n",
      "FN: 282 0.0456606217617\n",
      "\n",
      "BDB =  0.3 \n",
      "TP: 5794 0.938147668394 \n",
      "FP: 864 0.0171011222611 \n",
      "TN: 49659 0.982898877739 \n",
      "FN: 382 0.0618523316062\n",
      "\n",
      "BDB =  0.4 \n",
      "TP: 5679 0.919527202073 \n",
      "FP: 629 0.0124497753498 \n",
      "TN: 49894 0.98755022465 \n",
      "FN: 497 0.0804727979275\n",
      "\n",
      "BDB =  0.5 \n",
      "TP: 5551 0.898801813472 \n",
      "FP: 487 0.00963917423748 \n",
      "TN: 50036 0.990360825763 \n",
      "FN: 625 0.101198186528\n",
      "\n",
      "BDB =  0.6 \n",
      "TP: 5412 0.876295336788 \n",
      "FP: 356 0.00704629574649 \n",
      "TN: 50167 0.992953704254 \n",
      "FN: 764 0.123704663212\n",
      "\n",
      "BDB =  0.7 \n",
      "TP: 5201 0.842130829016 \n",
      "FP: 253 0.00500762029175 \n",
      "TN: 50270 0.994992379708 \n",
      "FN: 975 0.157869170984\n",
      "\n",
      "BDB =  0.8 \n",
      "TP: 4882 0.790479274611 \n",
      "FP: 144 0.00285018704352 \n",
      "TN: 50379 0.997149812956 \n",
      "FN: 1294 0.209520725389\n",
      "\n",
      "BDB =  0.9 \n",
      "TP: 4279 0.692843264249 \n",
      "FP: 73 0.00144488648734 \n",
      "TN: 50450 0.998555113513 \n",
      "FN: 1897 0.307156735751\n",
      "\n",
      "\n",
      "Gradient Boosting Classifier:\n",
      "R^2 Score: 0.980017284255\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell tells the accuracy of the model.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "model = etr\n",
    "print 'Extra Trees Regressor:'\n",
    "print 'R^2 Score:', model.score(testX, testY)\n",
    "print\n",
    "for i in np.arange(0.0, 1.0, 0.1):\n",
    "    printEval(model, testX, testY, i)\n",
    "    print\n",
    "print\n",
    "\n",
    "model = rfr\n",
    "print 'Random Forest Regressor:'\n",
    "print 'R^2 Score:', model.score(testX, testY)\n",
    "print\n",
    "for i in np.arange(0.0, 1.0, 0.1):\n",
    "    printEval(model, testX, testY, i)\n",
    "    print\n",
    "print\n",
    "\n",
    "model = gbr\n",
    "print 'Gradient Boosting Regressor:'\n",
    "print 'R^2 Score:', model.score(testX, testY)\n",
    "print\n",
    "for i in np.arange(0.0, 1.0, 0.1):\n",
    "    printEval(model, testX, testY, i)\n",
    "    print\n",
    "print\n",
    "\n",
    "model = gbc\n",
    "print 'Gradient Boosting Classifier:'\n",
    "print 'R^2 Score:', model.score(testX, testY)\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "def printEval(model, X, y, bdb = 0.5):\n",
    "    predY = model.predict(X)\n",
    "    \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        if predY[i] > bdb and y[i] == 1:\n",
    "            tp += 1\n",
    "        elif predY[i] > bdb and y[i] == 0:\n",
    "            fp += 1\n",
    "        elif predY[i] <= bdb and y[i] == 1:\n",
    "            fn += 1\n",
    "        elif predY[i] <= bdb and y[i] == 0:\n",
    "            tn += 1\n",
    "            \n",
    "    print 'BDB = ', bdb, '\\nTP:', tp, tp/(tp+fn), '\\nFP:', fp, fp/(fp+tn), '\\nTN:', tn, tn/(fp+tn), '\\nFN:', fn, fn/(tp+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save the model.\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "\n",
    "#pickle.dump(etr, open('/users/cs/amaral/wikisim/wikification/ml-models/model-etr-1.pkl', 'wb'))\n",
    "#pickle.dump(rfr, open('/users/cs/amaral/wikisim/wikification/ml-models/model-rfr-1.pkl', 'wb'))\n",
    "#pickle.dump(gbr, open('/users/cs/amaral/wikisim/wikification/ml-models/model-gbr-1.pkl', 'wb'))\n",
    "#pickle.dump(gbc, open('/users/cs/amaral/wikisim/wikification/ml-models/model-gbc-1.pkl', 'wb'))\n",
    "#pickle.dump(lmart, open('/users/cs/amaral/wikisim/wikification/ml-models/model-lmart-1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.38906408, -0.38142295,  0.14399559, -1.21259834,  6.16869182,\n",
       "        1.39566366,  6.07528854,  5.27429442, -5.77873421, -3.68008578,\n",
       "       -7.37840903, -6.02215263, -4.83235341, -5.99034009, -4.64082089,\n",
       "       -6.54484845, -3.38575076, -5.05438867])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import sys\n",
    "sys.path.append('./pyltr/')\n",
    "import pyltr\n",
    "\n",
    "model = pickle.load(open('/users/cs/amaral/wikisim/wikification/ml-models/model-lmart-1.pkl', 'rb'))\n",
    "\n",
    "model.predict(testX[2:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16, 0.37197801830625504, 0.75, 0.0, 0.8058462601912664], [0.12, 0.12183860865881956, 0.0, 0.0, 0.003000814788764128], [0.08, 0.15388780453451467, 0.0, 0.0, 0.0197314194384961], [0.04, 0.12183860865881956, 0.0, 0.0, 0.00597341299761156], [0.6666666666666666, 0.5524107765948493, 0.75, 0.0, 0.6424277609623221], [0.2222222222222222, 0.43160739972500395, 0.0, 0.0, 0.04122347126384451], [0.9875, 0.9595134734099676, 0.9090909090909091, 0.25918609576466367, 0.9999999999999998], [0.9585703450891164, 0.1346076964691621, 0.9090909090909091, 0.265528050509531, 0.6452690222713563], [0.010586525091644546, 0.0, 0.0, 0.13257166946600463, 0.0008528990630150002], [0.006415118189862217, 0.07584652897571567, 0.0, 0.18615200157548295, 0.021349869684028633], [0.004361016306408798, 0.0, 0.0, 0.13820647183391077, 0.0], [0.0034445708507141954, 0.0, 0.0, 0.2197684333582972, 0.001374533158081892], [0.002844141069397042, 0.06692287344969088, 0.0, 0.1855829880405545, 0.0038656172984994353], [0.0027177347996460623, 0.0, 0.0, 0.12775189841427181, 0.0008678591415569592], [0.002085703450891164, 0.0741983070523882, 0.0, 0.1666364333953726, 0.014134646571460907], [0.0015800783718872456, 0.0, 0.0, 0.13560885982084758, 0.0010101363516014095], [0.0011692579951965618, 0.13443111220948226, 0.0, 0.1846330840700947, 0.07855439768226724], [0.0009796485905700922, 0.0992152278375584, 0.0, 0.20990917625900218, 0.03458034001283328]]\n",
      "\n",
      "[1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print testX[2:20]\n",
    "print\n",
    "print testY[2:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
