{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ryan Amaral\n",
      "Dalhousie University\n",
      "Halifax\n",
      "Nova Scotia\n",
      "[{u'tokenBegin': 0, u'tokenEnd': 2, u'docTokenBegin': 0, u'characterOffsetEnd': 11, u'docTokenEnd': 2, u'text': u'Ryan Amaral', u'characterOffsetBegin': 0, u'ner': u'PERSON'}, {u'tokenBegin': 6, u'tokenEnd': 8, u'docTokenBegin': 6, u'characterOffsetEnd': 48, u'docTokenEnd': 8, u'text': u'Dalhousie University', u'characterOffsetBegin': 28, u'ner': u'ORGANIZATION'}, {u'tokenBegin': 9, u'tokenEnd': 10, u'docTokenBegin': 9, u'characterOffsetEnd': 59, u'docTokenEnd': 10, u'text': u'Halifax', u'characterOffsetBegin': 52, u'ner': u'LOCATION'}, {u'tokenBegin': 11, u'tokenEnd': 13, u'docTokenBegin': 11, u'characterOffsetEnd': 72, u'docTokenEnd': 13, u'text': u'Nova Scotia', u'characterOffsetBegin': 61, u'ner': u'LOCATION'}]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Demo of CoreNLP mention recognition accessed through a python wrapper.\"\"\"\n",
    "\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "text = 'Ryan Amaral is a student at Dalhousie University in Halifax, Nova Scotia. It is a cool school.'\n",
    "output = nlp.annotate(text, properties={\n",
    "    'annotators': 'entitymentions',\n",
    "    'outputFormat': 'json'\n",
    "})\n",
    "for mention in output['sentences'][0]['entitymentions']:\n",
    "    print mention['text']\n",
    "    \n",
    "print output['sentences'][0]['entitymentions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting gen-mention-data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile gen-mention-data.py\n",
    "from __future__ import division\n",
    "\"\"\"\n",
    "Generate data to be used for entity recognition.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Mention Data of form [pos before, pos on, pos after, mentions prob].\n",
    "[0:3] are to be One Hot Encoded.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from wikification import *\n",
    "from wikipedia import title2id\n",
    "import copy\n",
    "import sys\n",
    "import copy\n",
    "import nltk\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "scnlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "# need this for novelty detection\n",
    "#from sklearn.svm import OneClassSVM\n",
    "\n",
    "# for pos on pos parts\n",
    "#ohe = OneHotEncoder(n_values=[46,46,46])\n",
    "\n",
    "# convert pos values to numbers\n",
    "posDict = {\n",
    "    \"$\":0,\n",
    "    \"''\":1,\n",
    "    \"(\":2,\n",
    "    \")\":3,\n",
    "    \",\":4,\n",
    "    \"--\":5,\n",
    "    \".\":6,\n",
    "    \":\":7,\n",
    "    \"CC\":8,\n",
    "    \"CD\":9,\n",
    "    \"DT\":10,\n",
    "    \"EX\":11,\n",
    "    \"FW\":12,\n",
    "    \"IN\":13,\n",
    "    \"JJ\":14,\n",
    "    \"JJR\":15,\n",
    "    \"JJS\":16,\n",
    "    \"LS\":17,\n",
    "    \"MD\":18,\n",
    "    \"NN\":19,\n",
    "    \"NNP\":20,\n",
    "    \"NNPS\":21,\n",
    "    \"NNS\":22,\n",
    "    \"PDT\":23,\n",
    "    \"POS\":24,\n",
    "    \"PRP\":25,\n",
    "    \"PRP$\":26,\n",
    "    \"RBR\":27,\n",
    "    \"RBS\":28,\n",
    "    \"RP\":29,\n",
    "    \"SYM\":30,\n",
    "    \"TO\":31,\n",
    "    \"UH\":32,\n",
    "    \"VB\":33,\n",
    "    \"VBD\":34,\n",
    "    \"VBG\":35,\n",
    "    \"VBN\":36,\n",
    "    \"VBP\":37,\n",
    "    \"VBZ\":38,\n",
    "    \"WDT\":39,\n",
    "    \"WP\":40,\n",
    "    \"WP$\":41,\n",
    "    \"WRB\":42,\n",
    "    \"``\":43,\n",
    "    \"None\":44,\n",
    "    \"NONE\":45,\n",
    "    \"RB\":46\n",
    "}\n",
    "\n",
    "posBefDict = {\n",
    "    'IN':0,\n",
    "    'DT':1,\n",
    "    'NNP':2,\n",
    "    'JJ':3,\n",
    "    ',':4,\n",
    "    'CC':5,\n",
    "    'NN':6,\n",
    "    'VBD':7,\n",
    "    'CD':8,\n",
    "    '(':9,\n",
    "    'TO':10,\n",
    "    'FAIL':11\n",
    "}\n",
    "\n",
    "posCurDict = {\n",
    "    'NNP':0,\n",
    "    'NN':1,\n",
    "    'JJ':2,\n",
    "    'NNS':3,\n",
    "    'CD':4,\n",
    "    'NNPS':5,\n",
    "    'FAIL':6\n",
    "}\n",
    "\n",
    "posAftDict = {\n",
    "    ',':0,\n",
    "    '.':1,\n",
    "    'IN':2,\n",
    "    'NNP':3,\n",
    "    'CC':4,\n",
    "    'NN':5,\n",
    "    'VBD':6,\n",
    "    ':':7,\n",
    "    'VBZ':8,\n",
    "    'POS':9,\n",
    "    'NNS':10,\n",
    "    'TO':11,\n",
    "    'FAIL':12\n",
    "}\n",
    "\n",
    "def normalize(nums):\n",
    "    \"\"\"Normalizes a list of nums to its sum + 1\"\"\"\n",
    "    \n",
    "    numSum = sum(nums) + 1 # get max\n",
    "    \n",
    "    # fill with normalized\n",
    "    normNums = []\n",
    "    for num in nums:\n",
    "        normNums.append(num/numSum)\n",
    "        \n",
    "    return normNums\n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "dsPath = os.path.join(pathStrt,'wiki-mentions.30000.json')\n",
    "\n",
    "newData = []\n",
    "\n",
    "# exclude non-mentions to treat as novelty detection\n",
    "# include non-mentions to treat as classification\n",
    "nonMentions = True\n",
    "\n",
    "with open(dsPath, 'r') as dataFile:\n",
    "    dataLines = []\n",
    "    skip = 0\n",
    "    amount = 30000\n",
    "    i = 0\n",
    "    for line in dataFile:\n",
    "        if i >= skip:\n",
    "            dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        i += 1\n",
    "        if i >= skip + amount:\n",
    "            break\n",
    "            \n",
    "errors = 0\n",
    "        \n",
    "lnum = 0\n",
    "for line in dataLines:\n",
    "    \n",
    "    oMentions = copy.deepcopy(line['mentions']) # mentions in original form\n",
    "    oText = \" \".join(copy.deepcopy(line['text']))\n",
    "    #uni = unicode(oText, 'utf-8')\n",
    "    #print uni\n",
    "    line['mentions'] = mentionStartsAndEnds(line, True)\n",
    "\n",
    "    #Get POS tags of all text\n",
    "    postrs = nltk.pos_tag(copy.deepcopy(line['text']))\n",
    "\n",
    "    # get stanford core mentions\n",
    "    try:\n",
    "        stnfrdMentions0 = scnlp.annotate(oText.encode('utf-8'), properties={\n",
    "                'annotators': 'entitymentions',\n",
    "                'outputFormat': 'json'})\n",
    "    except:\n",
    "        errors += 1\n",
    "        print 'Error #' + str(errors) + ' on line #' + str(lnum)\n",
    "        lnum += 1\n",
    "        continue\n",
    "    stnfrdMentions = []\n",
    "    for sentence in stnfrdMentions0['sentences']:\n",
    "        for mention in sentence['entitymentions']:\n",
    "            stnfrdMentions.append(mention['text'])\n",
    "\n",
    "    for i in range(len(line['text'])):\n",
    "        \n",
    "        if nonMentions == False and i not in [item[0] for item in oMentions]:\n",
    "            continue\n",
    "        \n",
    "        newData.append([]) # add new row to mention data at mIdx\n",
    "             \n",
    "        \"\"\" \n",
    "        Append POS tags of before, on, and after mention.\n",
    "        \"\"\"\n",
    "        if i == 0:\n",
    "            bef = 'NONE'\n",
    "        else:\n",
    "            bef = postrs[i-1][1] # pos tag of before\n",
    "        if bef in posBefDict:\n",
    "            bef = posBefDict[bef]\n",
    "        else:\n",
    "            bef = posBefDict['FAIL']\n",
    "            \n",
    "        on = postrs[i][1] # pos tag of mention\n",
    "        if on in posCurDict:\n",
    "            on = posCurDict[on]\n",
    "        else:\n",
    "            on = posCurDict['FAIL']\n",
    "        \n",
    "        if i == len(line['text']) - 1:\n",
    "            aft = 'NONE'\n",
    "        else:\n",
    "            aft = postrs[i+1][1] # pos tag of after\n",
    "        if aft in posAftDict:\n",
    "            aft = posAftDict[aft]\n",
    "        else:\n",
    "            aft = posAftDict['FAIL']\n",
    "        \n",
    "        newData[-1].extend([bef, on, aft])\n",
    "        \n",
    "        \"\"\"\n",
    "        Append mention probability.\n",
    "        \"\"\"\n",
    "        newData[-1].append(mentionProb(line['text'][i]))\n",
    "        \n",
    "        \"\"\"\n",
    "        Find whether Stanford NER decides the word to be mention.\n",
    "        \"\"\"\n",
    "        if line['text'][i] in stnfrdMentions:\n",
    "            stnfrdMentions.remove(line['text'][i])\n",
    "            newData[-1].append(1)\n",
    "        else:\n",
    "            newData[-1].append(0)\n",
    "            \n",
    "        \"\"\"\n",
    "        Whether starts with capital.\n",
    "        \"\"\"\n",
    "        if line['text'][i][0].isupper():\n",
    "            newData[-1].append(1)\n",
    "        else:\n",
    "            newData[-1].append(0)\n",
    "            \n",
    "        \"\"\"\n",
    "        Whether there is an exact match in Wikipedia.\n",
    "        \"\"\"\n",
    "        if title2id(line['text'][i]) is not None:\n",
    "            newData[-1].append(1)\n",
    "        else:\n",
    "            newData[-1].append(0)\n",
    "            \n",
    "        \"\"\"\n",
    "        Whether word contains a space.\n",
    "        \"\"\"\n",
    "        if ' ' in line['text'][i]:\n",
    "            newData[-1].append(1)\n",
    "        else:\n",
    "            newData[-1].append(0)\n",
    "            \n",
    "        \"\"\"\n",
    "        Whether the word contains only ascii characters.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            line['text'][i].decode('ascii')\n",
    "            newData[-1].append(1)\n",
    "        except:\n",
    "            newData[-1].append(0)\n",
    "        \n",
    "        # put in whether is mention or not only if including nonMentions\n",
    "        if nonMentions == True:\n",
    "            if i in [item[0] for item in oMentions]:\n",
    "                newData[-1].append(1)\n",
    "            else:\n",
    "                newData[-1].append(0)\n",
    "    \n",
    "        #print newData[-1]\n",
    "        \n",
    "    lnum += 1\n",
    "    print 'Line: ' + str(lnum)\n",
    "    \n",
    "# nov for novelty, cls for classification\n",
    "#with open('/users/cs/amaral/wikisim/wikification/learning-data/er-10000-nov.txt', 'w') as f:\n",
    "with open('/users/cs/amaral/wikisim/wikification/learning-data/er-30000-cls-v2.txt', 'w') as f:\n",
    "    for data in newData:\n",
    "        f.write(str(data)[1:-1] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Ratio: 10\n",
      "[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "   7.01311452e-05   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.00000000e+00   0.00000000e+00]\n",
      "bagging:\n",
      "Ratio 1 done.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%%writefile er-model-create.py\n",
    "from __future__ import division\n",
    "\n",
    "\"\"\"\n",
    "Create the machine learned models for detection of mentions\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(n_values = [12,7,13], categorical_features = [0,1,2])\n",
    "\n",
    "def createModels(isNovel, posNegRatio = 1, posToUse = 48428, scores = None):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Tests different models on er.\n",
    "    Args:\n",
    "        isNovel: Whether to use novelty detection, or classification algorithms.\n",
    "        posNegRatio: How many negative examples to have per positive example.\n",
    "        posToUse: Amount of positive examples to use (48428 from 10000 dataset).\n",
    "    Return:\n",
    "        Nothing, but creates the models and saves the best.\n",
    "    \"\"\"\n",
    "\n",
    "    poss = [] # positive instances\n",
    "    negs = [] # negative instances\n",
    "\n",
    "    # with 10000 file\n",
    "        # poss: 48428\n",
    "        # negs: 454139\n",
    "        # about 1:10 pos to neg\n",
    "    \n",
    "    negToUse = posToUse * posNegRatio\n",
    "\n",
    "    if isNovel == True:\n",
    "        \n",
    "        # train it on 80% of positive then test on 20% of positive with the \n",
    "        # same amount of negatives\n",
    "        \n",
    "        # get the nov datasets\n",
    "        # positive examples\n",
    "        with open('/users/cs/amaral/wikisim/wikification/learning-data/er-10000-nov.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                data = line.split(',')\n",
    "                for i in range(len(data)):\n",
    "                    if i == 3: # float data\n",
    "                        data[i] = float(data[i])\n",
    "                    else: # int data\n",
    "                        data[i] = int(data[i])\n",
    "                poss.append(data)\n",
    "        \n",
    "        # some negatives\n",
    "        with open('/users/cs/amaral/wikisim/wikification/learning-data/er-10000-cls-neg.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                data = line.split(',')\n",
    "                for i in range(len(data)):\n",
    "                    if i == 3: # float data\n",
    "                        data[i] = float(data[i])\n",
    "                    else: # int data\n",
    "                        data[i] = int(data[i])\n",
    "                negs.append(data)\n",
    "                \n",
    "        poss = shuffle(poss)\n",
    "        \n",
    "        trainAmount = long(0.8 * len(poss))\n",
    "        testAmount = long(0.2 * len(poss))\n",
    "\n",
    "        XTrain = poss[0:trainAmount]\n",
    "        \n",
    "        XTestPos = poss[trainAmount:trainAmount + testAmount]\n",
    "        XTestNeg = [data[:-1] for data in shuffle(negs)[:testAmount]]\n",
    "        \n",
    "        from sklearn.svm import OneClassSVM\n",
    "        \n",
    "        svm = OneClassSVM()\n",
    "        svm.fit(XTrain)\n",
    "        \n",
    "        posPred = svm.predict(XTestPos)\n",
    "        negPred = svm.predict(XTestNeg)\n",
    "        \n",
    "        wrong = 0\n",
    "        for pred in posPred:\n",
    "            if pred == -1:\n",
    "                wrong += 1\n",
    "        print 'Positive Accuracy:', 1 - wrong/testAmount\n",
    "        \n",
    "        wrong = 0\n",
    "        for pred in negPred:\n",
    "            if pred == 1:\n",
    "                wrong += 1\n",
    "        print 'Negative Accuracy:', 1 - wrong/testAmount\n",
    "        \n",
    "    else:\n",
    "        # get the cls datasets\n",
    "        # positive examples\n",
    "        with open('/users/cs/amaral/wikisim/wikification/learning-data/er-30000-cls-pos-v2.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                data = line.split(',')\n",
    "                for i in range(len(data)):\n",
    "                    if i == 3: # float data\n",
    "                        data[i] = float(data[i])\n",
    "                    else: # int data\n",
    "                        data[i] = int(data[i])\n",
    "                poss.append(data)\n",
    "\n",
    "        with open('/users/cs/amaral/wikisim/wikification/learning-data/er-30000-cls-neg-v2.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                data = line.split(',')\n",
    "                for i in range(len(data)):\n",
    "                    if i == 3: # float data\n",
    "                        data[i] = float(data[i])\n",
    "                    else: # int data\n",
    "                        data[i] = int(data[i])\n",
    "                \n",
    "                negs.append(data)\n",
    "\n",
    "        poss = shuffle(poss)[:posToUse]\n",
    "        negs = shuffle(negs)[:negToUse]\n",
    "        poss.extend(negs)\n",
    "        allData = shuffle(poss)\n",
    "        \n",
    "        enc.fit(allData)\n",
    "        allData = enc.transform(allData).toarray()\n",
    "        \n",
    "        print allData[0]\n",
    "\n",
    "        trainAmount = long(0.8 * len(allData))\n",
    "        valiAmount = long(0.2 * len(allData))\n",
    "        testAmount = long(0.0 * len(allData))\n",
    "\n",
    "        XTrain = [data[:-1] for data in allData[0:trainAmount]]\n",
    "        yTrain = [data[-1] for data in allData[0:trainAmount]]\n",
    "\n",
    "        XVali = [data[:-1] for data in allData[trainAmount:trainAmount + valiAmount]]\n",
    "        yVali = [data[-1] for data in allData[trainAmount:trainAmount + valiAmount]]\n",
    "\n",
    "        XTest = [data[:-1] for data in allData[trainAmount + valiAmount:trainAmount + valiAmount + testAmount]]\n",
    "        yTest= [data[-1] for data in allData[trainAmount + valiAmount:trainAmount + valiAmount + testAmount]]\n",
    "\n",
    "        # try these classifiers\n",
    "        from sklearn.ensemble import AdaBoostClassifier\n",
    "        from sklearn.ensemble import BaggingClassifier\n",
    "        from sklearn.ensemble import ExtraTreesClassifier\n",
    "        from sklearn.ensemble import GradientBoostingClassifier\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.svm import LinearSVC\n",
    "        from sklearn.svm import SVC\n",
    "        \n",
    "        from sklearn.metrics import classification_report\n",
    "        \n",
    "        #print 'adaboost:'\n",
    "        #abc = AdaBoostClassifier(n_estimators=300)\n",
    "        #abc.fit(XTrain, yTrain)\n",
    "        #print classification_report(yVali, abc.predict(XVali))\n",
    "        #scores['abc'] = abc.score(XVali, yVali)\n",
    "        #print 'adaboost:', scores['abc']\n",
    "\n",
    "        print 'bagging:'\n",
    "        bgc = BaggingClassifier(verbose=0, n_estimators=300)\n",
    "        bgc.fit(XTrain, yTrain)\n",
    "        #print classification_report(yVali, bgc.predict(XVali))\n",
    "        #scores['bgc'] = bgc.score(XVali, yVali)\n",
    "        #print 'bagging:', scores['bgc']\n",
    "\n",
    "        #print 'extra trees:'\n",
    "        #etc = ExtraTreesClassifier(verbose=0, n_estimators=300, min_samples_split=5)\n",
    "        #etc.fit(XTrain, yTrain)\n",
    "        #print classification_report(yVali, etc.predict(XVali))\n",
    "        #scores['etc'] = etc.score(XVali, yVali)\n",
    "        #print 'extra trees:', scores['etc']\n",
    "\n",
    "        #print 'gradient boosting:'\n",
    "        #gbc = GradientBoostingClassifier(verbose=0, n_estimators=300, min_samples_split=5)\n",
    "        #gbc.fit(XTrain, yTrain)\n",
    "        #print classification_report(yVali, gbc.predict(XVali))\n",
    "        #scores['gbc'] = gbc.score(XVali, yVali)\n",
    "        #print 'gradient boosting:', scores['gbc']\n",
    "        \n",
    "        # save gradient boosting cause is tied for best and small size\n",
    "        import pickle\n",
    "        pickle.dump(bgc, open('/users/cs/amaral/wikisim/wikification/ml-models/er/er-model-bgc-30000.pkl', 'wb'))\n",
    "        #k = 1/0\n",
    "\n",
    "        #print 'random forest:'\n",
    "        #rfc = RandomForestClassifier(verbose=0, n_estimators=300, min_samples_split=5)\n",
    "        #rfc.fit(XTrain, yTrain)\n",
    "        #print classification_report(yVali, rfc.predict(XVali))\n",
    "        #scores['rfc'] = rfc.score(XVali, yVali)\n",
    "        #print 'random forest:', scores['rfc']\n",
    "\n",
    "        #print 'linear svc:'\n",
    "        #lsvc = LinearSVC(verbose=0)\n",
    "        #lsvc.fit(XTrain, yTrain)\n",
    "        #print classification_report(yVali, lsvc.predict(XVali))\n",
    "        #scores['lsvc'] = lsvc.score(XVali, yVali)\n",
    "        #print 'linear svc:', scores['lsvc']\n",
    "\n",
    "        #print 'svc:'\n",
    "        #svc = SVC(verbose=False)\n",
    "        #svc.fit(XTrain, yTrain)\n",
    "        #print classification_report(yVali, svc.predict(XVali))\n",
    "        #scores['svc'] = svc.score(XVali, yVali)\n",
    "        #print 'svc:', scores['svc']\n",
    "        \n",
    "\n",
    "# with novelty detection\n",
    "#Positive Accuracy: 0.498606091895\n",
    "#Negative Accuracy: 0.835002581311\n",
    "#createModels(True, posToUse = 48428) # it sucks\n",
    "\n",
    "\n",
    "# keep scores\n",
    "scores = {}\n",
    "\n",
    "# 48428\n",
    "# try with differing ratios of positive to negative instances\n",
    "for i in [1]:#, 2, 4, 7, 10]:\n",
    "    print 'With Ratio:', 10\n",
    "    scores[str(i)] = {}\n",
    "    createModels(False, posNegRatio = 10, posToUse = 139869, scores = scores[str(i)])\n",
    "    #print scores\n",
    "    print 'Ratio ' + str(i) + ' done.\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting el-model-create.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile el-model-create.py \n",
    "# originally model-create.py\n",
    "\n",
    "allX = []\n",
    "allY = []\n",
    "allMId = []\n",
    "\n",
    "\n",
    "trainX = []\n",
    "trainY = []\n",
    "trainMId = []\n",
    "\n",
    "bigTrainX = []\n",
    "bigTrainY = []\n",
    "bigTrainMId = []\n",
    "\n",
    "valiX = []\n",
    "valiY = []\n",
    "valiMId = []\n",
    "\n",
    "testX = []\n",
    "testY = []\n",
    "testMId = []\n",
    "\n",
    "# dataX from file is (id, isTrueEntity, popularity, context1, context2, word2vec, coherence, mentionId)\n",
    "\n",
    "linesToUse = 10000000 # limit amount of total data\n",
    "totalLines = 0\n",
    "# first try with just getting all data\n",
    "with open('/users/cs/amaral/wikisim/wikification/learning-data/el-10000-hybridgen.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        totalLines += 1\n",
    "        if totalLines > linesToUse:\n",
    "            break\n",
    "        data = line.split(',')\n",
    "        allX.append([float(data[2]), float(data[3]), float(data[4]), 0, float(data[6])])\n",
    "        allY.append(int(data[1]))\n",
    "        allMId.append(long(data[7]))\n",
    "        \n",
    "# split 60,20,20 or 80,20 with bigTrain\n",
    "trainLines = int(totalLines * 0.6)\n",
    "valiLines = int(totalLines * 0.2)\n",
    "testLines = int(totalLines * 0.2)\n",
    "\n",
    "for i in range(0, trainLines):\n",
    "    trainX.append(allX[i])\n",
    "    trainY.append(allY[i])\n",
    "    trainMId.append(allMId[i])\n",
    "    \n",
    "for i in range(0, trainLines + valiLines):\n",
    "    bigTrainX.append(allX[i])\n",
    "    bigTrainY.append(allY[i])\n",
    "    bigTrainMId.append(allMId[i])\n",
    "\n",
    "for i in range(trainLines, trainLines + valiLines):\n",
    "    valiX.append(allX[i])\n",
    "    valiY.append(allY[i])\n",
    "    valiMId.append(allMId[i])\n",
    "    \n",
    "for i in range(trainLines + valiLines, trainLines + valiLines + testLines):\n",
    "    testX.append(allX[i])\n",
    "    testY.append(allY[i])\n",
    "    testMId.append(allMId[i])\n",
    "    \n",
    "print 'about to start training'\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "#from sklearn.svm import NuSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import sys\n",
    "sys.path.append('./pyltr/')\n",
    "import pyltr\n",
    "\"\"\"\n",
    "abc = AdaBoostClassifier(n_estimators=300)\n",
    "abc.fit(bigTrainX, bigTrainY)\n",
    "\n",
    "print 'adaboost done'\n",
    "\n",
    "bgc = BaggingClassifier(verbose=1, n_estimators=300)\n",
    "bgc.fit(bigTrainX, bigTrainY)\n",
    "\n",
    "print 'bagging done'\n",
    "\n",
    "etc = ExtraTreesClassifier(verbose=1, n_estimators=300, min_samples_split=5)\n",
    "etc.fit(bigTrainX, bigTrainY)\n",
    "\n",
    "print 'extra trees done'\n",
    "\n",
    "gbc = GradientBoostingClassifier(verbose=1, n_estimators=300, min_samples_split=5)\n",
    "gbc.fit(bigTrainX, bigTrainY)\n",
    "\n",
    "print 'gradient boosting done'\n",
    "\n",
    "rfc = RandomForestClassifier(verbose=1, n_estimators=300, min_samples_split=5)\n",
    "rfc.fit(bigTrainX, bigTrainY)\n",
    "\n",
    "print 'random forest done'\n",
    "\n",
    "lsvc = LinearSVC(verbose=1)\n",
    "lsvc.fit(bigTrainX, bigTrainY)\n",
    "\n",
    "print 'linear svc done'\n",
    "\n",
    "#nsvc = NuSVC(verbose=True)\n",
    "#nsvc.fit(bigTrainX, bigTrainY)\n",
    "\n",
    "#print 'nusvc done'\n",
    "\n",
    "svc = SVC(verbose=True)\n",
    "svc.fit(bigTrainX, bigTrainY)\n",
    "\n",
    "print 'svc done'\"\"\"\n",
    "\n",
    "monitor = pyltr.models.monitors.ValidationMonitor(\n",
    "    valiX, valiY, valiMId, metric=pyltr.metrics.NDCG(k=10), stop_after=250)\n",
    "lmart = pyltr.models.LambdaMART(n_estimators=300, learning_rate=0.1, verbose = 1)\n",
    "lmart.fit(trainX, trainY, trainMId, monitor=monitor)\n",
    "\n",
    "print 'lmart done'\n",
    "\n",
    "\"\"\"\n",
    "Save the model.\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "\n",
    "#pickle.dump(abc, open('/users/cs/amaral/wikisim/wikification/ml-models/model-abc-10000-pop.pkl', 'wb'))\n",
    "#pickle.dump(bgc, open('/users/cs/amaral/wikisim/wikification/ml-models/model-bgc-10000-pop.pkl', 'wb'))\n",
    "#pickle.dump(etc, open('/users/cs/amaral/wikisim/wikification/ml-models/model-etc-10000-pop.pkl', 'wb'))\n",
    "#pickle.dump(gbc, open('/users/cs/amaral/wikisim/wikification/ml-models/model-gbc-10000-pop.pkl', 'wb'))\n",
    "#pickle.dump(rfc, open('/users/cs/amaral/wikisim/wikification/ml-models/model-rfc-10000-pop.pkl', 'wb'))\n",
    "#pickle.dump(lsvc, open('/users/cs/amaral/wikisim/wikification/ml-models/model-lsvc-10000-pop.pkl', 'wb'))\n",
    "#pickle.dump(nsvc, open('/users/cs/amaral/wikisim/wikification/ml-models/model-nsvc-10000-hyb.pkl', 'wb'))\n",
    "#pickle.dump(svc, open('/users/cs/amaral/wikisim/wikification/ml-models/model-svc-10000-pop.pkl', 'wb'))\n",
    "pickle.dump(lmart, open('/users/cs/amaral/wikisim/wikification/ml-models/model-lmart-10000-hyb-no-w2v.pkl', 'wb'))\n",
    "\n",
    "print 'models saved'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 170097\n",
      "56699\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell is to get all the data for the ml model\n",
    "\"\"\"\n",
    "\n",
    "allX = []\n",
    "allY = []\n",
    "allMId = []\n",
    "\n",
    "trainX = []\n",
    "trainY = []\n",
    "trainMId = []\n",
    "valiX = []\n",
    "valiY = []\n",
    "valiMId = []\n",
    "testX = []\n",
    "testY = []\n",
    "testMId = []\n",
    "\n",
    "linesToUse = 1000000 # limit amount of total data\n",
    "totalLines = 0\n",
    "# first try with just getting all data\n",
    "with open('/users/cs/amaral/wikisim/wikification/learning-data/el-5000.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        totalLines += 1\n",
    "        if totalLines > linesToUse:\n",
    "            break\n",
    "        data = line.split(',')\n",
    "        allX.append([float(data[2]), float(data[3]), float(data[4]), float(data[5]), float(data[6])])\n",
    "        allY.append(int(data[1]))\n",
    "        allMId.append(long(data[7]))\n",
    "        \n",
    "# split 75, 25\n",
    "trainLines = int(totalLines * 0.75)\n",
    "valiLines = int(totalLines * 0.0)\n",
    "testLines = int(totalLines * 0.25)\n",
    "\n",
    "for i in range(0, trainLines):\n",
    "    trainX.append(allX[i])\n",
    "    trainY.append(allY[i])\n",
    "    trainMId.append(allMId[i])\n",
    "\n",
    "for i in range(trainLines, trainLines + valiLines):\n",
    "    valiX.append(allX[i])\n",
    "    valiY.append(allY[i])\n",
    "    valiMId.append(allMId[i])\n",
    "    \n",
    "for i in range(trainLines + valiLines, trainLines + valiLines + testLines):\n",
    "    testX.append(allX[i])\n",
    "    testY.append(allY[i])\n",
    "    testMId.append(allMId[i])\n",
    "    \n",
    "print len(trainX)\n",
    "print len(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter  Train score    Remaining                           Monitor Output \n",
      "    1       0.8987       88.16m                                         \n",
      "    2       0.9178       86.97m                                         \n",
      "    3       0.9288       86.27m                                         \n",
      "    4       0.9290       85.70m                                         \n",
      "    5       0.9356       85.35m                                         \n",
      "    6       0.9362       84.96m                                         \n",
      "    7       0.9387       84.65m                                         \n",
      "    8       0.9411       84.34m                                         \n",
      "    9       0.9412       84.02m                                         \n",
      "   10       0.9414       83.73m                                         \n",
      "   15       0.9439       82.20m                                         \n",
      "   20       0.9479       80.73m                                         \n",
      "   25       0.9519       79.28m                                         \n",
      "   30       0.9546       77.82m                                         \n",
      "   35       0.9556       76.38m                                         \n",
      "   40       0.9571       74.97m                                         \n",
      "   45       0.9580       73.55m                                         \n",
      "   50       0.9585       72.14m                                         \n"
     ]
    }
   ],
   "source": [
    "\"\"\" This cell helped by: https://github.com/ogrisel/notebooks/blob/master/Learning%20to%20Rank.ipynb\n",
    "This cell is to train the model.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import sys\n",
    "sys.path.append('./pyltr/')\n",
    "import pyltr\n",
    "\n",
    "#etr = ExtraTreesRegressor(n_estimators=200, min_samples_split=5, random_state=1, n_jobs=-1)\n",
    "#etr.fit(trainX, trainY)\n",
    "\n",
    "#rfr = RandomForestRegressor(n_estimators=200, min_samples_split=5, random_state=1, n_jobs=-1)\n",
    "#rfr.fit(trainX, trainY)\n",
    "\n",
    "#gbr = GradientBoostingRegressor(n_estimators=300, max_depth=3, learning_rate=0.1, loss='ls', random_state=1)\n",
    "#gbr.fit(trainX, trainY)\n",
    "\n",
    "#gbc = GradientBoostingClassifier(n_estimators=200, min_samples_split=5, random_state=1)\n",
    "#gbc.fit(trainX, trainY)\n",
    "\n",
    "lmart = pyltr.models.LambdaMART(n_estimators=300, learning_rate=0.1, verbose = 1)\n",
    "lmart.fit(trainX, trainY, trainMId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees Regressor:\n",
      "R^2 Score: 0.843993888891\n",
      "\n",
      "BDB =  0.0 \n",
      "TP: 6134 0.993199481865 \n",
      "FP: 8991 0.17795855353 \n",
      "TN: 41532 0.82204144647 \n",
      "FN: 42 0.00680051813472\n",
      "\n",
      "BDB =  0.1 \n",
      "TP: 5993 0.970369170984 \n",
      "FP: 1909 0.0377847712923 \n",
      "TN: 48614 0.962215228708 \n",
      "FN: 183 0.0296308290155\n",
      "\n",
      "BDB =  0.2 \n",
      "TP: 5897 0.954825129534 \n",
      "FP: 1204 0.0238307305584 \n",
      "TN: 49319 0.976169269442 \n",
      "FN: 279 0.0451748704663\n",
      "\n",
      "BDB =  0.3 \n",
      "TP: 5788 0.937176165803 \n",
      "FP: 878 0.0173782237793 \n",
      "TN: 49645 0.982621776221 \n",
      "FN: 388 0.0628238341969\n",
      "\n",
      "BDB =  0.4 \n",
      "TP: 5679 0.919527202073 \n",
      "FP: 667 0.0132019080419 \n",
      "TN: 49856 0.986798091958 \n",
      "FN: 497 0.0804727979275\n",
      "\n",
      "BDB =  0.5 \n",
      "TP: 5533 0.895887305699 \n",
      "FP: 496 0.0098173109277 \n",
      "TN: 50027 0.990182689072 \n",
      "FN: 643 0.104112694301\n",
      "\n",
      "BDB =  0.6 \n",
      "TP: 5372 0.86981865285 \n",
      "FP: 374 0.00740256912693 \n",
      "TN: 50149 0.992597430873 \n",
      "FN: 804 0.13018134715\n",
      "\n",
      "BDB =  0.7 \n",
      "TP: 5177 0.838244818653 \n",
      "FP: 270 0.00534410070661 \n",
      "TN: 50253 0.994655899293 \n",
      "FN: 999 0.161755181347\n",
      "\n",
      "BDB =  0.8 \n",
      "TP: 4894 0.792422279793 \n",
      "FP: 174 0.00344397601093 \n",
      "TN: 50349 0.996556023989 \n",
      "FN: 1282 0.207577720207\n",
      "\n",
      "BDB =  0.9 \n",
      "TP: 4462 0.722474093264 \n",
      "FP: 91 0.00180115986778 \n",
      "TN: 50432 0.998198840132 \n",
      "FN: 1714 0.277525906736\n",
      "\n",
      "\n",
      "Random Forest Regressor:\n",
      "R^2 Score: 0.84456202897\n",
      "\n",
      "BDB =  0.0 \n",
      "TP: 6122 0.991256476684 \n",
      "FP: 6381 0.126298913366 \n",
      "TN: 44142 0.873701086634 \n",
      "FN: 54 0.00874352331606\n",
      "\n",
      "BDB =  0.1 \n",
      "TP: 5994 0.970531088083 \n",
      "FP: 1866 0.0369336737723 \n",
      "TN: 48657 0.963066326228 \n",
      "FN: 182 0.0294689119171\n",
      "\n",
      "BDB =  0.2 \n",
      "TP: 5895 0.954501295337 \n",
      "FP: 1227 0.0242859687667 \n",
      "TN: 49296 0.975714031233 \n",
      "FN: 281 0.0454987046632\n",
      "\n",
      "BDB =  0.3 \n",
      "TP: 5796 0.938471502591 \n",
      "FP: 883 0.0174771886072 \n",
      "TN: 49640 0.982522811393 \n",
      "FN: 380 0.0615284974093\n",
      "\n",
      "BDB =  0.4 \n",
      "TP: 5683 0.920174870466 \n",
      "FP: 659 0.0130435643172 \n",
      "TN: 49864 0.986956435683 \n",
      "FN: 493 0.0798251295337\n",
      "\n",
      "BDB =  0.5 \n",
      "TP: 5540 0.897020725389 \n",
      "FP: 503 0.00995586168676 \n",
      "TN: 50020 0.990044138313 \n",
      "FN: 636 0.102979274611\n",
      "\n",
      "BDB =  0.6 \n",
      "TP: 5393 0.873218911917 \n",
      "FP: 381 0.00754111988599 \n",
      "TN: 50142 0.992458880114 \n",
      "FN: 783 0.126781088083\n",
      "\n",
      "BDB =  0.7 \n",
      "TP: 5185 0.83954015544 \n",
      "FP: 271 0.00536389367219 \n",
      "TN: 50252 0.994636106328 \n",
      "FN: 991 0.16045984456\n",
      "\n",
      "BDB =  0.8 \n",
      "TP: 4902 0.79371761658 \n",
      "FP: 166 0.00328563228629 \n",
      "TN: 50357 0.996714367714 \n",
      "FN: 1274 0.20628238342\n",
      "\n",
      "BDB =  0.9 \n",
      "TP: 4490 0.727007772021 \n",
      "FP: 101 0.00199908952358 \n",
      "TN: 50422 0.998000910476 \n",
      "FN: 1686 0.272992227979\n",
      "\n",
      "\n",
      "Gradient Boosting Regressor:\n",
      "R^2 Score: 0.848231559571\n",
      "\n",
      "BDB =  0.0 \n",
      "TP: 6157 0.99692357513 \n",
      "FP: 26752 0.529501415197 \n",
      "TN: 23771 0.470498584803 \n",
      "FN: 19 0.00307642487047\n",
      "\n",
      "BDB =  0.1 \n",
      "TP: 6012 0.973445595855 \n",
      "FP: 1951 0.0386160758466 \n",
      "TN: 48572 0.961383924153 \n",
      "FN: 164 0.0265544041451\n",
      "\n",
      "BDB =  0.2 \n",
      "TP: 5894 0.954339378238 \n",
      "FP: 1210 0.0239494883518 \n",
      "TN: 49313 0.976050511648 \n",
      "FN: 282 0.0456606217617\n",
      "\n",
      "BDB =  0.3 \n",
      "TP: 5794 0.938147668394 \n",
      "FP: 864 0.0171011222611 \n",
      "TN: 49659 0.982898877739 \n",
      "FN: 382 0.0618523316062\n",
      "\n",
      "BDB =  0.4 \n",
      "TP: 5679 0.919527202073 \n",
      "FP: 629 0.0124497753498 \n",
      "TN: 49894 0.98755022465 \n",
      "FN: 497 0.0804727979275\n",
      "\n",
      "BDB =  0.5 \n",
      "TP: 5551 0.898801813472 \n",
      "FP: 487 0.00963917423748 \n",
      "TN: 50036 0.990360825763 \n",
      "FN: 625 0.101198186528\n",
      "\n",
      "BDB =  0.6 \n",
      "TP: 5412 0.876295336788 \n",
      "FP: 356 0.00704629574649 \n",
      "TN: 50167 0.992953704254 \n",
      "FN: 764 0.123704663212\n",
      "\n",
      "BDB =  0.7 \n",
      "TP: 5201 0.842130829016 \n",
      "FP: 253 0.00500762029175 \n",
      "TN: 50270 0.994992379708 \n",
      "FN: 975 0.157869170984\n",
      "\n",
      "BDB =  0.8 \n",
      "TP: 4882 0.790479274611 \n",
      "FP: 144 0.00285018704352 \n",
      "TN: 50379 0.997149812956 \n",
      "FN: 1294 0.209520725389\n",
      "\n",
      "BDB =  0.9 \n",
      "TP: 4279 0.692843264249 \n",
      "FP: 73 0.00144488648734 \n",
      "TN: 50450 0.998555113513 \n",
      "FN: 1897 0.307156735751\n",
      "\n",
      "\n",
      "Gradient Boosting Classifier:\n",
      "R^2 Score: 0.980017284255\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell tells the accuracy of the model.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "model = etr\n",
    "print 'Extra Trees Regressor:'\n",
    "print 'R^2 Score:', model.score(testX, testY)\n",
    "print\n",
    "for i in np.arange(0.0, 1.0, 0.1):\n",
    "    printEval(model, testX, testY, i)\n",
    "    print\n",
    "print\n",
    "\n",
    "model = rfr\n",
    "print 'Random Forest Regressor:'\n",
    "print 'R^2 Score:', model.score(testX, testY)\n",
    "print\n",
    "for i in np.arange(0.0, 1.0, 0.1):\n",
    "    printEval(model, testX, testY, i)\n",
    "    print\n",
    "print\n",
    "\n",
    "model = gbr\n",
    "print 'Gradient Boosting Regressor:'\n",
    "print 'R^2 Score:', model.score(testX, testY)\n",
    "print\n",
    "for i in np.arange(0.0, 1.0, 0.1):\n",
    "    printEval(model, testX, testY, i)\n",
    "    print\n",
    "print\n",
    "\n",
    "model = gbc\n",
    "print 'Gradient Boosting Classifier:'\n",
    "print 'R^2 Score:', model.score(testX, testY)\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "def printEval(model, X, y, bdb = 0.5):\n",
    "    predY = model.predict(X)\n",
    "    \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        if predY[i] > bdb and y[i] == 1:\n",
    "            tp += 1\n",
    "        elif predY[i] > bdb and y[i] == 0:\n",
    "            fp += 1\n",
    "        elif predY[i] <= bdb and y[i] == 1:\n",
    "            fn += 1\n",
    "        elif predY[i] <= bdb and y[i] == 0:\n",
    "            tn += 1\n",
    "            \n",
    "    print 'BDB = ', bdb, '\\nTP:', tp, tp/(tp+fn), '\\nFP:', fp, fp/(fp+tn), '\\nTN:', tn, tn/(fp+tn), '\\nFN:', fn, fn/(tp+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save the model.\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "\n",
    "#pickle.dump(etr, open('/users/cs/amaral/wikisim/wikification/ml-models/model-etr-1.pkl', 'wb'))\n",
    "#pickle.dump(rfr, open('/users/cs/amaral/wikisim/wikification/ml-models/model-rfr-1.pkl', 'wb'))\n",
    "#pickle.dump(gbr, open('/users/cs/amaral/wikisim/wikification/ml-models/model-gbr-1.pkl', 'wb'))\n",
    "#pickle.dump(gbc, open('/users/cs/amaral/wikisim/wikification/ml-models/model-gbc-1.pkl', 'wb'))\n",
    "#pickle.dump(lmart, open('/users/cs/amaral/wikisim/wikification/ml-models/model-lmart-1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.38906408, -0.38142295,  0.14399559, -1.21259834,  6.16869182,\n",
       "        1.39566366,  6.07528854,  5.27429442, -5.77873421, -3.68008578,\n",
       "       -7.37840903, -6.02215263, -4.83235341, -5.99034009, -4.64082089,\n",
       "       -6.54484845, -3.38575076, -5.05438867])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import sys\n",
    "sys.path.append('./pyltr/')\n",
    "import pyltr\n",
    "\n",
    "model = pickle.load(open('/users/cs/amaral/wikisim/wikification/ml-models/model-lmart-1.pkl', 'rb'))\n",
    "\n",
    "model.predict(testX[2:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16, 0.37197801830625504, 0.75, 0.0, 0.8058462601912664], [0.12, 0.12183860865881956, 0.0, 0.0, 0.003000814788764128], [0.08, 0.15388780453451467, 0.0, 0.0, 0.0197314194384961], [0.04, 0.12183860865881956, 0.0, 0.0, 0.00597341299761156], [0.6666666666666666, 0.5524107765948493, 0.75, 0.0, 0.6424277609623221], [0.2222222222222222, 0.43160739972500395, 0.0, 0.0, 0.04122347126384451], [0.9875, 0.9595134734099676, 0.9090909090909091, 0.25918609576466367, 0.9999999999999998], [0.9585703450891164, 0.1346076964691621, 0.9090909090909091, 0.265528050509531, 0.6452690222713563], [0.010586525091644546, 0.0, 0.0, 0.13257166946600463, 0.0008528990630150002], [0.006415118189862217, 0.07584652897571567, 0.0, 0.18615200157548295, 0.021349869684028633], [0.004361016306408798, 0.0, 0.0, 0.13820647183391077, 0.0], [0.0034445708507141954, 0.0, 0.0, 0.2197684333582972, 0.001374533158081892], [0.002844141069397042, 0.06692287344969088, 0.0, 0.1855829880405545, 0.0038656172984994353], [0.0027177347996460623, 0.0, 0.0, 0.12775189841427181, 0.0008678591415569592], [0.002085703450891164, 0.0741983070523882, 0.0, 0.1666364333953726, 0.014134646571460907], [0.0015800783718872456, 0.0, 0.0, 0.13560885982084758, 0.0010101363516014095], [0.0011692579951965618, 0.13443111220948226, 0.0, 0.1846330840700947, 0.07855439768226724], [0.0009796485905700922, 0.0992152278375584, 0.0, 0.20990917625900218, 0.03458034001283328]]\n",
      "\n",
      "[1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print testX[2:20]\n",
    "print\n",
    "print testY[2:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile el-data-gen.py \n",
    "\n",
    "\"\"\"\n",
    "This file/cell is to generate training data for entity linking for a supervised model.\n",
    "Each row has form: (id, isTrueEntity, popularity, context1, context2, word2vec, coherence, mentionId)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import division\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from wikification import *\n",
    "import copy\n",
    "import sys\n",
    " \n",
    "def normalize(nums):\n",
    "    \"\"\"Normalizes a list of nums to its sum + 1\"\"\"\n",
    "    \n",
    "    numSum = sum(nums) + 1 # get max\n",
    "    \n",
    "    # fill with normalized\n",
    "    normNums = []\n",
    "    for num in nums:\n",
    "        normNums.append(num/numSum)\n",
    "        \n",
    "    return normNums\n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "dsPath = os.path.join(pathStrt,'wiki-mentions.5000.json')\n",
    "\n",
    "with open(dsPath, 'r') as dataFile:\n",
    "    dataLines = []\n",
    "    i = 0\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        i += 1\n",
    "        if i > 5000:\n",
    "            break\n",
    "        \n",
    "cPerM = 20 # candidates per mention\n",
    "\n",
    "allCands = []\n",
    "\n",
    "# word2vec loading\n",
    "try:\n",
    "    word2vec\n",
    "except:\n",
    "    print 'loading word2vec'\n",
    "    word2vec = gensim_loadmodel('/users/cs/amaral/cgmdir/WikipediaClean5Negative300Skip10.Ehsan/WikipediaClean5Negative300Skip10')\n",
    "\n",
    "print 'word2vec loaded'\n",
    "    \n",
    "f = 0\n",
    "\n",
    "mNum = 0\n",
    "# see each line\n",
    "for line in dataLines:\n",
    "    \n",
    "    oMentions = copy.deepcopy(line['mentions']) # mentions in original form\n",
    "    oText = \" \".join(copy.deepcopy(line['text']))\n",
    "    \n",
    "    line['mentions'] = mentionStartsAndEnds(line)\n",
    "    # get what should be all candidates\n",
    "    candidates = generateCandidates(line, 999, True)\n",
    "    \n",
    "    i = 0\n",
    "    for i in range(0, len(candidates)):\n",
    "        entId = title2id(oMentions[i][1])\n",
    "        j = 0\n",
    "        candsRepl = []\n",
    "        for cand in candidates[i]:\n",
    "            if j >= cPerM:\n",
    "                break\n",
    "            \n",
    "            if cand[0] == entId:\n",
    "                candsRepl.append([entId, 1, cand[1]]) # put in correct cand id and popularity\n",
    "                j += 1\n",
    "            elif j < cPerM:\n",
    "                candsRepl.append([cand[0], 0, cand[1]]) # put false cand in\n",
    "                j += 1\n",
    "        candidates[i] = candsRepl\n",
    "    \n",
    "    i = 0 # index of mention\n",
    "    \n",
    "    hasCoherence = False # whether coherence scores for this line were obtained\n",
    "    \n",
    "    # see each mention\n",
    "    for mention in oMentions:\n",
    "    \n",
    "        entId = title2id(mention[1]) # id of the true entity\n",
    "                \n",
    "        candList = candidates[i]\n",
    "        \n",
    "        # normalize popularity scores\n",
    "        cScrs = []\n",
    "        for cand in candList:\n",
    "            cScrs.append(cand[2])\n",
    "        cScrs = normalize(cScrs)\n",
    "        j = 0\n",
    "        for cand in candList:\n",
    "            cand[2] = cScrs[j]\n",
    "            j += 1\n",
    "          \n",
    "        # get score from context1 method\n",
    "        context = getMentionsInSentence(line, line['mentions'][i]) # get context for some w methods\n",
    "        cScrs = getContext1Scores(line['text'][mention[0]], context, candList)\n",
    "        cScrs = normalize(cScrs)\n",
    "        # apply score to candList\n",
    "        for j in range(0, len(candList)):\n",
    "            candList[j].append(cScrs[j])\n",
    "            \n",
    "        # get score from context2 method\n",
    "        context = getMentionsInSentence(line, line['mentions'][i]) # get context for some w methods\n",
    "        cScrs = getContext2Scores(line['text'][mention[0]], context, candList)\n",
    "        cScrs = normalize(cScrs)\n",
    "        # apply score to candList\n",
    "        for j in range(0, len(candList)):\n",
    "            candList[j].append(cScrs[j])\n",
    "        \n",
    "        # get score form word2vec\n",
    "        context = getMentionSentence(oText, line['mentions'][i], asList = True)\n",
    "        cScrs = getWord2VecScores(context, candList)\n",
    "        #cScrs = normalize(cScrs)\n",
    "        # apply score to candList\n",
    "        for j in range(0, len(candList)):\n",
    "            candList[j].append(cScrs[j])\n",
    "\n",
    "        # get score from coherence\n",
    "        if hasCoherence == False:\n",
    "            cohScores = coherence_scores_driver(candidates, 5, method='rvspagerank', direction=DIR_BOTH, op_method=\"keydisamb\")\n",
    "            hasCoherence = True\n",
    "        for j in range(0, len(candList)):\n",
    "            candList[j].append(cohScores[i][j])\n",
    "            \n",
    "        # put the mention id\n",
    "        for j in range(len(candList)):\n",
    "            candList[j].append(mNum)\n",
    "            \n",
    "        allCands.append(candList)\n",
    "        \n",
    "        mNum += 1\n",
    "        \n",
    "        i += 1\n",
    "    f += 1\n",
    "    print 'Line: ' + str(f)\n",
    "        \n",
    "\n",
    "with open('/users/cs/amaral/wikisim/wikification/learning-data/el-5000-hybridgen.txt', 'w') as f:\n",
    "    for thing in allCands:\n",
    "        for thingy in thing:\n",
    "            f.write(str(thingy)[1:-1] + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
