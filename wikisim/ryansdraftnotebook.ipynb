{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diffs:\n",
      "['IN : ,', 0.0964467250436815]\n",
      "['IN : .', 0.0663983859118653]\n",
      "['DT : .', 0.06356270128891531]\n",
      "[', : ,', 0.0558655343623714]\n",
      "['NNP : NNP', 0.03914954427069999]\n",
      "['DT : IN', 0.0374654953394]\n",
      "['DT : ,', 0.036720164152527546]\n",
      "['IN : IN', 0.03425401140626]\n",
      "['NNP : DT', 0.0331820287963143]\n",
      "[', : .', 0.030977509144133657]\n",
      "\n",
      "Raw Diffs:\n",
      "['NNP : NNP', 87162L]\n",
      "['IN : NNP', 42580L]\n",
      "['NNP : DT', 42378L]\n",
      "['NN : NNP', 34568L]\n",
      "['NN : DT', 34356L]\n",
      "['IN : NN', 28871L]\n",
      "['DT : IN', 24297L]\n",
      "['DT : NN', 24266L]\n",
      "['NNP : CD', 23239L]\n",
      "['NNP : IN', 23097L]\n",
      "\n",
      "Mention Solos:\n",
      "len:  38\n",
      "['POS : .', 0.000915142025753]\n",
      "['VB : .', 0.000529066483638]\n",
      "['VBZ : POS', 0.000371776447962]\n",
      "['POS : ,', 0.0003002809772]\n",
      "['CC : POS', 0.000250234147667]\n",
      "['PRP$ : ,', 0.000243084600591]\n",
      "['JJS : ,', 0.0001501404886]\n",
      "['VBG : POS', 0.000135841394448]\n",
      "['WRB : ,', 8.57945649143e-05]\n",
      "[': : .', 6.43459236857e-05]\n",
      "\n",
      "Non-Mention Solos:\n",
      "len:  687\n",
      "['PRP : VBN', 5136L, 0.00402843750588]\n",
      "['PRP : RB', 2526L, 0.00198127592287]\n",
      "['NNP : PRP$', 2469L, 0.00193656779634]\n",
      "['VBN : PRP$', 1197L, 0.000938870657037]\n",
      "['PRP : JJ', 1023L, 0.000802393218169]\n",
      "['VBN : VB', 919L, 0.000720820496088]\n",
      "['NNS : RB', 898L, 0.000704349081052]\n",
      "['RB : CD', 892L, 0.00069964296247]\n",
      "['PRP : NNP', 863L, 0.000676896722659]\n",
      "['WDT : VBN', 809L, 0.000634541655424]\n",
      "\n",
      "Best Diffs:\n",
      "['NNP : DT', 9L, 42387L, 6.43459236857e-05, 0.03324637472]\n",
      "['NN : DT', 5L, 34361L, 3.5747735381e-05, 0.0269511567639]\n",
      "['CD : DT', 3L, 16313L, 2.14486412286e-05, 0.0127951520704]\n",
      "['NNP : NONE', 1L, 13148L, 7.14954707619e-06, 0.0103126745186]\n",
      "['VBN : DT', 1L, 10342L, 7.14954707619e-06, 0.00811177972855]\n",
      "['VBZ : JJ', 6L, 9676L, 4.28972824572e-05, 0.00758940056599]\n",
      "['NNP : PRP', 3L, 8673L, 2.14486412286e-05, 0.00680269440976]\n",
      "['NNS : DT', 1L, 8029L, 7.14954707619e-06, 0.00629757101533]\n",
      "['. : NN', 7L, 6136L, 5.00468295333e-05, 0.00481279060282]\n",
      "['PRP : DT', 2L, 5210L, 1.42990941524e-05, 0.00408647963506]\n",
      "['VBD : DT', 4L, 4390L, 2.85981883048e-05, 0.00344331009557]\n",
      "['RB : DT', 2L, 3627L, 1.42990941524e-05, 0.0028448486826]\n",
      "['CD : NONE', 1L, 3244L, 7.14954707619e-06, 0.00254444144647]\n",
      "['JJ : DT', 8L, 3185L, 5.71963766095e-05, 0.00249816461375]\n",
      "['VBN : CD', 1L, 2949L, 7.14954707619e-06, 0.00231305728288]\n",
      "[', : DT', 7L, 2888L, 5.00468295333e-05, 0.00226521174396]\n",
      "['VBZ : DT', 4L, 2696L, 2.85981883048e-05, 0.00211461594935]\n",
      "['NNS : JJ', 1L, 2485L, 7.14954707619e-06, 0.0019491174459]\n",
      "['PRP : IN', 7L, 2448L, 5.00468295333e-05, 0.00192009638131]\n",
      "['VBD : CD', 2L, 2340L, 1.42990941524e-05, 0.00183538624684]\n",
      "['NN : PRP$', 2L, 2335L, 1.42990941524e-05, 0.00183146448135]\n",
      "['VBZ : VBN', 7L, 2240L, 5.00468295333e-05, 0.00175695093715]\n",
      "['NN : VB', 2L, 2226L, 1.42990941524e-05, 0.00174596999379]\n",
      "['VBN : JJ', 10L, 2185L, 7.14954707619e-05, 0.00171381151681]\n",
      "['JJ : NONE', 3L, 2154L, 2.14486412286e-05, 0.00168949657081]\n",
      "['TO : DT', 9L, 2048L, 6.43459236857e-05, 0.00160635514253]\n",
      "['NN : NONE', 9L, 2016L, 6.43459236857e-05, 0.00158125584343]\n",
      "['NN : PRP', 8L, 1986L, 5.71963766095e-05, 0.00155772525052]\n",
      "['VBD : VBN', 3L, 1776L, 2.14486412286e-05, 0.00139301110017]\n",
      "['RB : JJ', 6L, 1675L, 4.28972824572e-05, 0.00131379143737]\n",
      "['JJ : PRP', 3L, 1541L, 2.14486412286e-05, 0.00120868812238]\n",
      "['VBG : DT', 3L, 1526L, 2.14486412286e-05, 0.00119692282593]\n",
      "['CD : RB', 7L, 1490L, 5.00468295333e-05, 0.00116868611444]\n",
      "[') : DT', 1L, 1440L, 7.14954707619e-06, 0.00112946845959]\n",
      "['NNS : NNS', 2L, 1404L, 1.42990941524e-05, 0.0011012317481]\n",
      "['NNS : CD', 1L, 1400L, 7.14954707619e-06, 0.00109809433572]\n",
      "['NNS : VBN', 7L, 1354L, 5.00468295333e-05, 0.00106201409326]\n",
      "['CD : PRP', 1L, 1307L, 7.14954707619e-06, 0.0010251494977]\n",
      "['RB : TO', 9L, 1217L, 6.43459236857e-05, 0.000954557718976]\n",
      "['NNP : VB', 8L, 1201L, 5.71963766095e-05, 0.000942008069425]\n",
      "['VB : JJ', 8L, 1156L, 5.71963766095e-05, 0.000906712180062]\n",
      "['. : NNP', 9L, 1151L, 6.43459236857e-05, 0.000902790414578]\n",
      "[', : JJ', 4L, 1142L, 2.85981883048e-05, 0.000895731236705]\n",
      "['. : JJ', 3L, 1142L, 2.14486412286e-05, 0.000895731236705]\n",
      "['PRP : NN', 5L, 1103L, 3.5747735381e-05, 0.000865141465925]\n",
      "['VBZ : CD', 2L, 1029L, 1.42990941524e-05, 0.000807099336751]\n",
      "['PRP$ : NNP', 2L, 1021L, 1.42990941524e-05, 0.000800824511976]\n",
      "['JJ : CD', 7L, 1020L, 5.00468295333e-05, 0.000800040158879]\n",
      "['. : DT', 1L, 1014L, 7.14954707619e-06, 0.000795334040297]\n",
      "['VBD : VB', 1L, 1005L, 7.14954707619e-06, 0.000788274862424]\n",
      "['VBP : JJ', 2L, 986L, 1.42990941524e-05, 0.000773372153583]\n",
      "['CC : PRP', 7L, 951L, 5.00468295333e-05, 0.00074591979519]\n",
      "['VB : DT', 1L, 905L, 7.14954707619e-06, 0.00070983955273]\n",
      "['CC : CD', 6L, 900L, 4.28972824572e-05, 0.000705917787246]\n",
      "['CC : NONE', 7L, 879L, 5.00468295333e-05, 0.00068944637221]\n",
      "['VBN : NNS', 6L, 877L, 4.28972824572e-05, 0.000687877666016]\n",
      "['RB : NNS', 10L, 876L, 7.14954707619e-05, 0.000687093312919]\n",
      "['WP : IN', 8L, 850L, 5.71963766095e-05, 0.000666700132399]\n",
      "['WDT : IN', 3L, 840L, 2.14486412286e-05, 0.000658856601429]\n",
      "['VBZ : RB', 6L, 798L, 4.28972824572e-05, 0.000625913771358]\n",
      "['WDT : DT', 2L, 795L, 1.42990941524e-05, 0.000623560712067]\n",
      "['TO : CD', 2L, 780L, 1.42990941524e-05, 0.000611795415613]\n",
      "['PRP : VBD', 1L, 763L, 7.14954707619e-06, 0.000598461412965]\n",
      "['VBD : RB', 10L, 749L, 7.14954707619e-05, 0.000587480469608]\n",
      "['IN : JJS', 5L, 739L, 3.5747735381e-05, 0.000579636938638]\n",
      "['VBZ : NNS', 5L, 719L, 3.5747735381e-05, 0.0005639498767]\n",
      "['TO : PRP', 2L, 709L, 1.42990941524e-05, 0.00055610634573]\n",
      "['NONE : JJ', 6L, 709L, 4.28972824572e-05, 0.00055610634573]\n",
      "['PRP : CD', 1L, 705L, 7.14954707619e-06, 0.000552968933343]\n",
      "['TO : NONE', 2L, 703L, 1.42990941524e-05, 0.000551400227149]\n",
      "['IN : NNPS', 1L, 694L, 7.14954707619e-06, 0.000544341049276]\n",
      "['NNS : VBP', 6L, 690L, 4.28972824572e-05, 0.000541203636888]\n",
      "[', : CD', 5L, 686L, 3.5747735381e-05, 0.000538066224501]\n",
      "['RB : VBN', 1L, 670L, 7.14954707619e-06, 0.00052551657495]\n",
      "['VBD : VBD', 8L, 656L, 5.71963766095e-05, 0.000514535631592]\n",
      "['TO : VBN', 2L, 647L, 1.42990941524e-05, 0.00050747645372]\n",
      "['PRP$ : VBD', 10L, 644L, 7.14954707619e-05, 0.000505123394429]\n",
      "['VBP : DT', 1L, 636L, 7.14954707619e-06, 0.000498848569654]\n",
      "['CD : TO', 4L, 631L, 2.85981883048e-05, 0.000494926804169]\n",
      "['VBP : VBN', 1L, 621L, 7.14954707619e-06, 0.0004870832732]\n",
      "['VBN : VBG', 1L, 584L, 7.14954707619e-06, 0.000458062208613]\n",
      "[', : PRP', 2L, 566L, 1.42990941524e-05, 0.000443943852868]\n",
      "['. : CD', 1L, 562L, 7.14954707619e-06, 0.00044080644048]\n",
      "['DT : NNPS', 10L, 558L, 7.14954707619e-05, 0.000437669028092]\n",
      "['. : NNS', 3L, 552L, 2.14486412286e-05, 0.000432962909511]\n",
      "['VBD : NONE', 2L, 534L, 1.42990941524e-05, 0.000418844553766]\n",
      "['VB : NNS', 10L, 524L, 7.14954707619e-05, 0.000411001022796]\n",
      "['PRP$ : NNS', 9L, 518L, 6.43459236857e-05, 0.000406294904215]\n",
      "['VBG : CD', 3L, 517L, 2.14486412286e-05, 0.000405510551118]\n",
      "[', : VBN', 5L, 508L, 3.5747735381e-05, 0.000398451373245]\n",
      "['PRP$ : CC', 8L, 493L, 5.71963766095e-05, 0.000386686076791]\n",
      "['PRP$ : JJ', 7L, 492L, 5.00468295333e-05, 0.000385901723694]\n",
      "['IN : PRP$', 2L, 491L, 1.42990941524e-05, 0.000385117370597]\n",
      "['DT : JJS', 6L, 475L, 4.28972824572e-05, 0.000372567721046]\n",
      "['NNP : JJS', 2L, 470L, 1.42990941524e-05, 0.000368645955562]\n",
      "['JJS : NNP', 4L, 470L, 2.85981883048e-05, 0.000368645955562]\n",
      "['VBN : VBN', 1L, 444L, 7.14954707619e-06, 0.000348252775041]\n",
      "['NNS : VBG', 1L, 439L, 7.14954707619e-06, 0.000344331009557]\n",
      "['RB : RB', 4L, 429L, 2.85981883048e-05, 0.000336487478587]\n",
      "['NNP : WRB', 4L, 424L, 2.85981883048e-05, 0.000332565713102]\n",
      "['IN : RBS', 1L, 411L, 7.14954707619e-06, 0.000322369122842]\n",
      "['NNPS : NNP', 2L, 410L, 1.42990941524e-05, 0.000321584769745]\n",
      "['NONE : CD', 1L, 402L, 7.14954707619e-06, 0.00031530994497]\n",
      "['VBP : NNS', 5L, 402L, 3.5747735381e-05, 0.00031530994497]\n",
      "['NONE : (', 1L, 401L, 7.14954707619e-06, 0.000314525591873]\n",
      "['CC : PRP$', 5L, 388L, 3.5747735381e-05, 0.000304329001613]\n",
      "['RBS : NN', 2L, 381L, 1.42990941524e-05, 0.000298838529934]\n",
      "['JJ : VB', 5L, 375L, 3.5747735381e-05, 0.000294132411352]\n",
      "['PRP$ : DT', 1L, 372L, 7.14954707619e-06, 0.000291779352062]\n",
      "['WDT : NN', 5L, 351L, 3.5747735381e-05, 0.000275307937026]\n",
      "['PRP$ : TO', 7L, 337L, 5.00468295333e-05, 0.000264326993669]\n",
      "['RBS : IN', 1L, 336L, 7.14954707619e-06, 0.000263542640572]\n",
      "['NNS : VBZ', 3L, 333L, 2.14486412286e-05, 0.000261189581281]\n",
      "['. : RB', 4L, 326L, 2.85981883048e-05, 0.000255699109602]\n",
      "[', : NNS', 2L, 319L, 1.42990941524e-05, 0.000250208637924]\n",
      "['NNP : NNPS', 1L, 308L, 7.14954707619e-06, 0.000241580753857]\n",
      "['VBD : PRP$', 1L, 306L, 7.14954707619e-06, 0.000240012047664]\n",
      "['MD : VB', 2L, 303L, 1.42990941524e-05, 0.000237658988373]\n",
      "['VBD : VBG', 3L, 300L, 2.14486412286e-05, 0.000235305929082]\n",
      "['VBN : RB', 4L, 298L, 2.85981883048e-05, 0.000233737222888]\n",
      "['VBN : TO', 3L, 291L, 2.14486412286e-05, 0.000228246751209]\n",
      "['NONE : DT', 1L, 286L, 7.14954707619e-06, 0.000224324985725]\n",
      "['IN : JJR', 1L, 260L, 7.14954707619e-06, 0.000203931805204]\n",
      "['CC : VB', 8L, 258L, 5.71963766095e-05, 0.00020236309901]\n",
      "['VBD : JJS', 1L, 258L, 7.14954707619e-06, 0.00020236309901]\n",
      "[': : VBZ', 3L, 253L, 2.14486412286e-05, 0.000198441333526]\n",
      "['NNP : MD', 2L, 251L, 1.42990941524e-05, 0.000196872627332]\n",
      "['RB : VBG', 1L, 249L, 7.14954707619e-06, 0.000195303921138]\n",
      "['NNPS : IN', 5L, 248L, 3.5747735381e-05, 0.000194519568041]\n",
      "['TO : RB', 7L, 246L, 5.00468295333e-05, 0.000192950861847]\n",
      "['WRB : NN', 6L, 246L, 4.28972824572e-05, 0.000192950861847]\n",
      "['VB : TO', 4L, 239L, 2.85981883048e-05, 0.000187460390169]\n",
      "['NN : MD', 7L, 232L, 5.00468295333e-05, 0.00018196991849]\n",
      "['FW : NNP', 10L, 231L, 7.14954707619e-05, 0.000181185565393]\n",
      "['NN : JJR', 1L, 221L, 7.14954707619e-06, 0.000173342034424]\n",
      "['VBZ : VBG', 2L, 220L, 1.42990941524e-05, 0.000172557681327]\n",
      "['PRP$ : VBP', 3L, 218L, 2.14486412286e-05, 0.000170988975133]\n",
      "['VB : VB', 6L, 217L, 4.28972824572e-05, 0.000170204622036]\n",
      "['VBP : CD', 1L, 212L, 7.14954707619e-06, 0.000166282856551]\n",
      "['WDT : NNP', 1L, 208L, 7.14954707619e-06, 0.000163145444163]\n",
      "['WRB : NNP', 6L, 207L, 4.28972824572e-05, 0.000162361091067]\n",
      "['DT : PRP$', 3L, 207L, 2.14486412286e-05, 0.000162361091067]\n",
      "['NNP : FW', 3L, 203L, 2.14486412286e-05, 0.000159223678679]\n",
      "['VBZ : NONE', 1L, 196L, 7.14954707619e-06, 0.000153733207]\n",
      "[': : CD', 1L, 190L, 7.14954707619e-06, 0.000149027088419]\n",
      "['RP : NN', 2L, 185L, 1.42990941524e-05, 0.000145105322934]\n",
      "['VB : CD', 3L, 183L, 2.14486412286e-05, 0.00014353661674]\n",
      "['VB : VBN', 2L, 183L, 1.42990941524e-05, 0.00014353661674]\n",
      "['CD : WDT', 8L, 182L, 5.71963766095e-05, 0.000142752263643]\n",
      "['VBG : VBN', 1L, 180L, 7.14954707619e-06, 0.000141183557449]\n",
      "[', : VBG', 9L, 176L, 6.43459236857e-05, 0.000138046145061]\n",
      "['WP : VBD', 2L, 176L, 1.42990941524e-05, 0.000138046145061]\n",
      "['WDT : VBZ', 6L, 174L, 4.28972824572e-05, 0.000136477438868]\n",
      "['VBP : RB', 3L, 172L, 2.14486412286e-05, 0.000134908732674]\n",
      "['JJ : JJS', 4L, 168L, 2.85981883048e-05, 0.000131771320286]\n",
      "['. : VBP', 6L, 168L, 4.28972824572e-05, 0.000131771320286]\n",
      "['VBP : TO', 9L, 167L, 6.43459236857e-05, 0.000130986967189]\n",
      "['TO : VBG', 10L, 161L, 7.14954707619e-05, 0.000126280848607]\n",
      "['VB : RB', 2L, 160L, 1.42990941524e-05, 0.00012549649551]\n",
      "['RBS : JJ', 1L, 158L, 7.14954707619e-06, 0.000123927789316]\n",
      "['JJS : NNS', 6L, 157L, 4.28972824572e-05, 0.00012314343622]\n",
      "['VBG : VBG', 5L, 152L, 3.5747735381e-05, 0.000119221670735]\n",
      "['IN : VB', 3L, 151L, 2.14486412286e-05, 0.000118437317638]\n",
      "['JJS : JJ', 6L, 147L, 4.28972824572e-05, 0.00011529990525]\n",
      "['TO : VB', 8L, 145L, 5.71963766095e-05, 0.000113731199056]\n",
      "['NNP : JJR', 1L, 144L, 7.14954707619e-06, 0.000112946845959]\n",
      "['VB : VBD', 8L, 142L, 5.71963766095e-05, 0.000111378139765]\n",
      "[', : JJS', 3L, 142L, 2.14486412286e-05, 0.000111378139765]\n",
      "['VBD : VBZ', 1L, 140L, 7.14954707619e-06, 0.000109809433572]\n",
      "['RP : NNP', 1L, 140L, 7.14954707619e-06, 0.000109809433572]\n",
      "[', : NONE', 2L, 134L, 1.42990941524e-05, 0.00010510331499]\n",
      "['NNPS : VBD', 3L, 126L, 2.14486412286e-05, 9.88284902144e-05]\n",
      "[': : IN', 5L, 125L, 3.5747735381e-05, 9.80441371175e-05]\n",
      "['. : PRP', 1L, 125L, 7.14954707619e-06, 9.80441371175e-05]\n",
      "['VBG : RB', 5L, 124L, 3.5747735381e-05, 9.72597840205e-05]\n",
      "['MD : NN', 1L, 122L, 7.14954707619e-06, 9.56910778267e-05]\n",
      "['NNPS : NN', 1L, 121L, 7.14954707619e-06, 9.49067247297e-05]\n",
      "['MD : TO', 1L, 120L, 7.14954707619e-06, 9.41223716328e-05]\n",
      "[': : JJ', 6L, 120L, 4.28972824572e-05, 9.41223716328e-05]\n",
      "['VBG : VBD', 10L, 118L, 7.14954707619e-05, 9.25536654389e-05]\n",
      "['PRP$ : VBN', 1L, 117L, 7.14954707619e-06, 9.1769312342e-05]\n",
      "['JJR : NNP', 9L, 116L, 6.43459236857e-05, 9.0984959245e-05]\n",
      "['NNP : EX', 1L, 111L, 7.14954707619e-06, 8.70631937603e-05]\n",
      "['RBR : NN', 2L, 111L, 1.42990941524e-05, 8.70631937603e-05]\n",
      "['CC : RBS', 1L, 111L, 7.14954707619e-06, 8.70631937603e-05]\n",
      "[': : VBD', 3L, 110L, 2.14486412286e-05, 8.62788406634e-05]\n",
      "['JJ : WRB', 5L, 107L, 3.5747735381e-05, 8.39257813726e-05]\n",
      "['PRP : CC', 2L, 104L, 1.42990941524e-05, 8.15727220817e-05]\n",
      "['NN : WRB', 3L, 102L, 2.14486412286e-05, 8.00040158879e-05]\n",
      "['VBG : PRP', 1L, 102L, 7.14954707619e-06, 8.00040158879e-05]\n",
      "['VBP : VB', 4L, 94L, 2.85981883048e-05, 7.37291911123e-05]\n",
      "['VBZ : VBZ', 1L, 89L, 7.14954707619e-06, 6.98074256276e-05]\n",
      "['VB : VBG', 1L, 89L, 7.14954707619e-06, 6.98074256276e-05]\n",
      "['VBG : VBZ', 3L, 89L, 2.14486412286e-05, 6.98074256276e-05]\n",
      "['VBP : VBZ', 1L, 89L, 7.14954707619e-06, 6.98074256276e-05]\n",
      "['VB : PRP', 1L, 86L, 7.14954707619e-06, 6.74543663368e-05]\n",
      "['TO : VBZ', 3L, 85L, 2.14486412286e-05, 6.66700132399e-05]\n",
      "['CC : NNPS', 7L, 85L, 5.00468295333e-05, 6.66700132399e-05]\n",
      "['WDT : PRP', 1L, 84L, 7.14954707619e-06, 6.58856601429e-05]\n",
      "[', : VBP', 4L, 83L, 2.85981883048e-05, 6.5101307046e-05]\n",
      "['IN : EX', 1L, 83L, 7.14954707619e-06, 6.5101307046e-05]\n",
      "['WDT : CD', 1L, 81L, 7.14954707619e-06, 6.35326008521e-05]\n",
      "['VBN : VBZ', 3L, 80L, 2.14486412286e-05, 6.27482477552e-05]\n",
      "['NN : (', 10L, 80L, 7.14954707619e-05, 6.27482477552e-05]\n",
      "['TO : VBD', 9L, 80L, 6.43459236857e-05, 6.27482477552e-05]\n",
      "['$ : NNP', 6L, 79L, 4.28972824572e-05, 6.19638946582e-05]\n",
      "['FW : NN', 8L, 78L, 5.71963766095e-05, 6.11795415613e-05]\n",
      "['VBN : VBD', 10L, 77L, 7.14954707619e-05, 6.03951884644e-05]\n",
      "[': : )', 4L, 73L, 2.85981883048e-05, 5.72577760766e-05]\n",
      "['VBP : VBD', 5L, 73L, 3.5747735381e-05, 5.72577760766e-05]\n",
      "['VBD : NNPS', 1L, 72L, 7.14954707619e-06, 5.64734229797e-05]\n",
      "['NONE : PRP', 1L, 71L, 7.14954707619e-06, 5.56890698827e-05]\n",
      "[') : IN', 1L, 69L, 7.14954707619e-06, 5.41203636888e-05]\n",
      "['NN : PDT', 3L, 69L, 2.14486412286e-05, 5.41203636888e-05]\n",
      "['WRB : NNS', 1L, 68L, 7.14954707619e-06, 5.33360105919e-05]\n",
      "['JJS : CC', 6L, 68L, 4.28972824572e-05, 5.33360105919e-05]\n",
      "['VBD : RBS', 1L, 66L, 7.14954707619e-06, 5.1767304398e-05]\n",
      "['RB : VBP', 1L, 65L, 7.14954707619e-06, 5.09829513011e-05]\n",
      "['DT : JJR', 2L, 64L, 1.42990941524e-05, 5.01985982041e-05]\n",
      "['POS : JJ', 4L, 64L, 2.85981883048e-05, 5.01985982041e-05]\n",
      "['NNS : JJR', 1L, 64L, 7.14954707619e-06, 5.01985982041e-05]\n",
      "['JJ : NNPS', 3L, 62L, 2.14486412286e-05, 4.86298920103e-05]\n",
      "['MD : JJ', 1L, 61L, 7.14954707619e-06, 4.78455389133e-05]\n",
      "['RP : IN', 9L, 60L, 6.43459236857e-05, 4.70611858164e-05]\n",
      "['VBP : VBP', 4L, 56L, 2.85981883048e-05, 4.39237734286e-05]\n",
      "['WP : VBZ', 1L, 55L, 7.14954707619e-06, 4.31394203317e-05]\n",
      "['CD : WP', 1L, 55L, 7.14954707619e-06, 4.31394203317e-05]\n",
      "['TO : NNPS', 1L, 54L, 7.14954707619e-06, 4.23550672347e-05]\n",
      "['MD : NNP', 2L, 53L, 1.42990941524e-05, 4.15707141378e-05]\n",
      "['RBS : NNP', 1L, 51L, 7.14954707619e-06, 4.00020079439e-05]\n",
      "['CC : WRB', 4L, 49L, 2.85981883048e-05, 3.843330175e-05]\n",
      "['JJ : WP$', 2L, 49L, 1.42990941524e-05, 3.843330175e-05]\n",
      "['PRP$ : CD', 1L, 48L, 7.14954707619e-06, 3.76489486531e-05]\n",
      "['WRB : IN', 1L, 48L, 7.14954707619e-06, 3.76489486531e-05]\n",
      "['NNS : WP', 2L, 48L, 1.42990941524e-05, 3.76489486531e-05]\n",
      "['WRB : VBP', 2L, 45L, 1.42990941524e-05, 3.52958893623e-05]\n",
      "['JJR : CC', 4L, 45L, 2.85981883048e-05, 3.52958893623e-05]\n",
      "['VBD : WRB', 1L, 45L, 7.14954707619e-06, 3.52958893623e-05]\n",
      "[') : NN', 1L, 45L, 7.14954707619e-06, 3.52958893623e-05]\n",
      "['CC : MD', 6L, 45L, 4.28972824572e-05, 3.52958893623e-05]\n",
      "['POS : VBZ', 7L, 43L, 5.00468295333e-05, 3.37271831684e-05]\n",
      "['WP$ : NN', 1L, 43L, 7.14954707619e-06, 3.37271831684e-05]\n",
      "['DT : FW', 4L, 42L, 2.85981883048e-05, 3.29428300715e-05]\n",
      "['FW : VBD', 4L, 42L, 2.85981883048e-05, 3.29428300715e-05]\n",
      "['NNPS : NNS', 1L, 41L, 7.14954707619e-06, 3.21584769745e-05]\n",
      "['FW : FW', 1L, 40L, 7.14954707619e-06, 3.13741238776e-05]\n",
      "['JJS : VBD', 3L, 38L, 2.14486412286e-05, 2.98054176837e-05]\n",
      "['VBG : VBP', 2L, 38L, 1.42990941524e-05, 2.98054176837e-05]\n",
      "['VBN : VBP', 1L, 37L, 7.14954707619e-06, 2.90210645868e-05]\n",
      "['NONE : VBN', 2L, 37L, 1.42990941524e-05, 2.90210645868e-05]\n",
      "['WP$ : VBZ', 1L, 37L, 7.14954707619e-06, 2.90210645868e-05]\n",
      "['POS : NNS', 3L, 36L, 2.14486412286e-05, 2.82367114898e-05]\n",
      "['JJR : NNS', 3L, 35L, 2.14486412286e-05, 2.74523583929e-05]\n",
      "['( : VBN', 1L, 34L, 7.14954707619e-06, 2.6668005296e-05]\n",
      "['JJ : FW', 2L, 33L, 1.42990941524e-05, 2.5883652199e-05]\n",
      "[', : WP', 3L, 33L, 2.14486412286e-05, 2.5883652199e-05]\n",
      "['VBN : WP', 1L, 32L, 7.14954707619e-06, 2.50992991021e-05]\n",
      "['VBN : WDT', 2L, 31L, 1.42990941524e-05, 2.43149460051e-05]\n",
      "['WDT : CC', 2L, 30L, 1.42990941524e-05, 2.35305929082e-05]\n",
      "['VBG : WDT', 4L, 29L, 2.85981883048e-05, 2.27462398113e-05]\n",
      "['POS : TO', 3L, 29L, 2.14486412286e-05, 2.27462398113e-05]\n",
      "['TO : WP', 2L, 27L, 1.42990941524e-05, 2.11775336174e-05]\n",
      "['NN : )', 6L, 27L, 4.28972824572e-05, 2.11775336174e-05]\n",
      "['( : RB', 8L, 27L, 5.71963766095e-05, 2.11775336174e-05]\n",
      "[': : VBP', 2L, 26L, 1.42990941524e-05, 2.03931805204e-05]\n",
      "['CD : FW', 5L, 25L, 3.5747735381e-05, 1.96088274235e-05]\n",
      "['CD : (', 5L, 24L, 3.5747735381e-05, 1.88244743266e-05]\n",
      "['RBR : JJ', 1L, 24L, 7.14954707619e-06, 1.88244743266e-05]\n",
      "['VBD : VBP', 1L, 24L, 7.14954707619e-06, 1.88244743266e-05]\n",
      "['VBD : WDT', 1L, 24L, 7.14954707619e-06, 1.88244743266e-05]\n",
      "['POS : DT', 1L, 23L, 7.14954707619e-06, 1.80401212296e-05]\n",
      "['VBD : WP', 1L, 22L, 7.14954707619e-06, 1.72557681327e-05]\n",
      "['DT : RBR', 1L, 21L, 7.14954707619e-06, 1.64714150357e-05]\n",
      "['MD : CC', 2L, 21L, 1.42990941524e-05, 1.64714150357e-05]\n",
      "['POS : CD', 2L, 20L, 1.42990941524e-05, 1.56870619388e-05]\n",
      "['JJS : VBG', 2L, 20L, 1.42990941524e-05, 1.56870619388e-05]\n",
      "['( : VB', 2L, 19L, 1.42990941524e-05, 1.49027088419e-05]\n",
      "['JJR : RB', 2L, 19L, 1.42990941524e-05, 1.49027088419e-05]\n",
      "['TO : WDT', 2L, 19L, 1.42990941524e-05, 1.49027088419e-05]\n",
      "['RB : WP', 2L, 19L, 1.42990941524e-05, 1.49027088419e-05]\n",
      "['JJR : TO', 1L, 19L, 7.14954707619e-06, 1.49027088419e-05]\n",
      "['CD : RBR', 1L, 19L, 7.14954707619e-06, 1.49027088419e-05]\n",
      "['POS : NONE', 1L, 18L, 7.14954707619e-06, 1.41183557449e-05]\n",
      "['JJS : WDT', 2L, 18L, 1.42990941524e-05, 1.41183557449e-05]\n",
      "['WRB : CC', 5L, 18L, 3.5747735381e-05, 1.41183557449e-05]\n",
      "[': : VBN', 1L, 17L, 7.14954707619e-06, 1.3334002648e-05]\n",
      "['IN : WP$', 7L, 17L, 5.00468295333e-05, 1.3334002648e-05]\n",
      "['MD : NNS', 1L, 17L, 7.14954707619e-06, 1.3334002648e-05]\n",
      "['VBN : WRB', 1L, 17L, 7.14954707619e-06, 1.3334002648e-05]\n",
      "['DT : RP', 2L, 17L, 1.42990941524e-05, 1.3334002648e-05]\n",
      "['RBR : NNS', 3L, 17L, 2.14486412286e-05, 1.3334002648e-05]\n",
      "[', : WRB', 4L, 17L, 2.85981883048e-05, 1.3334002648e-05]\n",
      "['VBG : WP', 4L, 17L, 2.85981883048e-05, 1.3334002648e-05]\n",
      "['VBZ : WRB', 2L, 16L, 1.42990941524e-05, 1.2549649551e-05]\n",
      "['NONE : MD', 4L, 16L, 2.85981883048e-05, 1.2549649551e-05]\n",
      "['RP : CC', 1L, 16L, 7.14954707619e-06, 1.2549649551e-05]\n",
      "['CC : WP$', 1L, 16L, 7.14954707619e-06, 1.2549649551e-05]\n",
      "['POS : RB', 3L, 16L, 2.14486412286e-05, 1.2549649551e-05]\n",
      "['DT : WP$', 9L, 15L, 6.43459236857e-05, 1.17652964541e-05]\n",
      "['VBZ : WDT', 5L, 15L, 3.5747735381e-05, 1.17652964541e-05]\n",
      "[': : VB', 1L, 14L, 7.14954707619e-06, 1.09809433572e-05]\n",
      "['VB : VBP', 1L, 14L, 7.14954707619e-06, 1.09809433572e-05]\n",
      "[': : PRP', 1L, 14L, 7.14954707619e-06, 1.09809433572e-05]\n",
      "['VBP : WDT', 1L, 14L, 7.14954707619e-06, 1.09809433572e-05]\n",
      "['WRB : VB', 1L, 13L, 7.14954707619e-06, 1.01965902602e-05]\n",
      "['JJR : PRP', 1L, 13L, 7.14954707619e-06, 1.01965902602e-05]\n",
      "[', : FW', 2L, 12L, 1.42990941524e-05, 9.41223716328e-06]\n",
      "['POS : VBN', 4L, 12L, 2.85981883048e-05, 9.41223716328e-06]\n",
      "['NNS : (', 1L, 12L, 7.14954707619e-06, 9.41223716328e-06]\n",
      "['VBG : WRB', 1L, 11L, 7.14954707619e-06, 8.62788406634e-06]\n",
      "['VBZ : WP', 3L, 10L, 2.14486412286e-05, 7.8435309694e-06]\n",
      "['MD : VBD', 1L, 10L, 7.14954707619e-06, 7.8435309694e-06]\n",
      "['( : VBD', 10L, 9L, 7.14954707619e-05, 7.05917787246e-06]\n",
      "['VBP : WRB', 1L, 8L, 7.14954707619e-06, 6.27482477552e-06]\n",
      "['JJR : WDT', 1L, 8L, 7.14954707619e-06, 6.27482477552e-06]\n",
      "['VBD : :', 8L, 8L, 5.71963766095e-05, 6.27482477552e-06]\n",
      "['PRP : ,', 4L, 8L, 2.85981883048e-05, 6.27482477552e-06]\n",
      "['$ : NN', 5L, 8L, 3.5747735381e-05, 6.27482477552e-06]\n",
      "['CC : FW', 2L, 8L, 1.42990941524e-05, 6.27482477552e-06]\n",
      "['RB : (', 2L, 8L, 1.42990941524e-05, 6.27482477552e-06]\n",
      "['RB : MD', 1L, 7L, 7.14954707619e-06, 5.49047167858e-06]\n",
      "['WRB : VBG', 2L, 7L, 1.42990941524e-05, 5.49047167858e-06]\n",
      "['POS : VBG', 3L, 7L, 2.14486412286e-05, 5.49047167858e-06]\n",
      "['( : DT', 2L, 7L, 1.42990941524e-05, 5.49047167858e-06]\n",
      "['MD : JJR', 1L, 6L, 7.14954707619e-06, 4.70611858164e-06]\n",
      "['( : JJ', 4L, 6L, 2.85981883048e-05, 4.70611858164e-06]\n",
      "['VBD : (', 1L, 6L, 7.14954707619e-06, 4.70611858164e-06]\n",
      "['NNPS : ,', 8L, 6L, 5.71963766095e-05, 4.70611858164e-06]\n",
      "['( : VBP', 7L, 6L, 5.00468295333e-05, 4.70611858164e-06]\n",
      "['TO : (', 4L, 6L, 2.85981883048e-05, 4.70611858164e-06]\n",
      "['NONE : RBS', 4L, 6L, 2.85981883048e-05, 4.70611858164e-06]\n",
      "['VBD : FW', 2L, 5L, 1.42990941524e-05, 3.9217654847e-06]\n",
      "['IN : $', 4L, 5L, 2.85981883048e-05, 3.9217654847e-06]\n",
      "['CD : POS', 6L, 5L, 4.28972824572e-05, 3.9217654847e-06]\n",
      "['VBN : (', 1L, 5L, 7.14954707619e-06, 3.9217654847e-06]\n",
      "['NNS : :', 1L, 5L, 7.14954707619e-06, 3.9217654847e-06]\n",
      "['PRP$ : POS', 1L, 5L, 7.14954707619e-06, 3.9217654847e-06]\n",
      "['PRP$ : EX', 1L, 5L, 7.14954707619e-06, 3.9217654847e-06]\n",
      "['VBP : WP', 5L, 4L, 3.5747735381e-05, 3.13741238776e-06]\n",
      "[', : WP$', 1L, 4L, 7.14954707619e-06, 3.13741238776e-06]\n",
      "['VBN : POS', 1L, 4L, 7.14954707619e-06, 3.13741238776e-06]\n",
      "['MD : WP', 1L, 4L, 7.14954707619e-06, 3.13741238776e-06]\n",
      "['VBP : :', 8L, 3L, 5.71963766095e-05, 2.35305929082e-06]\n",
      "['VBP : )', 4L, 3L, 2.85981883048e-05, 2.35305929082e-06]\n",
      "['VB : :', 1L, 3L, 7.14954707619e-06, 2.35305929082e-06]\n",
      "[') : .', 2L, 3L, 1.42990941524e-05, 2.35305929082e-06]\n",
      "['VBD : )', 3L, 3L, 2.14486412286e-05, 2.35305929082e-06]\n",
      "['( : POS', 3L, 3L, 2.14486412286e-05, 2.35305929082e-06]\n",
      "[\"'' : POS\", 7L, 3L, 5.00468295333e-05, 2.35305929082e-06]\n",
      "[': : POS', 1L, 3L, 7.14954707619e-06, 2.35305929082e-06]\n",
      "['POS : WP', 1L, 3L, 7.14954707619e-06, 2.35305929082e-06]\n",
      "['RB : :', 2L, 3L, 1.42990941524e-05, 2.35305929082e-06]\n",
      "['( : VBG', 2L, 2L, 1.42990941524e-05, 1.56870619388e-06]\n",
      "['POS : WDT', 1L, 2L, 7.14954707619e-06, 1.56870619388e-06]\n",
      "['VBG : (', 1L, 2L, 7.14954707619e-06, 1.56870619388e-06]\n",
      "['VBG : :', 1L, 2L, 7.14954707619e-06, 1.56870619388e-06]\n",
      "['JJR : :', 1L, 2L, 7.14954707619e-06, 1.56870619388e-06]\n",
      "['FW : :', 2L, 2L, 1.42990941524e-05, 1.56870619388e-06]\n",
      "['CD : $', 9L, 2L, 6.43459236857e-05, 1.56870619388e-06]\n",
      "['VBP : (', 2L, 1L, 1.42990941524e-05, 7.8435309694e-07]\n",
      "['JJR : )', 1L, 1L, 7.14954707619e-06, 7.8435309694e-07]\n",
      "['( : NNS', 1L, 1L, 7.14954707619e-06, 7.8435309694e-07]\n",
      "['$ : $', 6L, 1L, 4.28972824572e-05, 7.8435309694e-07]\n",
      "['. : WP', 1L, 1L, 7.14954707619e-06, 7.8435309694e-07]\n",
      "['VBP : POS', 8L, 1L, 5.71963766095e-05, 7.8435309694e-07]\n",
      "[\"POS : ''\", 1L, 1L, 7.14954707619e-06, 7.8435309694e-07]\n",
      "['TO : :', 4L, 1L, 2.85981883048e-05, 7.8435309694e-07]\n",
      "['PRP : .', 5L, 1L, 3.5747735381e-05, 7.8435309694e-07]\n",
      "['VBG : )', 5L, 1L, 3.5747735381e-05, 7.8435309694e-07]\n",
      "['VB : (', 1L, 1L, 7.14954707619e-06, 7.8435309694e-07]\n",
      "['VB : )', 3L, 1L, 2.14486412286e-05, 7.8435309694e-07]\n",
      "['( : TO', 2L, 1L, 1.42990941524e-05, 7.8435309694e-07]\n",
      "['VBN : )', 2L, 1L, 1.42990941524e-05, 7.8435309694e-07]\n",
      "['VBN : :', 7L, 1L, 5.00468295333e-05, 7.8435309694e-07]\n",
      "['WDT : .', 1L, 1L, 7.14954707619e-06, 7.8435309694e-07]\n",
      "['WDT : ,', 4L, 1L, 2.85981883048e-05, 7.8435309694e-07]\n",
      "['VBZ : )', 2L, 1L, 1.42990941524e-05, 7.8435309694e-07]\n",
      "['VBZ : (', 2L, 1L, 1.42990941524e-05, 7.8435309694e-07]\n",
      "['NNS : $', 1L, 1L, 7.14954707619e-06, 7.8435309694e-07]\n",
      "['NNS : POS', 2L, 1L, 1.42990941524e-05, 7.8435309694e-07]\n",
      "['POS : POS', 4L, 1L, 2.85981883048e-05, 7.8435309694e-07]\n",
      "['MD : ,', 6L, 1L, 4.28972824572e-05, 7.8435309694e-07]\n",
      "['WP : .', 1L, 1L, 7.14954707619e-06, 7.8435309694e-07]\n",
      "['FW : ,', 8L, 1L, 5.71963766095e-05, 7.8435309694e-07]\n",
      "['NONE : :', 1L, 1L, 7.14954707619e-06, 7.8435309694e-07]\n",
      "['NONE : WP', 1L, 1L, 7.14954707619e-06, 7.8435309694e-07]\n",
      "0.0093087102932\n",
      "0.225328957689\n"
     ]
    }
   ],
   "source": [
    "\"\"\"(Newer Better) This is to see the difference between pos tags of mentions and non-mentions.\"\"\"\n",
    "\n",
    "from __future__ import division\n",
    "import os\n",
    "from operator import itemgetter\n",
    "\n",
    "posDict = {'mentions':{}, 'non-mentions':{}}\n",
    "\n",
    "typ = 'befaft'\n",
    "\n",
    "with open('/users/cs/amaral/wikisim/wikification/pos-data/pos-mention-' + typ + '.tsv', 'r') as posFile:\n",
    "    content = posFile.readlines()\n",
    "    for line in content:\n",
    "        theLine = line.split('\\t')\n",
    "        posDict['mentions'][theLine[0]] = [long(theLine[1]),float(theLine[2].replace('\\\\n',''))]\n",
    "\n",
    "with open('/users/cs/amaral/wikisim/wikification/pos-data/pos-nonmention-' + typ + '.tsv', 'r') as posFile:\n",
    "    content = posFile.readlines()\n",
    "    for line in content:\n",
    "        theLine = line.split('\\t')\n",
    "        posDict['non-mentions'][theLine[0]] = [long(theLine[1]),float(theLine[2].replace('\\\\n',''))]\n",
    "\n",
    "diffs = []\n",
    "rawDiffs = []\n",
    "mSolos =[]\n",
    "bestDiffs = []\n",
    "for mKey in posDict['mentions'].keys():\n",
    "    if mKey in posDict['non-mentions']:\n",
    "        # find abs diff between \n",
    "        dif = [mKey, abs(posDict['mentions'][mKey][1] - posDict['non-mentions'][mKey][1])]\n",
    "        diffs.append(dif)\n",
    "        dif = [mKey, abs(posDict['mentions'][mKey][0] - posDict['non-mentions'][mKey][0])]\n",
    "        rawDiffs.append(dif)\n",
    "        if posDict['mentions'][mKey][0] <= 10:\n",
    "            bestDiffs.append([mKey, posDict['mentions'][mKey][0], posDict['non-mentions'][mKey][0], \n",
    "                             posDict['mentions'][mKey][1], posDict['non-mentions'][mKey][1]])\n",
    "    else:\n",
    "        # only for mentions\n",
    "        solo = [mKey, posDict['mentions'][mKey][1]]\n",
    "        mSolos.append(solo)\n",
    "nSolos =[]\n",
    "for mKey in posDict['non-mentions'].keys():\n",
    "    if mKey not in posDict['mentions']:\n",
    "        # only for non-mentions\n",
    "        solo = [mKey, posDict['non-mentions'][mKey][0], posDict['non-mentions'][mKey][1]]\n",
    "        nSolos.append(solo)\n",
    "        \n",
    "diffs = sorted(diffs, key = itemgetter(1), reverse = True)\n",
    "rawDiffs = sorted(rawDiffs, key = itemgetter(1), reverse = True)\n",
    "mSolos = sorted(mSolos, key = itemgetter(1), reverse = True)\n",
    "nSolos = sorted(nSolos, key = itemgetter(1), reverse = True)\n",
    "bestDiffs = sorted(bestDiffs, key = itemgetter(2), reverse = True)\n",
    "\n",
    "show = 10\n",
    "\n",
    "print 'Diffs:'\n",
    "for dif in diffs[:show]:\n",
    "    print dif\n",
    "    \n",
    "print '\\nRaw Diffs:'\n",
    "for dif in rawDiffs[:show]:\n",
    "    print dif\n",
    "    \n",
    "print '\\nMention Solos:'\n",
    "print 'len: ', len(mSolos)\n",
    "for solo in mSolos[:show]:\n",
    "    print solo\n",
    "    \n",
    "asum = 0\n",
    "print '\\nNon-Mention Solos:'\n",
    "print 'len: ', len(nSolos)\n",
    "for solo in nSolos[:show]:\n",
    "    print solo\n",
    "    asum += solo[1]\n",
    "\n",
    "print '\\nBest Diffs:'\n",
    "msum = 0\n",
    "nsum = 0\n",
    "for dif in bestDiffs:\n",
    "    print dif\n",
    "    msum += dif[3]\n",
    "    nsum += dif[4]\n",
    "    \n",
    "print msum\n",
    "print nsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diffs:\n",
      "['\"IN : ,\"', 0.092312986]\n",
      "['IN : .', 0.06629444400000001]\n",
      "['DT : .', 0.043011235]\n",
      "['\", : ,\"', 0.037971903]\n",
      "['\"DT : ,\"', 0.034344888000000004]\n",
      "['IN : CC', 0.031946389]\n",
      "['DT : IN', 0.02865779]\n",
      "['NNP : DT', 0.028027932000000002]\n",
      "['IN : IN', 0.027639607000000004]\n",
      "['NN : DT', 0.027124422]\n",
      "\n",
      "Raw Diffs:\n",
      "['NNP : NNP', 16188L]\n",
      "['IN : NNP', 7519L]\n",
      "['NN : NNP', 7500L]\n",
      "['NNP : DT', 7204L]\n",
      "['NN : DT', 6989L]\n",
      "['IN : NN', 6581L]\n",
      "['DT : NN', 5213L]\n",
      "['DT : IN', 4984L]\n",
      "['NNP : IN', 4379L]\n",
      "['JJ : IN', 4047L]\n",
      "\n",
      "Mention Solos:\n",
      "len:  43\n",
      "['CC : POS', 0.000664403]\n",
      "['TO : POS', 0.00059058]\n",
      "['VB : .', 0.000553669]\n",
      "['VBZ : POS', 0.000442935]\n",
      "['\". : ,\"', 0.000332201]\n",
      "['\"WRB : ,\"', 0.00029529]\n",
      "['VB : POS', 0.00029529]\n",
      "['\"POS : ,\"', 0.000258379]\n",
      "['VBG : POS', 0.000258379]\n",
      "['\", : POS\"', 0.000221468]\n",
      "\n",
      "Non-Mention Solos:\n",
      "len:  694\n",
      "['VBN : DT', 1899L, 0.007396963]\n",
      "['NNS : DT', 1567L, 0.00610376]\n",
      "['VBZ : JJ', 1559L, 0.006072599]\n",
      "['NN : CD', 1343L, 0.005231238]\n",
      "['PRP : DT', 1140L, 0.004440515]\n",
      "['PRP : VBN', 976L, 0.003801704]\n",
      "['NNP : PRP$', 669L, 0.002605881]\n",
      "['NN : PRP$', 642L, 0.002500711]\n",
      "['NN : VB', 581L, 0.002263104]\n",
      "['PRP : RB', 539L, 0.002099506]\n",
      "sum: 10915\n",
      "\n",
      "Best Diffs:\n",
      "['NNP : DT', 1L, 7205L, 3.69e-05, 0.028064832]\n",
      "['NN : DT', 3L, 6992L, 0.000110734, 0.027235156]\n",
      "['NNP : CD', 8L, 3977L, 0.00029529, 0.015491164]\n",
      "['CD : CD', 5L, 2644L, 0.000184556, 0.010298878]\n",
      "['NNP : NONE', 1L, 2348L, 3.69e-05, 0.009145902]\n",
      "['CD : DT', 1L, 2233L, 3.69e-05, 0.008697955]\n",
      "['VBD : JJ', 3L, 1959L, 0.000110734, 0.007630674]\n",
      "['VBZ : NN', 9L, 1958L, 0.000332201, 0.007626779]\n",
      "['NN : JJ', 5L, 1860L, 0.000184556, 0.00724505]\n",
      "['NNP : PRP', 1L, 1754L, 3.69e-05, 0.00683216]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nGood ones:\\n'NNP : DT'\\n'NN : DT'\\n'NNP : CD'\\nand pretty much all non-mention solos\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"(Older) This is to see the difference between pos tags of mentions and non-mentions.\"\"\"\n",
    "\n",
    "from __future__ import division\n",
    "import os\n",
    "from operator import itemgetter\n",
    "\n",
    "posDict = {'mentions':{}, 'non-mentions':{}}\n",
    "\n",
    "with open('/users/cs/amaral/wikisim/wikisim/posdata.txt', 'r') as posFile:\n",
    "    content = posFile.readlines()\n",
    "    for line in content[1:]:\n",
    "        theLine = line.split('\\t')\n",
    "        if theLine[0] <> '':\n",
    "            posDict['mentions'][theLine[0]] = [long(theLine[1]),float(theLine[2].replace('\\\\n',''))]\n",
    "        posDict['non-mentions'][theLine[3]] = [long(theLine[4]),float(theLine[5].replace('\\\\n',''))]\n",
    "        \n",
    "diffs = []\n",
    "rawDiffs = []\n",
    "mSolos =[]\n",
    "bestDiffs = []\n",
    "for mKey in posDict['mentions'].keys():\n",
    "    if mKey in posDict['non-mentions']:\n",
    "        # find abs diff between \n",
    "        dif = [mKey, abs(posDict['mentions'][mKey][1] - posDict['non-mentions'][mKey][1])]\n",
    "        diffs.append(dif)\n",
    "        dif = [mKey, abs(posDict['mentions'][mKey][0] - posDict['non-mentions'][mKey][0])]\n",
    "        rawDiffs.append(dif)\n",
    "        if posDict['mentions'][mKey][0] <= 10:\n",
    "            bestDiffs.append([mKey, posDict['mentions'][mKey][0], posDict['non-mentions'][mKey][0], \n",
    "                             posDict['mentions'][mKey][1], posDict['non-mentions'][mKey][1]])\n",
    "    else:\n",
    "        # only for mentions\n",
    "        solo = [mKey, posDict['mentions'][mKey][1]]\n",
    "        mSolos.append(solo)\n",
    "nSolos =[]\n",
    "for mKey in posDict['non-mentions'].keys():\n",
    "    if mKey not in posDict['mentions']:\n",
    "        # only for non-mentions\n",
    "        solo = [mKey, posDict['non-mentions'][mKey][0], posDict['non-mentions'][mKey][1]]\n",
    "        nSolos.append(solo)\n",
    "        \n",
    "diffs = sorted(diffs, key = itemgetter(1), reverse = True)\n",
    "rawDiffs = sorted(rawDiffs, key = itemgetter(1), reverse = True)\n",
    "mSolos = sorted(mSolos, key = itemgetter(1), reverse = True)\n",
    "nSolos = sorted(nSolos, key = itemgetter(1), reverse = True)\n",
    "bestDiffs = sorted(bestDiffs, key = itemgetter(2), reverse = True)\n",
    "\n",
    "show = 10\n",
    "\n",
    "print 'Diffs:'\n",
    "for dif in diffs[:show]:\n",
    "    print dif\n",
    "    \n",
    "print '\\nRaw Diffs:'\n",
    "for dif in rawDiffs[:show]:\n",
    "    print dif\n",
    "    \n",
    "print '\\nMention Solos:'\n",
    "print 'len: ', len(mSolos)\n",
    "for solo in mSolos[:show]:\n",
    "    print solo\n",
    "    \n",
    "asum = 0\n",
    "print '\\nNon-Mention Solos:'\n",
    "print 'len: ', len(nSolos)\n",
    "for solo in nSolos[:show]:\n",
    "    print solo\n",
    "    asum += solo[1]\n",
    "    \n",
    "print 'sum: ' + str(asum)\n",
    "\n",
    "with open('/users/cs/amaral/wikisim/wikification/pos-filter-out-nonmentions.txt', 'w') as resultFile:\n",
    "    # all that appear only for non mentions\n",
    "    for solo in nSolos:\n",
    "        resultFile.write(str(solo[0]) + '\\n')\n",
    "    # some well hand picked ones\n",
    "    resultFile.write(\"NNP : DT\\nNN : DT\\nNNP : CD\\nCD : CD\\nNNP : NONE\")\n",
    "\n",
    "print '\\nBest Diffs:'\n",
    "for dif in bestDiffs[:show]:\n",
    "    print dif\n",
    "\n",
    "\"\"\"\n",
    "Good ones:\n",
    "'NNP : DT'\n",
    "'NN : DT'\n",
    "'NNP : CD'\n",
    "and pretty much all non-mention solos\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Wikification for evaluation purposes\n",
    "\"\"\"\n",
    "\n",
    "from wikipedia import *\n",
    "from operator import itemgetter\n",
    "import requests\n",
    "import json\n",
    "from __future__ import division\n",
    "import nltk\n",
    "import scipy as sp\n",
    "import scipy.sparse as sprs\n",
    "import scipy.spatial\n",
    "import scipy.sparse.linalg\n",
    "from calcsim import *\n",
    "sys.path.append('../')\n",
    "from wsd.wsd import *\n",
    "\n",
    "MIN_MENTION_LENGTH = 3 # mentions must be at least this long\n",
    "MIN_FREQUENCY = 20 # anchor with frequency below is ignored\n",
    "\n",
    "def get_solr_count(s):\n",
    "    \"\"\" Gets the number of documents the string occurs \n",
    "        NOTE: Multi words should be quoted\n",
    "    Arg:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Returns:\n",
    "        The number of documents\n",
    "    \"\"\"\n",
    "\n",
    "    q='+text:(\\\"%s\\\")'%(s,)\n",
    "    qstr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'indent':'on', 'wt':'json', 'q':q, 'rows':0}\n",
    "    r = requests.get(qstr, params=params)\n",
    "    try:\n",
    "        if 'response' not in r.json():\n",
    "            return 0\n",
    "        else:\n",
    "            return r.json()['response']['numFound']\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_mention_count(s):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the amount of times that the given string appears as a mention in wikipedia.\n",
    "    Args:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Return:\n",
    "        The amount of times the given string appears as a mention in wikipedia\n",
    "    \"\"\"\n",
    "    \n",
    "    result = anchor2concept(s)\n",
    "    rSum = 0\n",
    "    for item in result:\n",
    "        rSum += item[1]\n",
    "        \n",
    "    return rSum\n",
    "\n",
    "def mentionProb(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the probability that the text is a mention in Wikipedia.\n",
    "    Args:\n",
    "        text: \n",
    "    Return:\n",
    "        The probability that the text is a mention in Wikipedia.\n",
    "    \"\"\"\n",
    "    \n",
    "    totalMentions = get_mention_count(text)\n",
    "    totalAppearances = get_solr_count(text.replace(\".\", \"\"))\n",
    "    \n",
    "    if totalAppearances == 0:\n",
    "        return 0 # a mention never used probably is not a good link\n",
    "    else:\n",
    "        return totalMentions/totalAppearances\n",
    "\n",
    "def destroyExclusiveOverlaps(textData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Removes all overlaps that start at same letter from text data, so that only the best mention in an\n",
    "        overlap set is left.\n",
    "    Args:\n",
    "        textData: [[start, end, text, anchProb],...]\n",
    "    Return:\n",
    "        textData minus the unesescary elements that overlap.\n",
    "    \"\"\"\n",
    "    \n",
    "    newTextData = [] # textData minus the unesescary parts of the overlapping\n",
    "    overlappingSets = [] # stores arrays of the indexes of overlapping items from textData\n",
    "    \n",
    "    # creates the overlappingSets array\n",
    "    i = 0\n",
    "    while i < len(textData)-1:\n",
    "        # even single elements considered overlapping set\n",
    "        # this is root of overlapping set\n",
    "        overlappingSets.append([i])\n",
    "        overlapIndex = len(overlappingSets) - 1\n",
    "        theBegin = textData[i][0]\n",
    "        \n",
    "        # look at next words untill not overlap\n",
    "        for j in range(i+1, len(textData)):\n",
    "            # if next word starts before endiest one ends\n",
    "            if textData[j][0] == theBegin:\n",
    "                overlappingSets[overlapIndex].append(j)\n",
    "                i = j # make sure not to repeat overlap set\n",
    "            else:\n",
    "                # add final word\n",
    "                if j == len(textData) - 1:\n",
    "                    overlappingSets.append([j])\n",
    "                break\n",
    "        i += 1\n",
    "                    \n",
    "    # get only the best overlapping element of each set\n",
    "    for oSet in overlappingSets:\n",
    "        bestIndex = 0\n",
    "        bestScore = -1\n",
    "        for i in oSet:\n",
    "            score = mentionProb(textData[i][2])\n",
    "            if score > bestScore:\n",
    "                bestScore = score\n",
    "                bestIndex = i\n",
    "        \n",
    "        # put right item in new textData\n",
    "        newTextData.append(textData[bestIndex])\n",
    "        \n",
    "    return newTextData\n",
    "\n",
    "def destroyResidualOverlaps(textData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Removes all overlaps from text data, so that only the best mention in an\n",
    "        overlap set is left.\n",
    "    Args:\n",
    "        textData: [[start, end, text, anchProb],...]\n",
    "    Return:\n",
    "        textData minus the unesescary elements that overlap.\n",
    "    \"\"\"\n",
    "    \n",
    "    newTextData = [] # to be returned\n",
    "    oSet = [] # the set of current overlaps\n",
    "    rootWIndex = 0 # the word to start looking from for finding root word\n",
    "    rEnd = 0 # the end index of the root word\n",
    "    \n",
    "    # keep looping as long as overlaps\n",
    "    while True:\n",
    "        oSet = []\n",
    "        oSet.append(textData[rootWIndex])\n",
    "        for i in range(rootWIndex + 1, len(textData)):\n",
    "            # if cur start before root end\n",
    "            if textData[i][0] < textData[rootWIndex][1]:\n",
    "                oSet.append(textData[i])\n",
    "            else:\n",
    "                break # have all overlap words\n",
    "\n",
    "        \n",
    "        bestIndex = 0\n",
    "        # deal with the overlaps\n",
    "        if len(oSet) > 1:\n",
    "            bestProb = 0\n",
    "            \n",
    "            # choose the most probable\n",
    "            i = 0\n",
    "            for mention in oSet:\n",
    "                prob = mentionProb(mention[2])\n",
    "                if prob > bestProb:\n",
    "                    bestProb = prob\n",
    "                    bestIndex = i\n",
    "                i += 1\n",
    "        else:\n",
    "            rootWIndex += 1 # move up one if no overlaps\n",
    "                \n",
    "        # remove from old text data all that is not best\n",
    "        for i in range(0, len(oSet)):\n",
    "            if i <> bestIndex:\n",
    "                textData.remove(oSet[i])\n",
    "                \n",
    "        # add the best to new\n",
    "        if not (oSet[bestIndex] in newTextData):\n",
    "            newTextData.append(oSet[bestIndex])\n",
    "            \n",
    "        if rootWIndex >= len(textData):\n",
    "            break\n",
    "    \n",
    "    return newTextData\n",
    "    \n",
    "def mentionStartsAndEnds(textData, forTruth = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes in a list of mentions and turns each of its mentions into the form: [wIndex, start, end]. \n",
    "        Or if forTruth is true: [[start,end,entityId]]\n",
    "    Args:\n",
    "        textData: {'text': [w1,w2,w3,...] , 'mentions': [[wordIndex,entityTitle],...]}, to be transformed \n",
    "            as described above.\n",
    "        forTruth: Changes form to use.\n",
    "    Return:\n",
    "        The mentions in the form [[wIndex, start, end],...]]. Or if forTruth is true: [[start,end,entityId]]\n",
    "    \"\"\"\n",
    "    \n",
    "    curWord = 0 \n",
    "    curStart = 0\n",
    "    for mention in textData['mentions']:\n",
    "        while curWord < mention[0]:\n",
    "            curStart += len(textData['text'][curWord]) + 1\n",
    "            curWord += 1\n",
    "            \n",
    "        ent = mention[1] # store entity title in case of forTruth\n",
    "        mention.pop() # get rid of entity text\n",
    "        \n",
    "        if forTruth:\n",
    "            mention.pop() # get rid of wIndex too\n",
    "            \n",
    "        mention.append(curStart) # start of the mention\n",
    "        mention.append(curStart + len(textData['text'][curWord])) # end of the mention\n",
    "        \n",
    "        if forTruth:\n",
    "            mention.append(title2id(ent)) # put on entityId\n",
    "    \n",
    "    return textData['mentions']\n",
    "     \n",
    "def mentionExtract(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes in a text and splits it into the different words/mentions.\n",
    "    Args:\n",
    "        phrase: The text to be split.\n",
    "    Return:\n",
    "        The text split it into the different words / mentions: \n",
    "        {'text':[w1,w2,...], 'mentions': [[wIndex,begin,end],...]}\n",
    "    \"\"\"\n",
    "    \n",
    "    addr = 'http://localhost:8983/solr/enwikianchors20160305/tag'\n",
    "    params={'overlaps':'ALL', 'tagsLimit':'5000', 'fl':'id','wt':'json','indent':'on'}\n",
    "    r = requests.post(addr, params=params, data=text.encode('utf-8'))\n",
    "    textData0 = r.json()['tags']\n",
    "    \n",
    "    splitText = [] # the text now in split form\n",
    "    mentions = [] # mentions before remove inadequate ones\n",
    "    \n",
    "    textData = [] # [[begin,end,word,anchorProb],...]\n",
    "    \n",
    "    i = 0 # for wordIndex\n",
    "    # get rid of extra un-needed Solr data, and add in anchor probability\n",
    "    for item in textData0:\n",
    "        totalMentions = get_mention_count(text[item[1]:item[3]])\n",
    "        totalAppearances = get_solr_count(text[item[1]:item[3]].replace(\".\", \"\"))\n",
    "        if totalAppearances == 0:\n",
    "            anchorProb = 0\n",
    "        else:\n",
    "            anchorProb = totalMentions/totalAppearances\n",
    "        # put in the new clean textData\n",
    "        textData.append([item[1], item[3], text[item[1]:item[3]], anchorProb, i])\n",
    "        i += 1\n",
    "        \n",
    "        # also fill split text\n",
    "        splitText.append(text[item[1]:item[3]])\n",
    "    \n",
    "    # get rid of overlaps\n",
    "    textData = destroyExclusiveOverlaps(textData)\n",
    "    textData = destroyResidualOverlaps(textData)\n",
    "        \n",
    "    # gets the POS labels for the words\n",
    "    postrs = []\n",
    "    for item in textData:\n",
    "        postrs.append(item[2])\n",
    "    postrs = nltk.pos_tag(postrs)\n",
    "    for i in range(0,len(textData)):\n",
    "        textData[i].append(postrs[i]) # [5][1] is index of type of word\n",
    "    \n",
    "    mentionPThrsh = 0.001 # for getting rid of unlikelies\n",
    "    \n",
    "    # put in only good mentions\n",
    "    for item in textData:\n",
    "        if (item[3] >= mentionPThrsh # if popular enough, and either some type of noun or JJ or CD\n",
    "                and (item[5][1][0:2] == 'NN' or item[5][1] == 'JJ' or item[5][1] == 'CD')):\n",
    "            mentions.append([item[4], item[0], item[1]]) # wIndex, start, end\n",
    "    \n",
    "    # get in same format as dataset provided data\n",
    "    newTextData = {'text':splitText, 'mentions':mentions}\n",
    "    \n",
    "    return newTextData\n",
    "\n",
    "def generateCandidates(textData, maxC):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates up to maxC candidates for each possible mention word in phrase.\n",
    "    Args:\n",
    "        textData: A text in split form along with its suspected mentions.\n",
    "        maxC: The max amount of candidates to accept.\n",
    "    Return:\n",
    "        The top maxC candidates for each possible mention word in textData.\n",
    "    \"\"\"\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    for mention in textData['mentions']:\n",
    "        results = sorted(anchor2concept(textData['text'][mention[0]]), key = itemgetter(1), \n",
    "                          reverse = True)\n",
    "        candidates.append(results[:maxC]) # take up to maxC of the results\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def precision(truthSet, mySet):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the precision of mySet against the truthSet.\n",
    "    Args:\n",
    "        truthSet: The 'right' answers for what the entities are. [[start,end,id],...]\n",
    "        mySet: My code's output for what it thinks the right entities are. [[start,end,id],...]\n",
    "    Return:\n",
    "        The precision: (# of correct entities)/(# of found entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numFound = len(mySet)\n",
    "    numCorrect = 0 # incremented in for loop\n",
    "    \n",
    "    truthIndex = 0\n",
    "    myIndex = 0\n",
    "    \n",
    "    while truthIndex < len(truthSet) and myIndex < len(mySet):\n",
    "        if mySet[myIndex][0] < truthSet[truthIndex][0]:\n",
    "            if mySet[myIndex][1] > truthSet[truthIndex][0]:\n",
    "                # overlap with mine behind\n",
    "                if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine not even reach truth\n",
    "                myIndex += 1\n",
    "                \n",
    "        elif mySet[myIndex][0] == truthSet[truthIndex][0]:\n",
    "            # same mention (same start atleast)\n",
    "            if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                numCorrect += 1\n",
    "                truthIndex += 1\n",
    "                myIndex += 1\n",
    "            elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                # truth ends first\n",
    "                truthIndex += 1\n",
    "            else:\n",
    "                # mine ends first\n",
    "                myIndex += 1\n",
    "                  \n",
    "        elif mySet[myIndex][0] > truthSet[truthIndex][0]:\n",
    "            if mySet[myIndex][0] < truthSet[truthIndex][1]:\n",
    "                # overlap with truth behind\n",
    "                if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine beyond mention, increment truth\n",
    "                truthIndex += 1\n",
    "\n",
    "    #print 'correct: ' + str(numCorrect) + '\\nfound: ' + str(numFound)\n",
    "    if numFound == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numFound)\n",
    "\n",
    "def recall(truthSet, mySet):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the recall of mySet against the truthSet.\n",
    "    Args:\n",
    "        truthSet: The 'right' answers for what the entities are. [[start,end,id],...]\n",
    "        mySet: My code's output for what it thinks the right entities are. [[start,end,id],...]\n",
    "    Return:\n",
    "        The recall: (# of correct entities)/(# of actual entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numActual = len(truthSet)\n",
    "    numCorrect = 0 # incremented in for loop)\n",
    "    \n",
    "    truthIndex = 0\n",
    "    myIndex = 0\n",
    "    \n",
    "    while truthIndex < len(truthSet) and myIndex < len(mySet):\n",
    "        if mySet[myIndex][0] < truthSet[truthIndex][0]:\n",
    "            if mySet[myIndex][1] > truthSet[truthIndex][0]:\n",
    "                # overlap with mine behind\n",
    "                if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine not even reach truth\n",
    "                myIndex += 1\n",
    "                \n",
    "        elif mySet[myIndex][0] == truthSet[truthIndex][0]:\n",
    "            # same mention (same start atleast)\n",
    "            if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                numCorrect += 1\n",
    "                truthIndex += 1\n",
    "                myIndex += 1\n",
    "            elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                # truth ends first\n",
    "                truthIndex += 1\n",
    "            else:\n",
    "                # mine ends first\n",
    "                myIndex += 1\n",
    "                  \n",
    "        elif mySet[myIndex][0] > truthSet[truthIndex][0]:\n",
    "            if mySet[myIndex][0] < truthSet[truthIndex][1]:\n",
    "                # overlap with truth behind\n",
    "                if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine beyond mention, increment truth\n",
    "                truthIndex += 1\n",
    "                \n",
    "    if numActual == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numActual)\n",
    "    \n",
    "def getSurroundingWords(text, mIndex, window, asList = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the words surround the given mention. Expanding out window elements\n",
    "        on both sides.\n",
    "    Args:\n",
    "        text: A list of words.\n",
    "        mIndex: The index of the word that is the center of where to get surrounding words.\n",
    "        window: The amount of words to the left and right to get.\n",
    "        asList: Whether to return the words as a list, otherwise just a string.\n",
    "    Return:\n",
    "        The words that surround the given mention. Expanding out window elements\n",
    "        on both sides.\n",
    "    \"\"\"\n",
    "    \n",
    "    imin = mIndex - window\n",
    "    imax = mIndex + window + 1\n",
    "    \n",
    "    # fix extreme bounds\n",
    "    if imin < 0:\n",
    "        imin = 0\n",
    "    if imax > len(text):\n",
    "        imax = len(text)\n",
    "        \n",
    "    if asList == True:\n",
    "        words = (text[imin:mIndex] + text[mIndex+1:imax])\n",
    "    else:\n",
    "        words = \" \".join(text[imin:mIndex] + text[mIndex+1:imax])\n",
    "    \n",
    "    # return surrounding part of word minus the mIndex word\n",
    "    return words\n",
    "\n",
    "def getMentionSentence(text, mention, asList = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the sentence of the mention, minus the mention.\n",
    "    Args:\n",
    "        text: The text to get the sentence from.\n",
    "        index: The mention.\n",
    "        asList: Whether to return the words as a list, otherwise just a string.\n",
    "    Return:\n",
    "        The sentence of the mention, minus the mention.\n",
    "    \"\"\"\n",
    "    \n",
    "    # the start and end indexes of the sentence\n",
    "    sStart = 0\n",
    "    sEnd = 0\n",
    "    \n",
    "    # get sentences using nltk\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # find sentence that mention is in\n",
    "    curLen = 0\n",
    "    for s in sents:\n",
    "        curLen += len(s)\n",
    "        # if greater than begin of mention\n",
    "        if curLen > mention[1]:\n",
    "            # remove mention from string to not get bias from self referencing article\n",
    "            if asList == True:\n",
    "                sentence = (s.replace(text[mention[1]:mention[2]],\"\")).split(\" \")\n",
    "            else:\n",
    "                sentence = s.replace(text[mention[1]:mention[2]],\"\")\n",
    "            \n",
    "            return sentence\n",
    "        \n",
    "    # in case it missed\n",
    "    if asList == True:\n",
    "        return []\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def escapeStringSolr(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Escapes a given string for use in Solr.\n",
    "    Args:\n",
    "        text: The string to escape.\n",
    "    Return:\n",
    "        The escaped text.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = text.replace(\"\\\\\", \"\\\\\\\\\\\\\")\n",
    "    text = text.replace('+', r'\\+')\n",
    "    text = text.replace(\"-\", \"\\-\")\n",
    "    text = text.replace(\"&&\", \"\\&&\")\n",
    "    text = text.replace(\"||\", \"\\||\")\n",
    "    text = text.replace(\"!\", \"\\!\")\n",
    "    text = text.replace(\"(\", \"\\(\")\n",
    "    text = text.replace(\")\", \"\\)\")\n",
    "    text = text.replace(\"{\", \"\\{\")\n",
    "    text = text.replace(\"}\", \"\\}\")\n",
    "    text = text.replace(\"[\", \"\\[\")\n",
    "    text = text.replace(\"]\", \"\\]\")\n",
    "    text = text.replace(\"^\", \"\\^\")\n",
    "    text = text.replace(\"\\\"\", \"\\\\\\\"\")\n",
    "    text = text.replace(\"~\", \"\\~\")\n",
    "    text = text.replace(\"*\", \"\\*\")\n",
    "    text = text.replace(\"?\", \"\\?\")\n",
    "    text = text.replace(\":\", \"\\:\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "def bestRelevancy1Match(mentionStr, context, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the candidate that gives the highest relevance when given the context.\n",
    "    Args:\n",
    "        mentionStr: The mention as it appears in the text\n",
    "        context: The words that surround the target word.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The index of the candidate with the best relevance score from the context.\n",
    "    \"\"\"\n",
    "    \n",
    "    # put text in right format\n",
    "    context = escapeStringSolr(context)\n",
    "    mentionStr = escapeStringSolr(mentionStr)\n",
    "    \n",
    "    strIds = ['id:' +  str(strId[0]) for strId in candidates]\n",
    "    \n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    addr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'fl':'id score', 'fq':\" \".join(strIds), 'indent':'on',\n",
    "            'q':'text:('+context.encode('utf-8')+')^1 title:(' + mentionStr.encode('utf-8')+')^1.35',\n",
    "            'wt':'json'}\n",
    "    r = requests.get(addr, params = params)\n",
    "    \n",
    "    if 'response' not in r.json():\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    if 'docs' not in r.json()['response']:\n",
    "        return 0\n",
    "    \n",
    "    results = r.json()['response']['docs']\n",
    "    if len(results) == 0:\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    bestId = long(r.json()['response']['docs'][0]['id'])\n",
    "    \n",
    "    #for doc in r.json()['response']['docs']:\n",
    "        #print '[' + id2title(doc['id']) + '] -> ' + str(doc['score'])\n",
    "    \n",
    "    # find which index has bestId\n",
    "    bestIndex = 0\n",
    "    for cand in candidates:\n",
    "        if cand[0] == bestId:\n",
    "            return bestIndex\n",
    "        else:\n",
    "            bestIndex += 1\n",
    "            \n",
    "    return bestIndex # in case it was missed\n",
    "\n",
    "def bestRelevancy2Match(context, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the candidate that gives the highest relevance when given the context.\n",
    "    Args:\n",
    "        context: The words that surround the target word.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The index of the candidate with the best relevance score from the context.\n",
    "    \"\"\"\n",
    "    \n",
    "    # put text in right format\n",
    "    context = escapeStringSolr(context)\n",
    "    strIds = ['entityid:' +  str(strId[0]) for strId in candidates]\n",
    "    \n",
    "    # dictionary to hold scores for each id\n",
    "    scoreDict = {}\n",
    "    for cand in candidates:\n",
    "        scoreDict[str(cand[0])] = 0\n",
    "    \n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    addr = 'http://localhost:8983/solr/enwiki20160305_context/select'\n",
    "    params={'fl':'entityid', 'fq':\" \".join(strIds), 'indent':'on',\n",
    "            'q':'_context_:('+context.encode('utf-8')+')',\n",
    "            'wt':'json'}\n",
    "    r = requests.get(addr, params = params)\n",
    "    \n",
    "    if 'response' not in r.json():\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    if 'docs' not in r.json()['response']:\n",
    "        return 0\n",
    "    \n",
    "    results = r.json()['response']['docs']\n",
    "    if len(results) == 0:\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    for doc in r.json()['response']['docs']:\n",
    "        scoreDict[str(doc['entityid'])] += 1\n",
    "    \n",
    "    # get the index that has the best score\n",
    "    bestScore = 0\n",
    "    bestIndex = 0\n",
    "    curIndex = 0\n",
    "    for cand in candidates:\n",
    "        if scoreDict[str(cand[0])] > bestScore:\n",
    "            bestScore = scoreDict[str(cand[0])]\n",
    "            bestIndex = curIndex\n",
    "        curIndex += 1\n",
    "            \n",
    "    return bestIndex\n",
    "\n",
    "def bestWord2VecMatch(context, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses word2vec to find the candidate with the best similarity to the context.\n",
    "    Args:\n",
    "        context: The words that surround the target word as a list.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The index of the candidate with the best similarity score with the context.\n",
    "    \"\"\"\n",
    "    \n",
    "    ctxVec = pd.Series(sp.zeros(300)) # default zero vector\n",
    "    # add all context words together\n",
    "    for word in context:\n",
    "        ctxVec += getword2vector(word)\n",
    "        \n",
    "    # compare context vector to each of the candidates\n",
    "    bestIndex = 0\n",
    "    bestScore = 0\n",
    "    i = 0\n",
    "    for cand in candidates:\n",
    "        eVec = getentity2vector(str(cand[0]))\n",
    "        score = 1-sp.spatial.distance.cosine(ctxVec, eVec)\n",
    "        #print '[' + id2title(cand[0]) + ']' + ' -> ' + str(score)\n",
    "        # update score and index\n",
    "        if score > bestScore: \n",
    "            bestIndex = i\n",
    "            bestScore = score\n",
    "            \n",
    "        i += 1 # next index\n",
    "            \n",
    "    return bestIndex\n",
    "    \n",
    "def wikifyPopular(textData, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the most popular candidate for each mention.\n",
    "    Args:\n",
    "        textData: A text in split form along with its suspected mentions.\n",
    "        candidates: A list of list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidates = []\n",
    "    i = 0 # track which mention's candidates we are looking at\n",
    "    # for each mention choose the top candidate\n",
    "    for mention in textData['mentions']:\n",
    "        if len(candidates[i]) > 0:\n",
    "            topCandidates.append([mention[1], mention[2], candidates[i][0][0]])\n",
    "        i += 1 # move to list of candidates for next mention\n",
    "            \n",
    "    return topCandidates\n",
    "\n",
    "def wikifyRelevancy(textData, candidates, oText, useSentence = False, window = 7, method2 = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the candidate that has the highest relevance with the surrounding window words.\n",
    "    Args:\n",
    "        textData: A textData in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "        oText: The original text to be used for getting sentence.\n",
    "        useSentence: Whether to set use whole sentence as context, or just windowsize.\n",
    "        window: How many words on both sides of a mention to search for context.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidates = []\n",
    "    i = 0 # track which mention's candidates we are looking at\n",
    "    # for each mention choose the top candidate\n",
    "    for mention in textData['mentions']:\n",
    "        if len(candidates[i]) > 0:\n",
    "            if not useSentence:\n",
    "                context = getSurroundingWords(textData['text'], mention[0], window)\n",
    "            else:\n",
    "                context = getMentionSentence(oText, mention)\n",
    "            #print '\\nMention: ' + textData['text'][mention[0]]\n",
    "            #print 'Context: ' + context\n",
    "            if method2 == False:\n",
    "                bestIndex = bestRelevancy1Match(textData['text'][mention[0]], context, candidates[i])\n",
    "            else:\n",
    "                bestIndex = bestRelevancy2Match(context, candidates[i])\n",
    "            topCandidates.append([mention[1], mention[2], candidates[i][bestIndex][0]])\n",
    "        i += 1 # move to list of candidates for next mention\n",
    "        \n",
    "    return topCandidates\n",
    "\n",
    "def wikifyWord2Vec(textData, candidates, oText, useSentence = False, window = 5):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the candidates that have the highest similarity to the context.\n",
    "    Args:\n",
    "        textData: A textData in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "        oText: The original text to be used for getting sentence.\n",
    "        useSentence: Whether to set use whole sentence as context, or just windowsize.\n",
    "        window: How many words on both sides of a mention to search for context.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidates = []\n",
    "    i = 0 # track which mention's candidates we are looking at\n",
    "    # for each mention choose the top candidate\n",
    "    for mention in textData['mentions']:\n",
    "        if len(candidates[i]) > 0:\n",
    "            if not useSentence:\n",
    "                context = getSurroundingWords(textData['text'], mention[0], window, asList = True)\n",
    "            else:\n",
    "                context = getMentionSentence(oText, mention, asList = True)\n",
    "            #print '\\nMention: ' + textData['text'][mention[0]]\n",
    "            #print 'Context: ' + \" \".join(context)\n",
    "            bestIndex = bestWord2VecMatch(context, candidates[i])\n",
    "            topCandidates.append([mention[1], mention[2], candidates[i][bestIndex][0]])\n",
    "        i += 1 # move to list of candidates for next mention\n",
    "        \n",
    "    return topCandidates\n",
    "\n",
    "def wikifyCoherence(textData, candidates, ws = 5):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the candidates that have the highest coherence according to rvs pagerank method.\n",
    "    Args:\n",
    "        textData: A textData in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "        ws: How many words on both sides of a mention to search for context.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    topCands = [] # the top candidate from each candidate list\n",
    "    candsScores = coherence_scores_driver(candidates, ws, method='rvspagerank', direction=DIR_BOTH, op_method=\"keydisamb\")\n",
    "    i = -1 # track what mention we are on\n",
    "    for cScores in candsScores:\n",
    "        i += 1\n",
    "        \n",
    "        if len(cScores) == 0:\n",
    "            continue # nothing to do with this one\n",
    "            \n",
    "        bestScore = sorted(cScores, reverse = True)[0]\n",
    "        curIndex = 0\n",
    "        for score in cScores:\n",
    "            if score == bestScore:\n",
    "                topCands.append([textData['mentions'][i][1], textData['mentions'][i][2], candidates[i][curIndex][0]])\n",
    "                break\n",
    "            curIndex += 1\n",
    "            \n",
    "    return topCands\n",
    "\n",
    "def wikifyEval(text, mentionsGiven, maxC = 20, method='popular', strict = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes the text (maybe text data), and wikifies it for evaluation purposes using the desired method.\n",
    "    Args:\n",
    "        text: The string to wikify. Either as just the original string to be modified, or in the \n",
    "            form of: [[w1,w2,...], [[wid,entityId],...] if the mentions are given.\n",
    "        mentionsGiven: Whether the mentions are given to us and the text is already split.\n",
    "        maxC: The max amount of candidates to extract.\n",
    "        method: The method used to wikify.\n",
    "        strict: Whether to use such rules as minimum metion length, or minimum frequency of concept.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    if not(mentionsGiven): # if words are not in pre-split form\n",
    "        textData = mentionExtract(text) # extract mentions from text\n",
    "        oText = text # the original text\n",
    "    else: # if they are\n",
    "        textData = text\n",
    "        textData['mentions'] = mentionStartsAndEnds(textData) # put mentions in right form\n",
    "        oText = \" \".join(text['text'])\n",
    "    \n",
    "    # get rid of small mentions\n",
    "    if strict:\n",
    "        textData['mentions'] = [item for item in textData['mentions']\n",
    "                    if  len(textData['text'][item[0]]) >= MIN_MENTION_LENGTH]\n",
    "    \n",
    "    candidates = generateCandidates(textData, maxC)\n",
    "    \n",
    "    if method == 'popular':\n",
    "        wikified = wikifyPopular(textData, candidates)\n",
    "    elif method == 'relevancy1':\n",
    "        wikified = wikifyRelevancy(textData, candidates, oText, useSentence = True, window = 7)\n",
    "    elif method == 'relevancy2':\n",
    "        wikified = wikifyRelevancy(textData, candidates, oText, useSentence = True, window = 7, method2 = True)\n",
    "    elif method == 'word2vec':\n",
    "        wikified = wikifyWord2Vec(textData, candidates, oText, useSentence = False, window = 5)\n",
    "    elif method == 'coherence':\n",
    "        wikified = wikifyCoherence(textData, candidates, ws = 5)\n",
    "    \n",
    "    # get rid of very unpopular mentions\n",
    "    if strict:\n",
    "        wikified = [item for item in wikified\n",
    "                    if item[3] >= MIN_FREQUENCY]\n",
    "        \n",
    "    return wikified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wikipedia import *\n",
    "from operator import itemgetter\n",
    "import requests\n",
    "anchor2concept('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id2title(48324759L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing the Solr splitting\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from wikipedia import *\n",
    "from __future__ import division\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode\n",
    "import tagme\n",
    "\n",
    "tagme.GCUBE_TOKEN = \"f6c2ba6c-751b-4977-a94c-c140c30e9b92-843339462\"\n",
    "\n",
    "def mentionProb(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the probability that the text is a mention in Wikipedia.\n",
    "    Args:\n",
    "        text: \n",
    "    Return:\n",
    "        The probability that the text is a mention in Wikipedia.\n",
    "    \"\"\"\n",
    "    \n",
    "    totalMentions = get_mention_count(text)\n",
    "    totalAppearances = get_solr_count(text.replace(\".\", \"\"))\n",
    "    \n",
    "    if totalAppearances == 0:\n",
    "        return 0 # a mention never used probably is not a good link\n",
    "    else:\n",
    "        return totalMentions/totalAppearances\n",
    "\n",
    "def get_solr_count(s):\n",
    "    \"\"\" Gets the number of documents the string occurs \n",
    "        NOTE: Multi words should be quoted\n",
    "    Arg:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Returns:\n",
    "        The number of documents\n",
    "    \"\"\"\n",
    "\n",
    "    q='+text:(\\\"%s\\\")'%(s,)\n",
    "    qstr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'indent':'on', 'wt':'json', 'q':q, 'rows':0}\n",
    "    r = requests.get(qstr, params=params)\n",
    "    try:\n",
    "        if 'response' not in r.json():\n",
    "            return 0\n",
    "        else:\n",
    "            return r.json()['response']['numFound']\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_mention_count(s):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the amount of times that the given string appears as a mention in wikipedia.\n",
    "    Args:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Return:\n",
    "        The amount of times the given string appears as a mention in wikipedia\n",
    "    \"\"\"\n",
    "    \n",
    "    result = anchor2concept(s)\n",
    "    rSum = 0\n",
    "    for item in result:\n",
    "        rSum += item[1]\n",
    "        \n",
    "    return rSum\n",
    "\n",
    "def getTextMentions(line):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A helper method to get the mentions in an evaluable format, includes the mentions'\n",
    "        start and end.\n",
    "    Args:\n",
    "        line: The json data that has info that needs to be converted.\n",
    "    Return:\n",
    "        The mentions in the form [[start, end, text],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    mentions = []\n",
    "    curWord = 0 \n",
    "    curStart = 0\n",
    "    for mention in line['mentions']:\n",
    "        while curWord < mention[0]:\n",
    "            curStart += len(line['text'][curWord]) + 1\n",
    "            curWord += 1\n",
    "        mentions.append([curStart, curStart + len(line['text'][curWord]), line['text'][curWord]])\n",
    "        \n",
    "    return mentions\n",
    "\n",
    "def destroyExclusiveOverlaps(textData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Removes all overlaps from text data, so that only the best mention in an\n",
    "        overlap set is left.\n",
    "    Args:\n",
    "        textData: [[start, end, text, anchProb],...]\n",
    "    Return:\n",
    "        textData minus the unesescary parts of the overlapping\n",
    "    \"\"\"\n",
    "    \n",
    "    newTextData = [] # textData minus the unesescary parts of the overlapping\n",
    "    overlappingSets = [] # stores arrays of the indexes of overlapping items from textData\n",
    "    \n",
    "    # creates the overlappingSets array\n",
    "    i = 0\n",
    "    while i < len(textData)-1:\n",
    "        # even single elements considered overlapping set\n",
    "        # this is root of overlapping set\n",
    "        overlappingSets.append([i])\n",
    "        overlapIndex = len(overlappingSets) - 1\n",
    "        theBegin = textData[i][0]\n",
    "        \n",
    "        # look at next words untill not overlap\n",
    "        for j in range(i+1, len(textData)):\n",
    "            # if next word starts before endiest one ends\n",
    "            if textData[j][0] == theBegin:\n",
    "                overlappingSets[overlapIndex].append(j)\n",
    "                i = j # make sure not to repeat overlap set\n",
    "            else:\n",
    "                # add final word\n",
    "                if j == len(textData) - 1:\n",
    "                    overlappingSets.append([j])\n",
    "                break\n",
    "        i += 1\n",
    "                    \n",
    "    # get only the best overlapping element of each set\n",
    "    for oSet in overlappingSets:\n",
    "        bestIndex = 0\n",
    "        bestScore = -1\n",
    "        for i in oSet:\n",
    "            totalMentions = get_mention_count(textData[i][2])\n",
    "            totalAppearances = get_solr_count(textData[i][2].replace(\".\", \"\"))\n",
    "            if totalAppearances == 0:\n",
    "                score = 0\n",
    "            else:\n",
    "                score = totalMentions/totalAppearances\n",
    "            \n",
    "            if score > bestScore:\n",
    "                bestScore = score\n",
    "                bestIndex = i\n",
    "        \n",
    "        # put right item in new textData\n",
    "        newTextData.append(textData[bestIndex])\n",
    "        \n",
    "    return newTextData\n",
    "\n",
    "def destroyResidualOverlaps(textData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Removes all overlaps from text data, so that only the best mention in an\n",
    "        overlap set is left.\n",
    "    Args:\n",
    "        textData: [[start, end, text, anchProb],...]\n",
    "    Return:\n",
    "        textData minus the unesescary elements that overlap.\n",
    "    \"\"\"\n",
    "    \n",
    "    newTextData = [] # to be returned\n",
    "    oSet = [] # the set of current overlaps\n",
    "    rootWIndex = 0 # the word to start looking from for finding root word\n",
    "    rEnd = 0 # the end index of the root word\n",
    "    \n",
    "    # keep looping as long as overlaps\n",
    "    while True:\n",
    "        oSet = []\n",
    "        oSet.append(textData[rootWIndex])\n",
    "        for i in range(rootWIndex + 1, len(textData)):\n",
    "            # if cur start before root end\n",
    "            if textData[i][0] < textData[rootWIndex][1]:\n",
    "                oSet.append(textData[i])\n",
    "            else:\n",
    "                break # have all overlap words\n",
    "\n",
    "        \n",
    "        bestIndex = 0\n",
    "        # deal with the overlaps\n",
    "        if len(oSet) > 1:\n",
    "            bestProb = 0\n",
    "            \n",
    "            # choose the most probable\n",
    "            i = 0\n",
    "            for mention in oSet:\n",
    "                prob = mentionProb(mention[2])\n",
    "                if prob > bestProb:\n",
    "                    bestProb = prob\n",
    "                    bestIndex = i\n",
    "                i += 1\n",
    "        else:\n",
    "            rootWIndex += 1 # move up one if no overlaps\n",
    "                \n",
    "        # remove from old text data all that is not best\n",
    "        for i in range(0, len(oSet)):\n",
    "            if i <> bestIndex:\n",
    "                textData.remove(oSet[i])\n",
    "                \n",
    "        # add the best to new\n",
    "        if not (oSet[bestIndex] in newTextData):\n",
    "            newTextData.append(oSet[bestIndex])\n",
    "            \n",
    "        if rootWIndex >= len(textData):\n",
    "            break\n",
    "    \n",
    "    return newTextData\n",
    "\n",
    "def destroyInclusiveOverlaps(text, textData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Removes all overlaps from text data, so that only the best mention in an\n",
    "        overlap set is left.\n",
    "    Args:\n",
    "        text: The source text.\n",
    "        textData: [[_, start, _, end, _, alts, pop],...]\n",
    "    Return:\n",
    "        textData minus the unesescary parts of the overlapping\n",
    "    \"\"\"\n",
    "    \n",
    "    newTextData = [] # textData minus the unesescary parts of the overlapping\n",
    "    overlappingSets = [] # stores arrays of the indexes of overlapping items from textData\n",
    "    \n",
    "    # creates the overlappingSets array\n",
    "    i = 0\n",
    "    while i < len(textData)-1:\n",
    "        # even single elements considered overlapping set\n",
    "        # this is root of overlapping set\n",
    "        overlappingSets.append([i])\n",
    "        overlapIndex = len(overlappingSets) - 1\n",
    "        endiestEnd = textData[i][1]\n",
    "        \n",
    "        # look at next words untill not overlap\n",
    "        for j in range(i+1, len(textData)):\n",
    "            # if next word starts before endiest one ends\n",
    "            if textData[j][0] < endiestEnd:\n",
    "                overlappingSets[overlapIndex].append(j)\n",
    "                i = j # make sure not to repeat overlap set\n",
    "                # update endiness if needed\n",
    "                if textData[j][1] > endiestEnd:\n",
    "                    endiestEnd = textData[j][1]\n",
    "            else:\n",
    "                # add final word\n",
    "                if j == len(textData) - 1:\n",
    "                    overlappingSets.append([j])\n",
    "                break\n",
    "        i += 1\n",
    "                    \n",
    "    # get only the best overlapping element of each set\n",
    "    for oSet in overlappingSets:\n",
    "        bestIndex = 0\n",
    "        bestScore = -1\n",
    "        for i in oSet:\n",
    "            totalMentions = get_mention_count(textData[i][2])\n",
    "            totalAppearances = get_solr_count(textData[i][2].replace(\".\", \"\"))\n",
    "            if totalAppearances == 0:\n",
    "                score = 0\n",
    "            else:\n",
    "                score = totalMentions/totalAppearances\n",
    "            \n",
    "            if score > bestScore:\n",
    "                bestScore = score\n",
    "                bestIndex = i\n",
    "        \n",
    "        # put right item in new textData\n",
    "        newTextData.append(textData[bestIndex])\n",
    "        \n",
    "    return newTextData\n",
    "                    \n",
    "def getSolrMentions(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A method to split the text and try to extract mentions using Solr.\n",
    "    Args:\n",
    "        text: The text to find mentions in.\n",
    "    Return:\n",
    "        The mentions as found from our method using Solr.\n",
    "    \"\"\"\n",
    "    \n",
    "    addr = 'http://localhost:8983/solr/enwikianchors20160305/tag'\n",
    "    params={'overlaps':'ALL', 'tagsLimit':'5000', 'fl':'id','wt':'json','indent':'on'}\n",
    "    r = requests.post(addr, params=params, data=text.encode('utf-8'))\n",
    "    textData0 = r.json()['tags']\n",
    "    \n",
    "    textData = []\n",
    "    # get rid of extra un-needed Solr data, and add in anchor probability\n",
    "    for item in textData0:\n",
    "        totalMentions = get_mention_count(text[item[1]:item[3]])\n",
    "        totalAppearances = get_solr_count(text[item[1]:item[3]].replace(\".\", \"\"))\n",
    "        if totalAppearances == 0:\n",
    "            anchorProb = 0\n",
    "        else:\n",
    "            anchorProb = totalMentions/totalAppearances\n",
    "        \n",
    "        # put in the new clean textData\n",
    "        textData.append([item[1], item[3], text[item[1]:item[3]], anchorProb])\n",
    "    \n",
    "    textData = destroyExclusiveOverlaps(textData)\n",
    "    textData = destroyResidualOverlaps(textData)\n",
    "    \n",
    "    # gets the POS labels for the words\n",
    "    postrs = []\n",
    "    for item in textData:\n",
    "        postrs.append(item[2])\n",
    "    postrs = nltk.pos_tag(postrs)\n",
    "    for i in range(0,len(textData)):\n",
    "        textData[i].append(postrs[i]) # [4][1] is index of type of word\n",
    "    \n",
    "    mentions = []\n",
    "    mentionPThrsh = 0.001\n",
    "    \n",
    "    for item in textData:\n",
    "        if (item[3] >= mentionPThrsh\n",
    "                and (item[4][1][0:2] == 'NN' or item[4][1] == 'JJ')):\n",
    "            mentions.append([item[0], item[1], item[2]])\n",
    "    \n",
    "    return mentions\n",
    "\n",
    "def precision(trueMentions, otherMentions):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the precision of otherSet against the trueMentions.\n",
    "    Args:\n",
    "        trueMentions: The 'right' answers for what the mentions are.\n",
    "        otherMentions: Our mentions obtained through some means.\n",
    "    Return:\n",
    "        The precision: (# of correct mentions)/(# of found mentions)\n",
    "    \"\"\"\n",
    "    \n",
    "    numFound = len(otherMentions)\n",
    "    numCorrect = 0 # incremented in for loop\n",
    "    \n",
    "    trueIndex = 0\n",
    "    otherIndex = 0\n",
    "    \n",
    "    while trueIndex < len(trueMentions) and otherIndex < len(otherMentions):\n",
    "        # if mentions start and end on the same\n",
    "        if (trueMentions[trueIndex][0] == otherMentions[otherIndex][0]\n",
    "               and trueMentions[trueIndex][1] == otherMentions[otherIndex][1]):\n",
    "            #print ('MATCH: [' + str(trueMentions[trueIndex][0]) + ',' + str(trueMentions[trueIndex][1]) + ']' + trueMentions[trueIndex][2] \n",
    "            #       + ' <===> [' + str(otherMentions[otherIndex][0]) + ',' + str(otherMentions[otherIndex][1]) + ']' + otherMentions[otherIndex][2])\n",
    "            numCorrect += 1\n",
    "            trueIndex += 1\n",
    "            otherIndex += 1\n",
    "        # if true mention starts before the other starts\n",
    "        elif trueMentions[trueIndex][0] < otherMentions[otherIndex][0]:\n",
    "            #print ('FAIL: [' + str(trueMentions[trueIndex][0]) + ',' + str(trueMentions[trueIndex][1]) + ']' + trueMentions[trueIndex][2] \n",
    "            #       + ' <XXX> [' + str(otherMentions[otherIndex][0]) + ',' + str(otherMentions[otherIndex][1]) + ']' + otherMentions[otherIndex][2])\n",
    "            trueIndex += 1\n",
    "        # if other mention starts before the true starts (same doesnt matter)\n",
    "        elif trueMentions[trueIndex][0] >= otherMentions[otherIndex][0]:\n",
    "            #print ('FAIL: [' + str(trueMentions[trueIndex][0]) + ',' + str(trueMentions[trueIndex][1]) + ']' + trueMentions[trueIndex][2] \n",
    "            #       + ' <XXX> [' + str(otherMentions[otherIndex][0]) + ',' + str(otherMentions[otherIndex][1]) + ']' + otherMentions[otherIndex][2])\n",
    "            otherIndex += 1\n",
    "        else:\n",
    "            print 'AAAAAAAHHHHHHHHHHHHHHHHHHHHHHHHHHHHH!!!!!!!!!!!!!!!!!!!'\n",
    "\n",
    "    #print 'correct: ' + str(numCorrect) + '\\nfound: ' + str(numFound)\n",
    "    if numFound == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numFound)\n",
    "\n",
    "def recall(trueMentions, otherMentions):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the recall of otherSet against the trueMentions.\n",
    "    Args:\n",
    "        trueMentions: The 'right' answers for what the mentions are.\n",
    "        otherMentions: Our mentions obtained through some means.\n",
    "    Return:\n",
    "        The recall: (# of correct entities)/(# of actual entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numActual = len(trueMentions)\n",
    "    numCorrect = 0 # incremented in for loop)\n",
    "    \n",
    "    trueIndex = 0\n",
    "    otherIndex = 0\n",
    "    \n",
    "    while trueIndex < len(trueMentions) and otherIndex < len(otherMentions):\n",
    "        # if mentions start and end on the same\n",
    "        if (trueMentions[trueIndex][0] == otherMentions[otherIndex][0]\n",
    "               and trueMentions[trueIndex][1] == otherMentions[otherIndex][1]):\n",
    "            numCorrect += 1\n",
    "            trueIndex += 1\n",
    "            otherIndex += 1\n",
    "        # if true mention starts before the other starts\n",
    "        elif trueMentions[trueIndex][0] < otherMentions[otherIndex][0]:\n",
    "            trueIndex += 1\n",
    "        # if other mention starts before the true starts (same doesnt matter)\n",
    "        elif trueMentions[trueIndex][0] >= otherMentions[otherIndex][0]:\n",
    "            otherIndex += 1\n",
    "        \n",
    "    print 'correct: ' + str(numCorrect) + '\\nactual: ' + str(numActual)\n",
    "    if numActual == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numActual)\n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "\n",
    "performances = {}\n",
    "\n",
    "# for each dataset, run all methods\n",
    "for dataset in datasets:\n",
    "    performances[dataset['name']] = {}\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "    \n",
    "    # reset counters\n",
    "    totalPrec = 0\n",
    "    totalRec = 0\n",
    "    totalLines = 0\n",
    "\n",
    "    # each method tests all lines\n",
    "    for line in dataLines:\n",
    "\n",
    "        print str(totalLines + 1)\n",
    "\n",
    "        trueMentions = getTextMentions(line)\n",
    "        solrMentions = getSolrMentions(\" \".join(line['text']))\n",
    "        \n",
    "        \"\"\"solrMentions0 = tagme.mentions(\" \".join(line['text']))\n",
    "        solrMentions = []\n",
    "        for item in solrMentions0.mentions:\n",
    "            solrMentions.append([item.begin, item.end, item.mention])\"\"\"\n",
    "\n",
    "        prec = precision(trueMentions, solrMentions)\n",
    "        rec = recall(trueMentions, solrMentions)\n",
    "        print str(prec) + ' ' + str(rec) + '\\n'\n",
    "\n",
    "        # track results\n",
    "        totalPrec += prec\n",
    "        totalRec += rec\n",
    "        totalLines += 1\n",
    "\n",
    "    # record results for this method on this dataset\n",
    "    performances[dataset['name']] = {'Precision':totalPrec/totalLines, \n",
    "                                     'Recall':totalRec/totalLines}\n",
    "            \n",
    "print performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = \"Dalhousie University\"\n",
    "\n",
    "print get_mention_count(text)\n",
    "print get_solr_count(text)\n",
    "\n",
    "print get_mention_count(text)/get_solr_count(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```curl -X POST \\\n",
    "  'http://localhost:8983/solr/geonames/tag?overlaps=NO_SUB&tagsLimit=5000&fl=id,name,countrycode&wt=json&indent=on' \\\n",
    "  -H 'Content-Type:text/plain' -d 'Hello New York City'```\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is for testing performance of different wikification methods.\n",
    "\"\"\"\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import tagme\n",
    "\n",
    "tagme.GCUBE_TOKEN = \"f6c2ba6c-751b-4977-a94c-c140c30e9b92-843339462\"\n",
    "    \n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "#datasets = [{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')},{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')},{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "datasets = [{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "\n",
    "methods = ['popular', 'relevancy1', 'relevancy2', 'word2vec', 'coherence']\n",
    "\n",
    "#if 'word2vec' in methods: # run in different cell\n",
    "    #word2vec = gensim_loadmodel('/users/cs/amaral/cgmdir/word2vecfiles/word2vec.enwiki-20160305-replace_surface.1.0.500.5.5.15.5.5')\n",
    "\n",
    "performances = {}\n",
    "\n",
    "# for each dataset, run all methods\n",
    "for dataset in datasets:\n",
    "    performances[dataset['name']] = {}\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    \n",
    "    # put in all lines that contain proper ascii\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        \n",
    "    print dataset['name'] + '\\n'\n",
    "    \n",
    "    # run each method on the data set\n",
    "    for mthd in methods:\n",
    "        print mthd\n",
    "        print str(datetime.now()) + '\\n'\n",
    "        \n",
    "        # reset counters\n",
    "        totalPrecS = 0\n",
    "        totalPrecM = 0\n",
    "        totalRecS = 0\n",
    "        totalRecM = 0\n",
    "        totalLines = 0\n",
    "        \n",
    "        # each method tests all lines\n",
    "        for line in dataLines:\n",
    "            \n",
    "            print str(totalLines + 1)\n",
    "            \n",
    "            # original split string with mentions given\n",
    "            #resultS = wikifyEval(copy.deepcopy(line), True, maxC = 50, method = mthd)\n",
    "            resultS = []\n",
    "            # unsplit string to be manually split and mentions found\n",
    "            resultM = wikifyEval(\" \".join(line['text']), False, maxC = 50, method = mthd)\n",
    "            # get absolute text indexes and entity id of each given mention\n",
    "            trueEntities = mentionStartsAndEnds(copy.deepcopy(line), forTruth = True) # the ground truth\n",
    "                \n",
    "            ## get statistical results from true entities and results S and M\n",
    "            #precS = precision(trueEntities, resultS) # precision of pre-split\n",
    "            precS = 0\n",
    "            precM = precision(trueEntities, resultM) # precision of manual split\n",
    "            #recS = recall(trueEntities, resultS) # recall of pre-split\n",
    "            recS = 0\n",
    "            recM = recall(trueEntities, resultM) # recall of manual split\n",
    "            \n",
    "            #clear_output() # delete this after\n",
    "            #print str(precS) + ' ' + str(precM) + ' ' + str(recS) + ' ' + str(recM) + '\\n'\n",
    "            print str(precM) + ' ' + str(recM) + '\\n'\n",
    "            #print str(precS) + ' ' + str(recS) + '\\n'\n",
    "            \n",
    "            # track results\n",
    "            totalPrecS += precS\n",
    "            totalPrecM += precM\n",
    "            totalRecS += recS\n",
    "            totalRecM += recM\n",
    "            totalLines += 1\n",
    "        \n",
    "        # record results for this method on this dataset\n",
    "        # [avg precision split, avg precision manual, avg recall split, avg recall manual]\n",
    "        performances[dataset['name']][mthd] = {'S Prec':totalPrecS/totalLines, \n",
    "                                               'M Prec':totalPrecM/totalLines,\n",
    "                                              'S Rec':totalRecS/totalLines, \n",
    "                                               'M Rec':totalRecM/totalLines\n",
    "                                              }\n",
    "        \n",
    "clear_output()\n",
    "print performances\n",
    "for dataset in datasets:\n",
    "    print dataset['name']\n",
    "    for mthd in methods:\n",
    "        print (mthd + ':'\n",
    "               + '\\n    S Prec:' + str(performances[dataset['name']][mthd]['S Prec'])\n",
    "               + '\\n    S Rec:' + str(performances[dataset['name']][mthd]['S Rec'])\n",
    "               + '\\n    M Prec:' + str(performances[dataset['name']][mthd]['M Prec'])\n",
    "               + '\\n    M Rec:' + str(performances[dataset['name']][mthd]['M Rec']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2vec = gensim_loadmodel('/users/cs/amaral/cgmdir/WikipediaClean5Negative300Skip10.Ehsan/WikipediaClean5Negative300Skip10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is for testing performance of TagMe wikification method.\n",
    "\"\"\"\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import tagme\n",
    "\n",
    "tagme.GCUBE_TOKEN = \"f6c2ba6c-751b-4977-a94c-c140c30e9b92-843339462\"\n",
    "    \n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "datasets = [{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "\n",
    "performances = {}\n",
    "\n",
    "# for each dataset, run all methods\n",
    "for dataset in datasets:\n",
    "    performances[dataset['name']] = {}\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    \n",
    "    # put in all lines that contain proper ascii\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        \n",
    "    print dataset['name'] + '\\n'\n",
    "\n",
    "    print str(datetime.now()) + '\\n'\n",
    "\n",
    "    # reset counters\n",
    "    totalPrecM = 0\n",
    "    totalRecM = 0\n",
    "    totalLines = 0\n",
    "\n",
    "    # each method tests all lines\n",
    "    for line in dataLines:\n",
    "\n",
    "        print str(totalLines + 1)\n",
    "        \n",
    "        antns = tagme.annotate(\" \".join(line['text']))\n",
    "        resultM = []\n",
    "        for an in antns.get_annotations(0.005):\n",
    "            resultM.append([an.begin,an.end,title2id(an.entity_title)])\n",
    "        trueEntities = mentionStartsAndEnds(copy.deepcopy(line), forTruth = True) # the ground truth\n",
    "\n",
    "        ## get statistical results from true entities and results\n",
    "        precM = precision(trueEntities, resultM)\n",
    "        recM = recall(trueEntities, resultM)\n",
    "\n",
    "        #clear_output() # delete this after\n",
    "        print str(precM) + ' ' + str(recM) + '\\n'\n",
    "        #print str(precS) + ' ' + str(recS)\n",
    "\n",
    "        # track results\n",
    "        totalPrecM += precM\n",
    "        totalRecM += recM\n",
    "        totalLines += 1\n",
    "\n",
    "    # record results for this method on this dataset\n",
    "    # [avg precision split, avg precision manual, avg recall split, avg recall manual]\n",
    "    performances[dataset['name']] = {'Prec':totalPrecM/totalLines,\n",
    "                                          'Rec':totalRecM/totalLines}\n",
    "            \n",
    "clear_output()\n",
    "print performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Try to compare the difference between mentions where the entity is the most popular and when it is not\"\"\"\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import tagme\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generateCandidates(textData, maxC):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates up to maxC candidates for each possible mention word in phrase (most frequent).\n",
    "    Args:\n",
    "        textData: A text in split form along with its suspected mentions.\n",
    "        maxC: The max amount of candidates to accept.\n",
    "    Return:\n",
    "        The top maxC candidates for each possible mention word in textData.\n",
    "    \"\"\"\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    for mention in textData['mentions']:\n",
    "        results = sorted(anchor2concept(textData['text'][mention[0]]), key = itemgetter(1), \n",
    "                          reverse = True)\n",
    "        candidates.append(results[:maxC]) # take up to maxC of the results\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "\n",
    "maxC = 1\n",
    "corCandPosDict = {} # dictionary containing the amount of correct entities found at each index\n",
    "correctPositions = []\n",
    "incorrectPositions = []\n",
    "correctRelevancies = []\n",
    "incorrectRelevancies = []\n",
    "for i in range(0, maxC):\n",
    "    corCandPosDict[str(i)] = 0\n",
    "\n",
    "totalMentions = 0\n",
    "\n",
    "jdata = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    \n",
    "    # put in all lines\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        \n",
    "    print dataset['name'] + '\\n'\n",
    "    print str(datetime.now()) + '\\n'\n",
    "    totalLines = 0\n",
    "    \n",
    "    for line in dataLines: \n",
    "        print str(totalLines + 1)\n",
    "        \n",
    "        totalMentions += len(line['mentions'])\n",
    "        \n",
    "        oLine = copy.deepcopy(line)\n",
    "        \n",
    "        oMentions = mentionStartsAndEnds(copy.deepcopy(line), forTruth = True)\n",
    "        \n",
    "        # get in right format\n",
    "        line['mentions'] = mentionStartsAndEnds(line) # put mentions in right form\n",
    "        oText = \" \".join(line['text'])\n",
    "            \n",
    "        candss = generateCandidates(line, maxC)\n",
    "        \n",
    "        copyCands = []\n",
    "        i = 0\n",
    "        for cands in candss:\n",
    "            if len(cands) > 0 and cands[0][0] != oMentions[i][2]:\n",
    "                copyCands.append(i)\n",
    "            i += 1\n",
    "            \n",
    "        # put in file\n",
    "        if len(copyCands) > 0:\n",
    "            jdata.append({'text':oLine['text'], 'mentions':[\n",
    "                oLine['mentions'][i] for i in copyCands]})\n",
    "        \n",
    "        totalLines += 1\n",
    "        \n",
    "        #print jdata\n",
    "        \n",
    "clear_output()\n",
    "with open('/users/cs/amaral/wsd-datasets/nopop.json', 'w') as f:\n",
    "    json.dump(jdata, f)\n",
    "    \n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with open('/users/cs/amaral/wsd-datasets/nopop.json', 'w') as f:\n",
    "    for item in jdata:\n",
    "        json.dump(item, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posRels = []\n",
    "for i in range(0, len(correctPositions)):\n",
    "    posRels.append([correctPositions[i], relevancies[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "##### https://stackoverflow.com/questions/470690/how-to-automatically-generate-n-distinct-colors\n",
    "##### Uri Cohen\n",
    "import colorsys\n",
    "def _get_colors(num_colors):\n",
    "    colors=[]\n",
    "    for i in np.arange(0., 360., 360. / num_colors):\n",
    "        hue = i/360.\n",
    "        lightness = (50 + np.random.rand() * 10)/100.\n",
    "        saturation = (90 + np.random.rand() * 10)/100.\n",
    "        colors.append(colorsys.hls_to_rgb(hue, lightness, saturation))\n",
    "    return colors\n",
    "#####\n",
    "#####\n",
    "\n",
    "maxC = 50\n",
    "df = pickle.load(open('/users/cs/amaral/wikisim/storage/correct-pos-frequencies.pkl', 'rb'))\n",
    "\n",
    "# https://chrisalbon.com/python/matplotlib_percentage_stacked_bar_plot.html\n",
    "f, ax = plt.subplots(1, figsize =(20,10))\n",
    "bar_width = 1\n",
    "bar_l = [i for i in range(len(df['freq_lvl']))]\n",
    "tick_pos = [i+(bar_width/2) for i in bar_l]\n",
    "\n",
    "# get the total at each level\n",
    "totals = []\n",
    "totalIndex = -1\n",
    "for i in range(0,len(df['freq_lvl'])):\n",
    "    totals.append(0)\n",
    "    totalIndex += 1\n",
    "    for j in range(0,maxC):\n",
    "        totals[totalIndex] += df['rank: ' + str(j)][i]\n",
    "        \n",
    "# get the percentage of each rank at each level\n",
    "data_rel = []\n",
    "drIndex = -1\n",
    "for i in range(0,maxC): # for each rank\n",
    "    data_rel.append([])\n",
    "    drIndex += 1\n",
    "    for j in range(0,len(df['freq_lvl'])): # for each level\n",
    "        data_rel[drIndex].append((df['rank: ' + str(i)][j] / totals[j]) * 100)\n",
    "        \n",
    "colors = _get_colors(maxC)\n",
    "\n",
    "# get numbers for stacking bars\n",
    "stackNums = []\n",
    "stackNums.append([0]*len(df['freq_lvl'])) # last all zero\n",
    "for i in range(1,maxC):\n",
    "    stackNums.append(copy.deepcopy(stackNums[i - 1]))\n",
    "    for j in range(0,len(df['freq_lvl'])):\n",
    "        # add all of current column\n",
    "        stackNums[i][j] += (df['rank: '+str(maxC - i)][j] / totals[j]) * 100\n",
    "\n",
    "for i in range(0, maxC):\n",
    "    ax.bar(bar_l,\n",
    "          data_rel[i],\n",
    "          bottom=stackNums[maxC - i - 1],\n",
    "          label='Rank: ' + str(i),\n",
    "          alpha=1,\n",
    "          color=colors[i],\n",
    "          width=bar_width,\n",
    "          edgecolor='white')\n",
    "    \n",
    "plt.xticks(tick_pos, df['freq_lvl'])\n",
    "ax.set_ylabel('Percentage')\n",
    "ax.set_xlabel('Relevancy Threshold')\n",
    "plt.xlim([min(tick_pos)-bar_width, max(tick_pos)])\n",
    "plt.ylim(-5, 105)\n",
    "\n",
    "red_patch = mpatches.Patch(color='red', label='The red data')\n",
    "\n",
    "\n",
    "plt.legend(handles=[mpatches.Patch(color=colors[i], label='Rank: '+str(i)) for i in range(0,10)])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://chrisalbon.com/python/matplotlib_percentage_stacked_bar_plot.html\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "raw_data = {'freq_lvl': [i for i in range(300,-1,-10)]} # all the freq levels\n",
    "\n",
    "for i in range(0, maxC):\n",
    "    raw_data['rank: ' + str(i)] = []\n",
    "\n",
    "# get the frequency of each rank at each freq level\n",
    "for lvl in raw_data['freq_lvl']:\n",
    "    for i in range(0, maxC):\n",
    "        raw_data['rank: ' + str(i)].append(len([1 for posRel in posRels \n",
    "                                       if posRel[0] == i and posRel[1] > lvl]))\n",
    "        \n",
    "df = pd.DataFrame(raw_data, columns = ['first_name']\n",
    "                  .extend(['rank: ' + str(i) for i in range(0, maxC)]))\n",
    "with open('/users/cs/amaral/wikisim/storage/correct-pos-frequencies.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)\n",
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "fig.suptitle('Relevancy Score for Candidates at given rank', fontsize=32)\n",
    "plt.xlabel('Candidate Popularity Rank (zero based)', fontsize=18)\n",
    "plt.ylabel('Relevancy Score (from containing sentence)', fontsize=18)\n",
    "plt.plot(incorrectPositions, incorrectRelevancies, 'ro')\n",
    "plt.plot(correctPositions, correctRelevancies, 'bo')\n",
    "plt.legend(handles=[mpatches.Patch(color='red', label='Incorrect'), mpatches.Patch(color='blue', label='Correct')])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"This is for seeing how often correct entity shows up in candidates\"\"\"\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import tagme\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def generateCandidates1(textData, maxC, oText):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates up to maxC candidates for each possible mention word in phrase.\n",
    "    Args:\n",
    "        textData: A text in split form along with its suspected mentions.\n",
    "        maxC: The max amount of candidates to accept.\n",
    "    Return:\n",
    "        The top maxC candidates for each possible mention word in textData.\n",
    "    \"\"\"\n",
    "    \n",
    "    candidates = []\n",
    "    #print textData['text']\n",
    "    for mention in textData['mentions']:\n",
    "        \n",
    "        # get all concepts for the anchor\n",
    "        concepts = anchor2concept(textData['text'][mention[0]])\n",
    "        \n",
    "        # get the ids as string for solr query\n",
    "        strIds = ['id:' +  str(strId[0]) for strId in concepts]\n",
    "        \n",
    "        context = getMentionSentence(oText, mention)\n",
    "        context = escapeStringSolr(context)\n",
    "        mentionStr = escapeStringSolr(textData['text'][mention[0]])\n",
    "        \n",
    "        # gets the relevancy scores of all of the given potential concepts\n",
    "        addr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "        params={'fl':'id score', 'indent':'on', 'start': '0', 'rows': str(maxC),\n",
    "                'fq':\" \".join(strIds),\n",
    "                'q':'text:('+context.encode('utf-8')+')^1 title:(' + mentionStr.encode('utf-8')+')^1.35',\n",
    "                'wt':'json'}\n",
    "        r = requests.get(addr, params = params)\n",
    "        \n",
    "        solrRes = []\n",
    "        try:\n",
    "            if not ('response' not in r.json()\n",
    "                   or 'docs' not in r.json()['response']\n",
    "                   or len(r.json()['response']['docs']) == 0):\n",
    "                for doc in r.json()['response']['docs'][:maxC]:\n",
    "                    freq = 0\n",
    "                    for concept in concepts:\n",
    "                        # find concept frequency\n",
    "                        if concept[0] == int(doc['id']):\n",
    "                            freq = concept[1]\n",
    "                    solrRes.append([long(doc['id']), freq, doc['score']])\n",
    "        except:\n",
    "            solrRes = []\n",
    "                \n",
    "        # sort by frequency\n",
    "        solrRes = sorted(solrRes, key = itemgetter(1), reverse = True)\n",
    "        \n",
    "        #print '\\nMention: ' + textData['text'][mention[0]]\n",
    "        #for res in solrRes:\n",
    "        #    print '[' + id2title(res[0]) + '] -> freq: ' + str(res[1]) + ', rel: ' + str(res[2])\n",
    "        \n",
    "        candidates.append(solrRes) # take up to maxC of the results\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def generateCandidates2(textData, maxC):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates up to maxC candidates for each possible mention word in phrase (most frequent).\n",
    "    Args:\n",
    "        textData: A text in split form along with its suspected mentions.\n",
    "        maxC: The max amount of candidates to accept.\n",
    "    Return:\n",
    "        The top maxC candidates for each possible mention word in textData.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    \n",
    "    for mention in textData['mentions']:\n",
    "        results = sorted(anchor2concept(textData['text'][mention[0]]), key = itemgetter(1), \n",
    "                          reverse = True)\n",
    "        candidates.append(results[:maxC]) # take up to maxC of the results\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def generateCandidates3(textData, maxC):\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    for mention in textData['mentions']:\n",
    "        anchors = anchor2concept(textData['text'][mention[0]])\n",
    "        entities = []\n",
    "        \n",
    "        for anchor in anchors:\n",
    "            wanchors = id2anchor(anchor[0]) # get all anchors of the id in this anchor\n",
    "            totalFreq = 0\n",
    "            for wanchor in wanchors:\n",
    "                totalFreq += wanchor[1]\n",
    "            \n",
    "            entities.append([anchor[0], totalFreq])\n",
    "        \n",
    "        results = sorted(entities, key = itemgetter(1), reverse = True)\n",
    "        \n",
    "        candidates.append(results[:maxC]) # take up to maxC of the results\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def generateCandidates5(textData, maxC):\n",
    "    \n",
    "    candidates = []\n",
    "    #print textData['text']\n",
    "    for mention in textData['mentions']:\n",
    "        \n",
    "        # get all concepts for the anchor\n",
    "        concepts = anchor2concept(textData['text'][mention[0]])\n",
    "        \n",
    "        # get the ids as string for solr query\n",
    "        strIds = ['id:' +  str(strId[0]) for strId in concepts]\n",
    "        \n",
    "        context = []\n",
    "        \n",
    "        for mention2 in textData['mentions']:\n",
    "            if mention2 <> mention:\n",
    "                context += escapeStringSolr(textData['text'][mention2[0]])\n",
    "        context = \" \".join(context)\n",
    "        mentionStr = escapeStringSolr(textData['text'][mention[0]])\n",
    "        \n",
    "        # gets the relevancy scores of all of the given potential concepts\n",
    "        addr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "        params={'fl':'id score', 'indent':'on', 'start': '0', 'rows': str(maxC),\n",
    "                'fq':\" \".join(strIds),\n",
    "                'q':'text:('+context.encode('utf-8')+')^1 title:(' + mentionStr.encode('utf-8')+')^1.35',\n",
    "                'wt':'json'}\n",
    "        r = requests.get(addr, params = params)\n",
    "        \n",
    "        solrRes = []\n",
    "        try:\n",
    "            if not ('response' not in r.json()\n",
    "                   or 'docs' not in r.json()['response']\n",
    "                   or len(r.json()['response']['docs']) == 0):\n",
    "                for doc in r.json()['response']['docs'][:maxC]:\n",
    "                    freq = 0\n",
    "                    for concept in concepts:\n",
    "                        # find concept frequency\n",
    "                        if concept[0] == int(doc['id']):\n",
    "                            freq = concept[1]\n",
    "                    solrRes.append([long(doc['id']), freq, doc['score']])\n",
    "        except:\n",
    "            solrRes = []\n",
    "        \n",
    "        #print '\\nMention: ' + textData['text'][mention[0]]\n",
    "        #for res in solrRes:\n",
    "        #    print '[' + id2title(res[0]) + '] -> freq: ' + str(res[1]) + ', rel: ' + str(res[2])\n",
    "        \n",
    "        candidates.append(solrRes) # take up to maxC of the results\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "\n",
    "maxC = 20\n",
    "correctCands1 = {} # dictionary containing the amount of correct entities found at each index\n",
    "correctCands2 = {} # dictionary containing the amount of correct entities found at each index\n",
    "correctCands3 = {} # dictionary containing the amount of correct entities found at each index\n",
    "correctCands4 = {} # dictionary containing the amount of correct entities found at each index\n",
    "correctCands5 = {} # dictionary containing the amount of correct entities found at each index\n",
    "for i in range(-1, maxC):\n",
    "    correctCands1[str(i)] = 0\n",
    "    correctCands2[str(i)] = 0\n",
    "    correctCands3[str(i)] = 0\n",
    "    correctCands4[str(i)] = 0\n",
    "    correctCands5[str(i)] = 0\n",
    "\n",
    "totalMentions = 0\n",
    "\n",
    "for dataset in datasets:\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    \n",
    "    # put in all lines\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        \n",
    "    print dataset['name'] + '\\n'\n",
    "    print str(datetime.now()) + '\\n'\n",
    "    totalLines = 0\n",
    "    \n",
    "    for line in dataLines: \n",
    "        print str(totalLines + 1)\n",
    "        \n",
    "        totalMentions += len(line['mentions'])\n",
    "        \n",
    "        oMentions = mentionStartsAndEnds(copy.deepcopy(line), forTruth = True)\n",
    "        \n",
    "        # get in right format\n",
    "        line['mentions'] = mentionStartsAndEnds(line) # put mentions in right form\n",
    "        oText = \" \".join(line['text'])\n",
    "            \n",
    "        cands1 = generateCandidates1(line, maxC, oText)\n",
    "        cands2 = generateCandidates2(line, maxC)\n",
    "        cands3 = generateCandidates3(line, maxC)\n",
    "        cands4 = []\n",
    "        for cands in cands1:\n",
    "            cands4.append(sorted(cands, key = itemgetter(2), reverse = True))\n",
    "        cands5 = generateCandidates5(line, maxC)\n",
    "        \n",
    "        i = 0\n",
    "        for cand in cands1:\n",
    "            corCand = False\n",
    "            for j in range(0, len(cand)):\n",
    "                if cand[j][0] == oMentions[i][2]:\n",
    "                    correctCands1[str(j)] += 1\n",
    "                    corCand = True\n",
    "                    break\n",
    "            if corCand == False:\n",
    "                correctCands1['-1'] += 1\n",
    "            i += 1\n",
    "            \n",
    "        i = 0\n",
    "        for cand in cands2:\n",
    "            corCand = False\n",
    "            for j in range(0, len(cand)):\n",
    "                if cand[j][0] == oMentions[i][2]:\n",
    "                    correctCands2[str(j)] += 1\n",
    "                    corCand = True\n",
    "                    break\n",
    "            if corCand == False:\n",
    "                correctCands2['-1'] += 1\n",
    "            i += 1\n",
    "            \n",
    "        i = 0\n",
    "        for cand in cands3:\n",
    "            corCand = False\n",
    "            for j in range(0, len(cand)):\n",
    "                if cand[j][0] == oMentions[i][2]:\n",
    "                    correctCands3[str(j)] += 1\n",
    "                    corCand = True\n",
    "                    break\n",
    "            if corCand == False:\n",
    "                correctCands3['-1'] += 1\n",
    "            i += 1\n",
    "            \n",
    "        i = 0\n",
    "        for cand in cands4:\n",
    "            corCand = False\n",
    "            for j in range(0, len(cand)):\n",
    "                if cand[j][0] == oMentions[i][2]:\n",
    "                    correctCands4[str(j)] += 1\n",
    "                    corCand = True\n",
    "                    break\n",
    "            if corCand == False:\n",
    "                correctCands4['-1'] += 1\n",
    "            i += 1\n",
    "            \n",
    "        i = 0\n",
    "        for cand in cands5:\n",
    "            corCand = False\n",
    "            for j in range(0, len(cand)):\n",
    "                if cand[j][0] == oMentions[i][2]:\n",
    "                    correctCands5[str(j)] += 1\n",
    "                    corCand = True\n",
    "                    break\n",
    "            if corCand == False:\n",
    "                correctCands5['-1'] += 1\n",
    "            i += 1\n",
    "            \n",
    "            \n",
    "        totalLines += 1\n",
    "        \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "candsPopRel = []\n",
    "for i in range(-1,maxC):\n",
    "    candsPopRel.append(correctCands1[str(i)])\n",
    "    \n",
    "candsPop = []\n",
    "for i in range(-1,maxC):\n",
    "    candsPop.append(correctCands2[str(i)])\n",
    "    \n",
    "candsPopPop = []\n",
    "for i in range(-1,maxC):\n",
    "    candsPopPop.append(correctCands3[str(i)])\n",
    "    \n",
    "candsRelSentence = []\n",
    "for i in range(-1,maxC):\n",
    "    candsRelSentence.append(correctCands4[str(i)])\n",
    "    \n",
    "candsRelMentions = []\n",
    "for i in range(-1,maxC):\n",
    "    candsRelMentions.append(correctCands5[str(i)])\n",
    "    \n",
    "x = range(-1, maxC)\n",
    "\n",
    "print 'Total Mentions: ' + str(totalMentions)\n",
    "\n",
    "\n",
    "plt.bar(x, candsPopRel, 0.5, color='red')\n",
    "plt.xlabel('Most Popular of most Relevant Rank')\n",
    "plt.ylabel('Correct Entities')\n",
    "plt.title('Correct Entities at given Rank')\n",
    "plt.show()\n",
    "print 'Misplaced: ' + str(correctCands1[str(-1)])\n",
    "print str(candsPopRel) + '\\n\\n'\n",
    "\n",
    "\n",
    "plt.bar(x, candsPop, 0.5, color='orange')\n",
    "plt.xlabel('Popularity v1 Rank')\n",
    "plt.ylabel('Correct Entities')\n",
    "plt.title('Correct Entities at given Rank')\n",
    "plt.show()\n",
    "print 'Misplaced: ' + str(correctCands2[str(-1)])\n",
    "print str(candsPop) + '\\n\\n'\n",
    "\n",
    "\n",
    "plt.bar(x, candsPopPop, 0.5, color='green')\n",
    "plt.xlabel('Popularity v2 Rank')\n",
    "plt.ylabel('Correct Entities')\n",
    "plt.title('Correct Entities at given Rank')\n",
    "plt.show()\n",
    "print 'Misplaced: ' + str(correctCands3[str(-1)])\n",
    "print str(candsPopPop) + '\\n\\n'\n",
    "\n",
    "\n",
    "plt.bar(x, candsRelSentence, 0.5, color='blue')\n",
    "plt.xlabel('Sentence Relevancy Rank')\n",
    "plt.ylabel('Correct Entities')\n",
    "plt.title('Correct Entities at given Rank')\n",
    "plt.show()\n",
    "print 'Misplaced: ' + str(correctCands4[str(-1)])\n",
    "print str(candsRelSentence) + '\\n\\n'\n",
    "\n",
    "\n",
    "plt.bar(x, candsRelMentions, 0.5, color='purple')\n",
    "plt.xlabel('Mentions Relevancy Rank')\n",
    "plt.ylabel('Correct Entities')\n",
    "plt.title('Correct Entities at given Rank')\n",
    "plt.show()\n",
    "print 'Misplaced: ' + str(correctCands5[str(-1)])\n",
    "print str(candsRelMentions) + '\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "concepts = anchor2concept('Tiger')\n",
    "concepts = sorted(concepts, key = itemgetter(1), reverse = True)[:50]\n",
    "for concept in concepts:\n",
    "    print 'id:' + str(concept[0]) + ' ' + id2title(concept[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PRP$': 1, 'VBG': 26, 'VBD': 17, 'VBN': 60, 'VBP': 7, 'WDT': 1, 'JJ': 1084, 'VBZ': 17, 'DT': 2, '$': 1, 'NN': 1946, 'FW': 2, 'POS': 1, '.': 2, 'PRP': 2, 'RB': 15, 'NNS': 569, 'NNP': 22726, 'VB': 31, 'CC': 1, 'CD': 347, 'IN': 4, 'MD': 2, 'NNPS': 212, 'JJS': 11, 'JJR': 2, 'SYM': 3}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Test some of the POS stuff\"\"\"\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import tagme\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "\n",
    "totalMentions = 0\n",
    "\n",
    "posDict = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    \n",
    "    # put in all lines\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        \n",
    "    print dataset['name'] + '\\n'\n",
    "    print str(datetime.now()) + '\\n'\n",
    "    \n",
    "    totalLines = 0\n",
    "    for line in dataLines: \n",
    "        print str(totalLines + 1)\n",
    "        totalMentions += len(line['mentions'])\n",
    "        pos = nltk.pos_tag(line['text'])\n",
    "        for mnt in line['mentions']:\n",
    "            if str(pos[mnt[0]][1]) not in posDict:\n",
    "                posDict[str(pos[mnt[0]][1])] = 1\n",
    "            else:\n",
    "                posDict[str(pos[mnt[0]][1])] += 1\n",
    "                \n",
    "        totalLines += 1\n",
    "        \n",
    "clear_output()\n",
    "print posDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
