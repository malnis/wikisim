{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diffs:\n",
      "['\"IN : ,\"', 0.092312986]\n",
      "['IN : .', 0.06629444400000001]\n",
      "['DT : .', 0.043011235]\n",
      "['\", : ,\"', 0.037971903]\n",
      "['\"DT : ,\"', 0.034344888000000004]\n",
      "['IN : CC', 0.031946389]\n",
      "['DT : IN', 0.02865779]\n",
      "['NNP : DT', 0.028027932000000002]\n",
      "['IN : IN', 0.027639607000000004]\n",
      "['NN : DT', 0.027124422]\n",
      "\n",
      "Raw Diffs:\n",
      "['NNP : NNP', 16188L]\n",
      "['IN : NNP', 7519L]\n",
      "['NN : NNP', 7500L]\n",
      "['NNP : DT', 7204L]\n",
      "['NN : DT', 6989L]\n",
      "['IN : NN', 6581L]\n",
      "['DT : NN', 5213L]\n",
      "['DT : IN', 4984L]\n",
      "['NNP : IN', 4379L]\n",
      "['JJ : IN', 4047L]\n",
      "\n",
      "Mention Solos:\n",
      "len:  43\n",
      "['CC : POS', 0.000664403]\n",
      "['TO : POS', 0.00059058]\n",
      "['VB : .', 0.000553669]\n",
      "['VBZ : POS', 0.000442935]\n",
      "['\". : ,\"', 0.000332201]\n",
      "['\"WRB : ,\"', 0.00029529]\n",
      "['VB : POS', 0.00029529]\n",
      "['\"POS : ,\"', 0.000258379]\n",
      "['VBG : POS', 0.000258379]\n",
      "['\", : POS\"', 0.000221468]\n",
      "\n",
      "Non-Mention Solos:\n",
      "len:  694\n",
      "['VBN : DT', 1899L, 0.007396963]\n",
      "['NNS : DT', 1567L, 0.00610376]\n",
      "['VBZ : JJ', 1559L, 0.006072599]\n",
      "['NN : CD', 1343L, 0.005231238]\n",
      "['PRP : DT', 1140L, 0.004440515]\n",
      "['PRP : VBN', 976L, 0.003801704]\n",
      "['NNP : PRP$', 669L, 0.002605881]\n",
      "['NN : PRP$', 642L, 0.002500711]\n",
      "['NN : VB', 581L, 0.002263104]\n",
      "['PRP : RB', 539L, 0.002099506]\n",
      "sum: 10915\n",
      "\n",
      "Best Diffs:\n",
      "['NNP : DT', 1L, 7205L, 3.69e-05, 0.028064832]\n",
      "['NN : DT', 3L, 6992L, 0.000110734, 0.027235156]\n",
      "['NNP : CD', 8L, 3977L, 0.00029529, 0.015491164]\n",
      "['CD : CD', 5L, 2644L, 0.000184556, 0.010298878]\n",
      "['NNP : NONE', 1L, 2348L, 3.69e-05, 0.009145902]\n",
      "['CD : DT', 1L, 2233L, 3.69e-05, 0.008697955]\n",
      "['VBD : JJ', 3L, 1959L, 0.000110734, 0.007630674]\n",
      "['VBZ : NN', 9L, 1958L, 0.000332201, 0.007626779]\n",
      "['NN : JJ', 5L, 1860L, 0.000184556, 0.00724505]\n",
      "['NNP : PRP', 1L, 1754L, 3.69e-05, 0.00683216]\n",
      "['VBN : NNP', 4L, 1686L, 0.000147645, 0.006567287]\n",
      "['IN : CD', 10L, 1487L, 0.000369113, 0.005792145]\n",
      "['NNP : VBN', 6L, 1373L, 0.000221468, 0.005348094]\n",
      "['NNP : RB', 3L, 1282L, 0.000110734, 0.004993631]\n",
      "['NNP : JJ', 8L, 1232L, 0.00029529, 0.004798872]\n",
      "['NNS : NNP', 7L, 1214L, 0.000258379, 0.004728759]\n",
      "['NN : VBN', 4L, 1055L, 0.000147645, 0.004109424]\n",
      "['VBD : DT', 6L, 868L, 0.000221468, 0.003381023]\n",
      "['TO : NN', 6L, 749L, 0.000221468, 0.002917496]\n",
      "['NNS : IN', 5L, 720L, 0.000184556, 0.002804536]\n",
      "['RB : DT', 1L, 715L, 3.69e-05, 0.00278506]\n",
      "['JJ : DT', 2L, 705L, 7.38e-05, 0.002746108]\n",
      "['CC : JJ', 6L, 676L, 0.000221468, 0.002633147]\n",
      "['NN : NNS', 2L, 659L, 7.38e-05, 0.002566929]\n",
      "['. : NN', 3L, 645L, 0.000110734, 0.002512396]\n",
      "['CC : DT', 5L, 642L, 0.000184556, 0.002500711]\n",
      "['NNP : VBG', 10L, 605L, 0.000369113, 0.002356589]\n",
      "['RB : NNP', 9L, 593L, 0.000332201, 0.002309847]\n",
      "['\", : DT\"', 2L, 591L, 7.38e-05, 0.002302056]\n",
      "['IN : NONE', 2L, 565L, 7.38e-05, 0.002200781]\n",
      "['VBN : NN', 10L, 545L, 0.000369113, 0.002122878]\n",
      "['VB : NN', 10L, 542L, 0.000369113, 0.002111192]\n",
      "['NNS : JJ', 1L, 531L, 3.69e-05, 0.002068345]\n",
      "['VBG : NN', 10L, 517L, 0.000369113, 0.002013812]\n",
      "['VBZ : DT', 1L, 502L, 3.69e-05, 0.001955385]\n",
      "['NNS : NN', 7L, 501L, 0.000258379, 0.001951489]\n",
      "['TO : DT', 3L, 486L, 0.000110734, 0.001893062]\n",
      "['PRP : IN', 4L, 474L, 0.000147645, 0.001846319]\n",
      "['VBN : JJ', 3L, 473L, 0.000110734, 0.001842424]\n",
      "['VBZ : VBN', 1L, 470L, 3.69e-05, 0.001830738]\n",
      "['NNP : NNS', 9L, 449L, 0.000332201, 0.00174894]\n",
      "['NNP : TO', 9L, 449L, 0.000332201, 0.00174894]\n",
      "['NN : VBG', 4L, 436L, 0.000147645, 0.001698302]\n",
      "['VBD : CD', 6L, 435L, 0.000221468, 0.001694407]\n",
      "['PRP$ : NNP', 4L, 401L, 0.000147645, 0.00156197]\n",
      "['VBG : NNP', 3L, 388L, 0.000110734, 0.001511333]\n",
      "['TO : JJ', 7L, 385L, 0.000258379, 0.001499647]\n",
      "['VBD : VBN', 3L, 365L, 0.000110734, 0.001421744]\n",
      "['VBG : DT', 2L, 360L, 7.38e-05, 0.001402268]\n",
      "['RB : JJ', 3L, 352L, 0.000110734, 0.001371106]\n",
      "['CD : VBZ', 6L, 350L, 0.000221468, 0.001363316]\n",
      "['NNP : VB', 3L, 342L, 0.000110734, 0.001332154]\n",
      "['DT : DT', 10L, 337L, 0.000369113, 0.001312678]\n",
      "['NNP : WDT', 6L, 330L, 0.000221468, 0.001285412]\n",
      "['VBD : NNS', 6L, 323L, 0.000221468, 0.001258146]\n",
      "['JJ : PRP', 1L, 320L, 3.69e-05, 0.00124646]\n",
      "['VBD : TO', 9L, 318L, 0.000332201, 0.00123867]\n",
      "['DT : NONE', 1L, 310L, 3.69e-05, 0.001207508]\n",
      "['. : JJ', 1L, 309L, 3.69e-05, 0.001203613]\n",
      "['CC : VBN', 2L, 308L, 7.38e-05, 0.001199718]\n",
      "['VB : JJ', 4L, 299L, 0.000147645, 0.001164661]\n",
      "['RB : TO', 1L, 295L, 3.69e-05, 0.001149081]\n",
      "['JJ : VBG', 10L, 292L, 0.000369113, 0.001137395]\n",
      "['NONE : NN', 4L, 283L, 0.000147645, 0.001102338]\n",
      "['VBG : JJ', 3L, 263L, 0.000110734, 0.001024435]\n",
      "['\", : JJ\"', 3L, 263L, 0.000110734, 0.001024435]\n",
      "['JJ : RB', 5L, 262L, 0.000184556, 0.001020539]\n",
      "['CC : RB', 2L, 261L, 7.38e-05, 0.001016644]\n",
      "['NNS : CD', 1L, 251L, 3.69e-05, 0.000977692]\n",
      "['JJS : IN', 9L, 246L, 0.000332201, 0.000958216]\n",
      "['NNP : WP', 10L, 245L, 0.000369113, 0.000954321]\n",
      "['VBP : IN', 9L, 241L, 0.000332201, 0.00093874]\n",
      "['VBP : NN', 8L, 241L, 0.00029529, 0.00093874]\n",
      "['PRP : JJ', 1L, 231L, 3.69e-05, 0.000899788]\n",
      "['CC : PRP', 3L, 229L, 0.000110734, 0.000891998]\n",
      "['\", : VBZ\"', 8L, 229L, 0.00029529, 0.000891998]\n",
      "['VBZ : CD', 1L, 223L, 3.69e-05, 0.000868627]\n",
      "['NNS : VBD', 3L, 219L, 0.000110734, 0.000853046]\n",
      "['CD : VBN', 5L, 219L, 0.000184556, 0.000853046]\n",
      "['PRP$ : VBD', 2L, 210L, 7.38e-05, 0.00081799]\n",
      "['NN : VBP', 7L, 204L, 0.000258379, 0.000794618]\n",
      "['. : NNP', 3L, 201L, 0.000110734, 0.000782933]\n",
      "['JJ : CD', 1L, 189L, 3.69e-05, 0.000736191]\n",
      "['VBP : JJ', 3L, 185L, 0.000110734, 0.00072061]\n",
      "['RB : NNS', 2L, 173L, 7.38e-05, 0.000673868]\n",
      "['VBD : RB', 5L, 172L, 0.000184556, 0.000669972]\n",
      "['WDT : IN', 1L, 170L, 3.69e-05, 0.000662182]\n",
      "['RB : VBN', 1L, 169L, 3.69e-05, 0.000658287]\n",
      "['VB : NNS', 4L, 168L, 0.000147645, 0.000654392]\n",
      "['. : IN', 2L, 168L, 7.38e-05, 0.000654392]\n",
      "['NN : WDT', 10L, 165L, 0.000369113, 0.000642706]\n",
      "['\", : RB\"', 3L, 165L, 0.000110734, 0.000642706]\n",
      "['VBZ : TO', 2L, 164L, 7.38e-05, 0.000638811]\n",
      "[': : NNP', 3L, 161L, 0.000110734, 0.000627125]\n",
      "['CC : CD', 2L, 160L, 7.38e-05, 0.00062323]\n",
      "['TO : NNS', 4L, 157L, 0.000147645, 0.000611545]\n",
      "['VBD : VBD', 6L, 153L, 0.000221468, 0.000595964]\n",
      "['VBG : NNS', 8L, 148L, 0.00029529, 0.000576488]\n",
      "['IN : JJS', 1L, 147L, 3.69e-05, 0.000572593]\n",
      "['RB : CD', 1L, 144L, 3.69e-05, 0.000560907]\n",
      "['CC : VBZ', 7L, 143L, 0.000258379, 0.000557012]\n",
      "['NNS : VBP', 1L, 140L, 3.69e-05, 0.000545326]\n",
      "['PRP$ : CC', 5L, 140L, 0.000184556, 0.000545326]\n",
      "['NNP : (', 10L, 140L, 0.000369113, 0.000545326]\n",
      "['VBZ : RB', 1L, 138L, 3.69e-05, 0.000537536]\n",
      "['PRP$ : NNS', 4L, 133L, 0.000147645, 0.00051806]\n",
      "['. : NNS', 2L, 132L, 7.38e-05, 0.000514165]\n",
      "['JJ : PRP$', 1L, 131L, 3.69e-05, 0.00051027]\n",
      "['VBN : VBG', 1L, 130L, 3.69e-05, 0.000506374]\n",
      "['NNS : TO', 3L, 123L, 0.000110734, 0.000479108]\n",
      "['WDT : VBD', 4L, 121L, 0.000147645, 0.000471318]\n",
      "['\", : VBN\"', 1L, 119L, 3.69e-05, 0.000463527]\n",
      "['VBZ : NNS', 1L, 119L, 3.69e-05, 0.000463527]\n",
      "['TO : CD', 1L, 119L, 3.69e-05, 0.000463527]\n",
      "['JJS : NN', 5L, 118L, 0.000184556, 0.000459632]\n",
      "['PRP$ : JJ', 2L, 117L, 7.38e-05, 0.000455737]\n",
      "['CD : VBG', 1L, 116L, 3.69e-05, 0.000451842]\n",
      "['PRP$ : VBZ', 2L, 106L, 7.38e-05, 0.00041289]\n",
      "['NONE : IN', 10L, 106L, 0.000369113, 0.00041289]\n",
      "['CD : TO', 2L, 103L, 7.38e-05, 0.000401204]\n",
      "['NNPS : NNP', 1L, 101L, 3.69e-05, 0.000393414]\n",
      "['NNP : WRB', 1L, 100L, 3.69e-05, 0.000389519]\n",
      "['CC : PRP$', 1L, 97L, 3.69e-05, 0.000377833]\n",
      "['VBN : VBN', 1L, 96L, 3.69e-05, 0.000373938]\n",
      "['VBN : TO', 1L, 96L, 3.69e-05, 0.000373938]\n",
      "['RB : CC', 10L, 93L, 0.000369113, 0.000362253]\n",
      "['NN : MD', 4L, 92L, 0.000147645, 0.000358357]\n",
      "['JJR : IN', 6L, 92L, 0.000221468, 0.000358357]\n",
      "['NN : WP', 2L, 91L, 7.38e-05, 0.000354462]\n",
      "['VB : VB', 2L, 87L, 7.38e-05, 0.000338881]\n",
      "['VBP : NNS', 4L, 87L, 0.000147645, 0.000338881]\n",
      "['NNP : MD', 8L, 82L, 0.00029529, 0.000319405]\n",
      "['CC : VBG', 5L, 79L, 0.000184556, 0.00030772]\n",
      "['VBG : CD', 1L, 78L, 3.69e-05, 0.000303825]\n",
      "['VBD : VBG', 1L, 77L, 3.69e-05, 0.000299929]\n",
      "['CD : VBP', 2L, 75L, 7.38e-05, 0.000292139]\n",
      "['POS : NN', 8L, 75L, 0.00029529, 0.000292139]\n",
      "['VB : TO', 2L, 74L, 7.38e-05, 0.000288244]\n",
      "['VBD : PRP', 1L, 73L, 3.69e-05, 0.000284349]\n",
      "['DT : MD', 10L, 73L, 0.000369113, 0.000284349]\n",
      "[': : VBZ', 4L, 69L, 0.000147645, 0.000268768]\n",
      "['RB : VBD', 2L, 69L, 7.38e-05, 0.000268768]\n",
      "['VB : CC', 4L, 67L, 0.000147645, 0.000260978]\n",
      "['NONE : PRP', 1L, 67L, 3.69e-05, 0.000260978]\n",
      "['NNS : VBZ', 2L, 66L, 7.38e-05, 0.000257082]\n",
      "['IN : WRB', 9L, 66L, 0.000332201, 0.000257082]\n",
      "['VBG : TO', 2L, 66L, 7.38e-05, 0.000257082]\n",
      "['IN : MD', 6L, 63L, 0.000221468, 0.000245397]\n",
      "['TO : RB', 1L, 62L, 3.69e-05, 0.000241502]\n",
      "['CC : VB', 3L, 60L, 0.000110734, 0.000233711]\n",
      "['WP : VBD', 4L, 60L, 0.000147645, 0.000233711]\n",
      "['RP : NN', 1L, 60L, 3.69e-05, 0.000233711]\n",
      "['VBP : TO', 1L, 60L, 3.69e-05, 0.000233711]\n",
      "['WRB : NN', 1L, 58L, 3.69e-05, 0.000225921]\n",
      "['TO : TO', 3L, 54L, 0.000110734, 0.00021034]\n",
      "['CC : WP', 6L, 53L, 0.000221468, 0.000206445]\n",
      "['NN : JJS', 1L, 53L, 3.69e-05, 0.000206445]\n",
      "[': : VBD', 1L, 52L, 3.69e-05, 0.00020255]\n",
      "['POS : IN', 10L, 52L, 0.000369113, 0.00020255]\n",
      "['. : RB', 3L, 52L, 0.000110734, 0.00020255]\n",
      "['VB : VBN', 1L, 50L, 3.69e-05, 0.000194759]\n",
      "['\", : TO\"', 3L, 50L, 0.000110734, 0.000194759]\n",
      "['MD : IN', 2L, 49L, 7.38e-05, 0.000190864]\n",
      "['VB : VBD', 1L, 48L, 3.69e-05, 0.000186969]\n",
      "['VBP : VB', 1L, 47L, 3.69e-05, 0.000183074]\n",
      "['PRP$ : VBP', 1L, 46L, 3.69e-05, 0.000179179]\n",
      "['VB : RB', 2L, 45L, 7.38e-05, 0.000175283]\n",
      "['VBZ : VBG', 2L, 45L, 7.38e-05, 0.000175283]\n",
      "['JJ : MD', 5L, 45L, 0.000184556, 0.000175283]\n",
      "['NONE : NNS', 7L, 45L, 0.000258379, 0.000175283]\n",
      "['RB : VBG', 1L, 43L, 3.69e-05, 0.000167493]\n",
      "['NNPS : IN', 4L, 43L, 0.000147645, 0.000167493]\n",
      "['IN : VB', 1L, 40L, 3.69e-05, 0.000155808]\n",
      "['WDT : NNS', 1L, 39L, 3.69e-05, 0.000151912]\n",
      "['FW : NNP', 1L, 39L, 3.69e-05, 0.000151912]\n",
      "['\", : VBG\"', 3L, 38L, 0.000110734, 0.000148017]\n",
      "['CD : WDT', 3L, 38L, 0.000110734, 0.000148017]\n",
      "['TO : VB', 2L, 38L, 7.38e-05, 0.000148017]\n",
      "['WRB : NNP', 3L, 36L, 0.000110734, 0.000140227]\n",
      "['\", : (\"', 1L, 35L, 3.69e-05, 0.000136332]\n",
      "['VBG : PRP$', 1L, 35L, 3.69e-05, 0.000136332]\n",
      "['WDT : VBZ', 3L, 35L, 0.000110734, 0.000136332]\n",
      "['CC : RBS', 1L, 35L, 3.69e-05, 0.000136332]\n",
      "['NNP : FW', 1L, 34L, 3.69e-05, 0.000132436]\n",
      "['NONE : RB', 3L, 33L, 0.000110734, 0.000128541]\n",
      "['RB : VBZ', 2L, 32L, 7.38e-05, 0.000124646]\n",
      "['VBG : VBG', 1L, 32L, 3.69e-05, 0.000124646]\n",
      "['NN : WRB', 3L, 31L, 0.000110734, 0.000120751]\n",
      "['$ : NNP', 3L, 31L, 0.000110734, 0.000120751]\n",
      "['\", : WDT\"', 2L, 30L, 7.38e-05, 0.000116856]\n",
      "['JJR : NN', 3L, 28L, 0.000110734, 0.000109065]\n",
      "['VBP : CC', 10L, 28L, 0.000369113, 0.000109065]\n",
      "['VBG : VBD', 1L, 28L, 3.69e-05, 0.000109065]\n",
      "['NNPS : NN', 1L, 27L, 3.69e-05, 0.00010517]\n",
      "['WRB : NNS', 1L, 27L, 3.69e-05, 0.00010517]\n",
      "['JJ : (', 3L, 27L, 0.000110734, 0.00010517]\n",
      "['CC : MD', 2L, 27L, 7.38e-05, 0.00010517]\n",
      "['JJ : JJS', 1L, 27L, 3.69e-05, 0.00010517]\n",
      "['IN : FW', 6L, 26L, 0.000221468, 0.000101275]\n",
      "['PRP : CC', 3L, 26L, 0.000110734, 0.000101275]\n",
      "['VBP : VBD', 2L, 26L, 7.38e-05, 0.000101275]\n",
      "['\", : VBP\"', 4L, 26L, 0.000147645, 0.000101275]\n",
      "['PRP$ : VBG', 2L, 25L, 7.38e-05, 9.74e-05]\n",
      "[': : JJ', 5L, 25L, 0.000184556, 9.74e-05]\n",
      "['WRB : VBZ', 3L, 25L, 0.000110734, 9.74e-05]\n",
      "[': : )', 2L, 24L, 7.38e-05, 9.35e-05]\n",
      "['VBZ : NONE', 1L, 23L, 3.69e-05, 8.96e-05]\n",
      "['( : VBZ', 9L, 23L, 0.000332201, 8.96e-05]\n",
      "['CC : (', 7L, 23L, 0.000258379, 8.96e-05]\n",
      "['\", : WP\"', 1L, 22L, 3.69e-05, 8.57e-05]\n",
      "['VBP : VBZ', 1L, 22L, 3.69e-05, 8.57e-05]\n",
      "['CC : NNPS', 4L, 21L, 0.000147645, 8.18e-05]\n",
      "['FW : NN', 2L, 21L, 7.38e-05, 8.18e-05]\n",
      "['NN : PDT', 1L, 21L, 3.69e-05, 8.18e-05]\n",
      "['. : CC', 6L, 21L, 0.000221468, 8.18e-05]\n",
      "['NONE : VBP', 3L, 20L, 0.000110734, 7.79e-05]\n",
      "['VBG : VBZ', 1L, 20L, 3.69e-05, 7.79e-05]\n",
      "['NNS : WDT', 1L, 20L, 3.69e-05, 7.79e-05]\n",
      "[': : RB', 1L, 19L, 3.69e-05, 7.4e-05]\n",
      "['NNPS : CC', 2L, 19L, 7.38e-05, 7.4e-05]\n",
      "['\", : NONE\"', 2L, 19L, 7.38e-05, 7.4e-05]\n",
      "['NN : (', 3L, 19L, 0.000110734, 7.4e-05]\n",
      "['WP$ : VBZ', 1L, 19L, 3.69e-05, 7.4e-05]\n",
      "['VBN : VBZ', 1L, 18L, 3.69e-05, 7.01e-05]\n",
      "['JJS : CC', 1L, 18L, 3.69e-05, 7.01e-05]\n",
      "['POS : CC', 2L, 17L, 7.38e-05, 6.62e-05]\n",
      "['CC : )', 10L, 17L, 0.000369113, 6.62e-05]\n",
      "['CD : NNPS', 1L, 16L, 3.69e-05, 6.23e-05]\n",
      "['POS : VBD', 4L, 15L, 0.000147645, 5.84e-05]\n",
      "['RBR : NN', 3L, 15L, 0.000110734, 5.84e-05]\n",
      "['TO : VBD', 1L, 15L, 3.69e-05, 5.84e-05]\n",
      "['JJ : WP$', 2L, 15L, 7.38e-05, 5.84e-05]\n",
      "['WP : VBZ', 1L, 15L, 3.69e-05, 5.84e-05]\n",
      "['RP : IN', 2L, 15L, 7.38e-05, 5.84e-05]\n",
      "['TO : WRB', 3L, 15L, 0.000110734, 5.84e-05]\n",
      "['CD : WP', 3L, 15L, 0.000110734, 5.84e-05]\n",
      "[': : VBP', 2L, 14L, 7.38e-05, 5.45e-05]\n",
      "['VBD : WRB', 1L, 14L, 3.69e-05, 5.45e-05]\n",
      "['CC : WDT', 9L, 14L, 0.000332201, 5.45e-05]\n",
      "['. : VBG', 1L, 14L, 3.69e-05, 5.45e-05]\n",
      "['RB : VBP', 1L, 13L, 3.69e-05, 5.06e-05]\n",
      "['( : VBD', 2L, 13L, 7.38e-05, 5.06e-05]\n",
      "['WP : VBP', 1L, 13L, 3.69e-05, 5.06e-05]\n",
      "['JJR : NNS', 1L, 13L, 3.69e-05, 5.06e-05]\n",
      "[': : CC', 6L, 13L, 0.000221468, 5.06e-05]\n",
      "['DT : VB', 5L, 12L, 0.000184556, 4.67e-05]\n",
      "['POS : VBZ', 1L, 11L, 3.69e-05, 4.28e-05]\n",
      "['IN : RBR', 1L, 11L, 3.69e-05, 4.28e-05]\n",
      "[': : NNS', 1L, 11L, 3.69e-05, 4.28e-05]\n",
      "['WRB : RB', 1L, 11L, 3.69e-05, 4.28e-05]\n",
      "['RBS : NNP', 1L, 11L, 3.69e-05, 4.28e-05]\n",
      "['POS : TO', 3L, 11L, 0.000110734, 4.28e-05]\n",
      "['FW : CC', 1L, 10L, 3.69e-05, 3.9e-05]\n",
      "['( : VBN', 1L, 10L, 3.69e-05, 3.9e-05]\n",
      "['PRP$ : MD', 1L, 10L, 3.69e-05, 3.9e-05]\n",
      "['VBD : WP', 1L, 10L, 3.69e-05, 3.9e-05]\n",
      "['VBD : RBR', 1L, 9L, 3.69e-05, 3.51e-05]\n",
      "['NONE : VBN', 2L, 9L, 7.38e-05, 3.51e-05]\n",
      "['NN : )', 3L, 9L, 0.000110734, 3.51e-05]\n",
      "['WRB : IN', 1L, 9L, 3.69e-05, 3.51e-05]\n",
      "['NONE : :', 3L, 9L, 0.000110734, 3.51e-05]\n",
      "['VBN : WP', 2L, 8L, 7.38e-05, 3.12e-05]\n",
      "['CD : :', 9L, 8L, 0.000332201, 3.12e-05]\n",
      "['WDT : CC', 3L, 8L, 0.000110734, 3.12e-05]\n",
      "['VBG : WDT', 1L, 8L, 3.69e-05, 3.12e-05]\n",
      "['VBG : VBP', 1L, 8L, 3.69e-05, 3.12e-05]\n",
      "['( : RB', 1L, 8L, 3.69e-05, 3.12e-05]\n",
      "['WP$ : NN', 1L, 8L, 3.69e-05, 3.12e-05]\n",
      "['VBN : VBP', 1L, 7L, 3.69e-05, 2.73e-05]\n",
      "['CD : (', 1L, 7L, 3.69e-05, 2.73e-05]\n",
      "['VB : VBP', 1L, 7L, 3.69e-05, 2.73e-05]\n",
      "['DT : RP', 1L, 7L, 3.69e-05, 2.73e-05]\n",
      "['WDT : MD', 2L, 7L, 7.38e-05, 2.73e-05]\n",
      "['NN : WP$', 1L, 7L, 3.69e-05, 2.73e-05]\n",
      "['FW : FW', 1L, 7L, 3.69e-05, 2.73e-05]\n",
      "['( : CC', 10L, 7L, 0.000369113, 2.73e-05]\n",
      "['DT : WP$', 1L, 6L, 3.69e-05, 2.34e-05]\n",
      "[') : VBN', 1L, 6L, 3.69e-05, 2.34e-05]\n",
      "['VBZ : WP', 1L, 6L, 3.69e-05, 2.34e-05]\n",
      "['JJS : VBN', 1L, 6L, 3.69e-05, 2.34e-05]\n",
      "['RB : MD', 1L, 5L, 3.69e-05, 1.95e-05]\n",
      "[': : NONE', 2L, 5L, 7.38e-05, 1.95e-05]\n",
      "['( : CD', 5L, 5L, 0.000184556, 1.95e-05]\n",
      "['NN : POS', 8L, 5L, 0.00029529, 1.95e-05]\n",
      "['VBD : VBP', 1L, 4L, 3.69e-05, 1.56e-05]\n",
      "['NNS : (', 2L, 4L, 7.38e-05, 1.56e-05]\n",
      "['WRB : CC', 2L, 4L, 7.38e-05, 1.56e-05]\n",
      "['WP$ : NNP', 1L, 4L, 3.69e-05, 1.56e-05]\n",
      "['WP : MD', 1L, 3L, 3.69e-05, 1.17e-05]\n",
      "['VBG : RBR', 1L, 3L, 3.69e-05, 1.17e-05]\n",
      "['VBD : (', 1L, 3L, 3.69e-05, 1.17e-05]\n",
      "['POS : CD', 1L, 3L, 3.69e-05, 1.17e-05]\n",
      "['IN : $', 1L, 3L, 3.69e-05, 1.17e-05]\n",
      "['POS : WDT', 1L, 3L, 3.69e-05, 1.17e-05]\n",
      "['TO : MD', 1L, 3L, 3.69e-05, 1.17e-05]\n",
      "[': : MD', 2L, 3L, 7.38e-05, 1.17e-05]\n",
      "['NNS : :', 4L, 3L, 0.000147645, 1.17e-05]\n",
      "['NNS : .', 6L, 3L, 0.000221468, 1.17e-05]\n",
      "['NNS : POS', 1L, 3L, 3.69e-05, 1.17e-05]\n",
      "['VBG : WP', 1L, 3L, 3.69e-05, 1.17e-05]\n",
      "['\", : WP$\"', 1L, 2L, 3.69e-05, 7.79e-06]\n",
      "['CD : $', 2L, 2L, 7.38e-05, 7.79e-06]\n",
      "['$ : NN', 2L, 2L, 7.38e-05, 7.79e-06]\n",
      "['VBD : :', 5L, 2L, 0.000184556, 7.79e-06]\n",
      "['NONE : MD', 1L, 2L, 3.69e-05, 7.79e-06]\n",
      "['JJR : VBZ', 1L, 2L, 3.69e-05, 7.79e-06]\n",
      "['VBZ : :', 4L, 2L, 0.000147645, 7.79e-06]\n",
      "['POS : POS', 2L, 2L, 7.38e-05, 7.79e-06]\n",
      "['NONE : $', 2L, 2L, 7.38e-05, 7.79e-06]\n",
      "['RB : (', 1L, 2L, 3.69e-05, 7.79e-06]\n",
      "['RB : )', 4L, 2L, 0.000147645, 7.79e-06]\n",
      "['\"FW : ,\"', 1L, 1L, 3.69e-05, 3.9e-06]\n",
      "['IN : WP$', 4L, 1L, 0.000147645, 3.9e-06]\n",
      "['CD : )', 5L, 1L, 0.000184556, 3.9e-06]\n",
      "['VBP : .', 7L, 1L, 0.000258379, 3.9e-06]\n",
      "['( : VBP', 5L, 1L, 0.000184556, 3.9e-06]\n",
      "['TO : )', 3L, 1L, 0.000110734, 3.9e-06]\n",
      "['VBP : :', 3L, 1L, 0.000110734, 3.9e-06]\n",
      "['$ : JJ', 1L, 1L, 3.69e-05, 3.9e-06]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nGood ones:\\n'NNP : DT'\\n'NN : DT'\\n'NNP : CD'\\nand pretty much all non-mention solos\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"This is to see the difference between pos tags of mentions and non-mentions.\"\"\"\n",
    "\n",
    "from __future__ import division\n",
    "import os\n",
    "from operator import itemgetter\n",
    "\n",
    "posDict = {'mentions':{}, 'non-mentions':{}}\n",
    "\n",
    "with open('/users/cs/amaral/wikisim/wikisim/posdata.txt', 'r') as posFile:\n",
    "    content = posFile.readlines()\n",
    "    for line in content[1:]:\n",
    "        theLine = line.split('\\t')\n",
    "        if theLine[0] <> '':\n",
    "            posDict['mentions'][theLine[0]] = [long(theLine[1]),float(theLine[2].replace('\\\\n',''))]\n",
    "        posDict['non-mentions'][theLine[3]] = [long(theLine[4]),float(theLine[5].replace('\\\\n',''))]\n",
    "        \n",
    "diffs = []\n",
    "rawDiffs = []\n",
    "mSolos =[]\n",
    "bestDiffs = []\n",
    "for mKey in posDict['mentions'].keys():\n",
    "    if mKey in posDict['non-mentions']:\n",
    "        # find abs diff between \n",
    "        dif = [mKey, abs(posDict['mentions'][mKey][1] - posDict['non-mentions'][mKey][1])]\n",
    "        diffs.append(dif)\n",
    "        dif = [mKey, abs(posDict['mentions'][mKey][0] - posDict['non-mentions'][mKey][0])]\n",
    "        rawDiffs.append(dif)\n",
    "        if posDict['mentions'][mKey][0] <= 10:\n",
    "            bestDiffs.append([mKey, posDict['mentions'][mKey][0], posDict['non-mentions'][mKey][0], \n",
    "                             posDict['mentions'][mKey][1], posDict['non-mentions'][mKey][1]])\n",
    "    else:\n",
    "        # only for mentions\n",
    "        solo = [mKey, posDict['mentions'][mKey][1]]\n",
    "        mSolos.append(solo)\n",
    "nSolos =[]\n",
    "for mKey in posDict['non-mentions'].keys():\n",
    "    if mKey not in posDict['mentions']:\n",
    "        # only for non-mentions\n",
    "        solo = [mKey, posDict['non-mentions'][mKey][0], posDict['non-mentions'][mKey][1]]\n",
    "        nSolos.append(solo)\n",
    "        \n",
    "diffs = sorted(diffs, key = itemgetter(1), reverse = True)\n",
    "rawDiffs = sorted(rawDiffs, key = itemgetter(1), reverse = True)\n",
    "mSolos = sorted(mSolos, key = itemgetter(1), reverse = True)\n",
    "nSolos = sorted(nSolos, key = itemgetter(1), reverse = True)\n",
    "bestDiffs = sorted(bestDiffs, key = itemgetter(2), reverse = True)\n",
    "\n",
    "show = 10\n",
    "\n",
    "print 'Diffs:'\n",
    "for dif in diffs[:show]:\n",
    "    print dif\n",
    "    \n",
    "print '\\nRaw Diffs:'\n",
    "for dif in rawDiffs[:show]:\n",
    "    print dif\n",
    "    \n",
    "print '\\nMention Solos:'\n",
    "print 'len: ', len(mSolos)\n",
    "for solo in mSolos[:show]:\n",
    "    print solo\n",
    "    \n",
    "asum = 0\n",
    "print '\\nNon-Mention Solos:'\n",
    "print 'len: ', len(nSolos)\n",
    "for solo in nSolos[:show]:\n",
    "    print solo\n",
    "    asum += solo[1]\n",
    "    \n",
    "print 'sum: ' + str(asum)\n",
    "\n",
    "with open('/users/cs/amaral/wikisim/wikification/pos-filter-out-nonmentions.txt', 'w') as resultFile:\n",
    "    # all that appear only for non mentions\n",
    "    for solo in nSolos:\n",
    "        resultFile.write(str(solo[0]) + '\\n')\n",
    "    # some well hand picked ones\n",
    "    resultFile.write(\"NNP : DT\\nNN : DT\\nNNP : CD\\nCD : CD\\nNNP : NONE\")\n",
    "\n",
    "print '\\nBest Diffs:'\n",
    "for dif in bestDiffs:\n",
    "    print dif\n",
    "\n",
    "\"\"\"\n",
    "Good ones:\n",
    "'NNP : DT'\n",
    "'NN : DT'\n",
    "'NNP : CD'\n",
    "and pretty much all non-mention solos\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Wikification for evaluation purposes\n",
    "\"\"\"\n",
    "\n",
    "from wikipedia import *\n",
    "from operator import itemgetter\n",
    "import requests\n",
    "import json\n",
    "from __future__ import division\n",
    "import nltk\n",
    "import scipy as sp\n",
    "import scipy.sparse as sprs\n",
    "import scipy.spatial\n",
    "import scipy.sparse.linalg\n",
    "from calcsim import *\n",
    "sys.path.append('../')\n",
    "from wsd.wsd import *\n",
    "\n",
    "MIN_MENTION_LENGTH = 3 # mentions must be at least this long\n",
    "MIN_FREQUENCY = 20 # anchor with frequency below is ignored\n",
    "\n",
    "def get_solr_count(s):\n",
    "    \"\"\" Gets the number of documents the string occurs \n",
    "        NOTE: Multi words should be quoted\n",
    "    Arg:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Returns:\n",
    "        The number of documents\n",
    "    \"\"\"\n",
    "\n",
    "    q='+text:(\\\"%s\\\")'%(s,)\n",
    "    qstr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'indent':'on', 'wt':'json', 'q':q, 'rows':0}\n",
    "    r = requests.get(qstr, params=params)\n",
    "    try:\n",
    "        if 'response' not in r.json():\n",
    "            return 0\n",
    "        else:\n",
    "            return r.json()['response']['numFound']\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_mention_count(s):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the amount of times that the given string appears as a mention in wikipedia.\n",
    "    Args:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Return:\n",
    "        The amount of times the given string appears as a mention in wikipedia\n",
    "    \"\"\"\n",
    "    \n",
    "    result = anchor2concept(s)\n",
    "    rSum = 0\n",
    "    for item in result:\n",
    "        rSum += item[1]\n",
    "        \n",
    "    return rSum\n",
    "\n",
    "def mentionProb(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the probability that the text is a mention in Wikipedia.\n",
    "    Args:\n",
    "        text: \n",
    "    Return:\n",
    "        The probability that the text is a mention in Wikipedia.\n",
    "    \"\"\"\n",
    "    \n",
    "    totalMentions = get_mention_count(text)\n",
    "    totalAppearances = get_solr_count(text.replace(\".\", \"\"))\n",
    "    \n",
    "    if totalAppearances == 0:\n",
    "        return 0 # a mention never used probably is not a good link\n",
    "    else:\n",
    "        return totalMentions/totalAppearances\n",
    "\n",
    "def destroyExclusiveOverlaps(textData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Removes all overlaps that start at same letter from text data, so that only the best mention in an\n",
    "        overlap set is left.\n",
    "    Args:\n",
    "        textData: [[start, end, text, anchProb],...]\n",
    "    Return:\n",
    "        textData minus the unesescary elements that overlap.\n",
    "    \"\"\"\n",
    "    \n",
    "    newTextData = [] # textData minus the unesescary parts of the overlapping\n",
    "    overlappingSets = [] # stores arrays of the indexes of overlapping items from textData\n",
    "    \n",
    "    # creates the overlappingSets array\n",
    "    i = 0\n",
    "    while i < len(textData)-1:\n",
    "        # even single elements considered overlapping set\n",
    "        # this is root of overlapping set\n",
    "        overlappingSets.append([i])\n",
    "        overlapIndex = len(overlappingSets) - 1\n",
    "        theBegin = textData[i][0]\n",
    "        \n",
    "        # look at next words untill not overlap\n",
    "        for j in range(i+1, len(textData)):\n",
    "            # if next word starts before endiest one ends\n",
    "            if textData[j][0] == theBegin:\n",
    "                overlappingSets[overlapIndex].append(j)\n",
    "                i = j # make sure not to repeat overlap set\n",
    "            else:\n",
    "                # add final word\n",
    "                if j == len(textData) - 1:\n",
    "                    overlappingSets.append([j])\n",
    "                break\n",
    "        i += 1\n",
    "                    \n",
    "    # get only the best overlapping element of each set\n",
    "    for oSet in overlappingSets:\n",
    "        bestIndex = 0\n",
    "        bestScore = -1\n",
    "        for i in oSet:\n",
    "            score = mentionProb(textData[i][2])\n",
    "            if score > bestScore:\n",
    "                bestScore = score\n",
    "                bestIndex = i\n",
    "        \n",
    "        # put right item in new textData\n",
    "        newTextData.append(textData[bestIndex])\n",
    "        \n",
    "    return newTextData\n",
    "\n",
    "def destroyResidualOverlaps(textData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Removes all overlaps from text data, so that only the best mention in an\n",
    "        overlap set is left.\n",
    "    Args:\n",
    "        textData: [[start, end, text, anchProb],...]\n",
    "    Return:\n",
    "        textData minus the unesescary elements that overlap.\n",
    "    \"\"\"\n",
    "    \n",
    "    newTextData = [] # to be returned\n",
    "    oSet = [] # the set of current overlaps\n",
    "    rootWIndex = 0 # the word to start looking from for finding root word\n",
    "    rEnd = 0 # the end index of the root word\n",
    "    \n",
    "    # keep looping as long as overlaps\n",
    "    while True:\n",
    "        oSet = []\n",
    "        oSet.append(textData[rootWIndex])\n",
    "        for i in range(rootWIndex + 1, len(textData)):\n",
    "            # if cur start before root end\n",
    "            if textData[i][0] < textData[rootWIndex][1]:\n",
    "                oSet.append(textData[i])\n",
    "            else:\n",
    "                break # have all overlap words\n",
    "\n",
    "        \n",
    "        bestIndex = 0\n",
    "        # deal with the overlaps\n",
    "        if len(oSet) > 1:\n",
    "            bestProb = 0\n",
    "            \n",
    "            # choose the most probable\n",
    "            i = 0\n",
    "            for mention in oSet:\n",
    "                prob = mentionProb(mention[2])\n",
    "                if prob > bestProb:\n",
    "                    bestProb = prob\n",
    "                    bestIndex = i\n",
    "                i += 1\n",
    "        else:\n",
    "            rootWIndex += 1 # move up one if no overlaps\n",
    "                \n",
    "        # remove from old text data all that is not best\n",
    "        for i in range(0, len(oSet)):\n",
    "            if i <> bestIndex:\n",
    "                textData.remove(oSet[i])\n",
    "                \n",
    "        # add the best to new\n",
    "        if not (oSet[bestIndex] in newTextData):\n",
    "            newTextData.append(oSet[bestIndex])\n",
    "            \n",
    "        if rootWIndex >= len(textData):\n",
    "            break\n",
    "    \n",
    "    return newTextData\n",
    "    \n",
    "def mentionStartsAndEnds(textData, forTruth = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes in a list of mentions and turns each of its mentions into the form: [wIndex, start, end]. \n",
    "        Or if forTruth is true: [[start,end,entityId]]\n",
    "    Args:\n",
    "        textData: {'text': [w1,w2,w3,...] , 'mentions': [[wordIndex,entityTitle],...]}, to be transformed \n",
    "            as described above.\n",
    "        forTruth: Changes form to use.\n",
    "    Return:\n",
    "        The mentions in the form [[wIndex, start, end],...]]. Or if forTruth is true: [[start,end,entityId]]\n",
    "    \"\"\"\n",
    "    \n",
    "    curWord = 0 \n",
    "    curStart = 0\n",
    "    for mention in textData['mentions']:\n",
    "        while curWord < mention[0]:\n",
    "            curStart += len(textData['text'][curWord]) + 1\n",
    "            curWord += 1\n",
    "            \n",
    "        ent = mention[1] # store entity title in case of forTruth\n",
    "        mention.pop() # get rid of entity text\n",
    "        \n",
    "        if forTruth:\n",
    "            mention.pop() # get rid of wIndex too\n",
    "            \n",
    "        mention.append(curStart) # start of the mention\n",
    "        mention.append(curStart + len(textData['text'][curWord])) # end of the mention\n",
    "        \n",
    "        if forTruth:\n",
    "            mention.append(title2id(ent)) # put on entityId\n",
    "    \n",
    "    return textData['mentions']\n",
    "     \n",
    "def mentionExtract(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes in a text and splits it into the different words/mentions.\n",
    "    Args:\n",
    "        phrase: The text to be split.\n",
    "    Return:\n",
    "        The text split it into the different words / mentions: \n",
    "        {'text':[w1,w2,...], 'mentions': [[wIndex,begin,end],...]}\n",
    "    \"\"\"\n",
    "    \n",
    "    addr = 'http://localhost:8983/solr/enwikianchors20160305/tag'\n",
    "    params={'overlaps':'ALL', 'tagsLimit':'5000', 'fl':'id','wt':'json','indent':'on'}\n",
    "    r = requests.post(addr, params=params, data=text.encode('utf-8'))\n",
    "    textData0 = r.json()['tags']\n",
    "    \n",
    "    splitText = [] # the text now in split form\n",
    "    mentions = [] # mentions before remove inadequate ones\n",
    "    \n",
    "    textData = [] # [[begin,end,word,anchorProb],...]\n",
    "    \n",
    "    i = 0 # for wordIndex\n",
    "    # get rid of extra un-needed Solr data, and add in anchor probability\n",
    "    for item in textData0:\n",
    "        totalMentions = get_mention_count(text[item[1]:item[3]])\n",
    "        totalAppearances = get_solr_count(text[item[1]:item[3]].replace(\".\", \"\"))\n",
    "        if totalAppearances == 0:\n",
    "            anchorProb = 0\n",
    "        else:\n",
    "            anchorProb = totalMentions/totalAppearances\n",
    "        # put in the new clean textData\n",
    "        textData.append([item[1], item[3], text[item[1]:item[3]], anchorProb, i])\n",
    "        i += 1\n",
    "        \n",
    "        # also fill split text\n",
    "        splitText.append(text[item[1]:item[3]])\n",
    "    \n",
    "    # get rid of overlaps\n",
    "    textData = destroyExclusiveOverlaps(textData)\n",
    "    textData = destroyResidualOverlaps(textData)\n",
    "        \n",
    "    # gets the POS labels for the words\n",
    "    postrs = []\n",
    "    for item in textData:\n",
    "        postrs.append(item[2])\n",
    "    postrs = nltk.pos_tag(postrs)\n",
    "    for i in range(0,len(textData)):\n",
    "        textData[i].append(postrs[i]) # [5][1] is index of type of word\n",
    "    \n",
    "    mentionPThrsh = 0.001 # for getting rid of unlikelies\n",
    "    \n",
    "    # put in only good mentions\n",
    "    for item in textData:\n",
    "        if (item[3] >= mentionPThrsh # if popular enough, and either some type of noun or JJ or CD\n",
    "                and (item[5][1][0:2] == 'NN' or item[5][1] == 'JJ' or item[5][1] == 'CD')):\n",
    "            mentions.append([item[4], item[0], item[1]]) # wIndex, start, end\n",
    "    \n",
    "    # get in same format as dataset provided data\n",
    "    newTextData = {'text':splitText, 'mentions':mentions}\n",
    "    \n",
    "    return newTextData\n",
    "\n",
    "def generateCandidates(textData, maxC):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates up to maxC candidates for each possible mention word in phrase.\n",
    "    Args:\n",
    "        textData: A text in split form along with its suspected mentions.\n",
    "        maxC: The max amount of candidates to accept.\n",
    "    Return:\n",
    "        The top maxC candidates for each possible mention word in textData.\n",
    "    \"\"\"\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    for mention in textData['mentions']:\n",
    "        results = sorted(anchor2concept(textData['text'][mention[0]]), key = itemgetter(1), \n",
    "                          reverse = True)\n",
    "        candidates.append(results[:maxC]) # take up to maxC of the results\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def precision(truthSet, mySet):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the precision of mySet against the truthSet.\n",
    "    Args:\n",
    "        truthSet: The 'right' answers for what the entities are. [[start,end,id],...]\n",
    "        mySet: My code's output for what it thinks the right entities are. [[start,end,id],...]\n",
    "    Return:\n",
    "        The precision: (# of correct entities)/(# of found entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numFound = len(mySet)\n",
    "    numCorrect = 0 # incremented in for loop\n",
    "    \n",
    "    truthIndex = 0\n",
    "    myIndex = 0\n",
    "    \n",
    "    while truthIndex < len(truthSet) and myIndex < len(mySet):\n",
    "        if mySet[myIndex][0] < truthSet[truthIndex][0]:\n",
    "            if mySet[myIndex][1] > truthSet[truthIndex][0]:\n",
    "                # overlap with mine behind\n",
    "                if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine not even reach truth\n",
    "                myIndex += 1\n",
    "                \n",
    "        elif mySet[myIndex][0] == truthSet[truthIndex][0]:\n",
    "            # same mention (same start atleast)\n",
    "            if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                numCorrect += 1\n",
    "                truthIndex += 1\n",
    "                myIndex += 1\n",
    "            elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                # truth ends first\n",
    "                truthIndex += 1\n",
    "            else:\n",
    "                # mine ends first\n",
    "                myIndex += 1\n",
    "                  \n",
    "        elif mySet[myIndex][0] > truthSet[truthIndex][0]:\n",
    "            if mySet[myIndex][0] < truthSet[truthIndex][1]:\n",
    "                # overlap with truth behind\n",
    "                if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine beyond mention, increment truth\n",
    "                truthIndex += 1\n",
    "\n",
    "    #print 'correct: ' + str(numCorrect) + '\\nfound: ' + str(numFound)\n",
    "    if numFound == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numFound)\n",
    "\n",
    "def recall(truthSet, mySet):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the recall of mySet against the truthSet.\n",
    "    Args:\n",
    "        truthSet: The 'right' answers for what the entities are. [[start,end,id],...]\n",
    "        mySet: My code's output for what it thinks the right entities are. [[start,end,id],...]\n",
    "    Return:\n",
    "        The recall: (# of correct entities)/(# of actual entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numActual = len(truthSet)\n",
    "    numCorrect = 0 # incremented in for loop)\n",
    "    \n",
    "    truthIndex = 0\n",
    "    myIndex = 0\n",
    "    \n",
    "    while truthIndex < len(truthSet) and myIndex < len(mySet):\n",
    "        if mySet[myIndex][0] < truthSet[truthIndex][0]:\n",
    "            if mySet[myIndex][1] > truthSet[truthIndex][0]:\n",
    "                # overlap with mine behind\n",
    "                if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine not even reach truth\n",
    "                myIndex += 1\n",
    "                \n",
    "        elif mySet[myIndex][0] == truthSet[truthIndex][0]:\n",
    "            # same mention (same start atleast)\n",
    "            if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                numCorrect += 1\n",
    "                truthIndex += 1\n",
    "                myIndex += 1\n",
    "            elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                # truth ends first\n",
    "                truthIndex += 1\n",
    "            else:\n",
    "                # mine ends first\n",
    "                myIndex += 1\n",
    "                  \n",
    "        elif mySet[myIndex][0] > truthSet[truthIndex][0]:\n",
    "            if mySet[myIndex][0] < truthSet[truthIndex][1]:\n",
    "                # overlap with truth behind\n",
    "                if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine beyond mention, increment truth\n",
    "                truthIndex += 1\n",
    "                \n",
    "    if numActual == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numActual)\n",
    "    \n",
    "def getSurroundingWords(text, mIndex, window, asList = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the words surround the given mention. Expanding out window elements\n",
    "        on both sides.\n",
    "    Args:\n",
    "        text: A list of words.\n",
    "        mIndex: The index of the word that is the center of where to get surrounding words.\n",
    "        window: The amount of words to the left and right to get.\n",
    "        asList: Whether to return the words as a list, otherwise just a string.\n",
    "    Return:\n",
    "        The words that surround the given mention. Expanding out window elements\n",
    "        on both sides.\n",
    "    \"\"\"\n",
    "    \n",
    "    imin = mIndex - window\n",
    "    imax = mIndex + window + 1\n",
    "    \n",
    "    # fix extreme bounds\n",
    "    if imin < 0:\n",
    "        imin = 0\n",
    "    if imax > len(text):\n",
    "        imax = len(text)\n",
    "        \n",
    "    if asList == True:\n",
    "        words = (text[imin:mIndex] + text[mIndex+1:imax])\n",
    "    else:\n",
    "        words = \" \".join(text[imin:mIndex] + text[mIndex+1:imax])\n",
    "    \n",
    "    # return surrounding part of word minus the mIndex word\n",
    "    return words\n",
    "\n",
    "def getMentionSentence(text, mention, asList = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the sentence of the mention, minus the mention.\n",
    "    Args:\n",
    "        text: The text to get the sentence from.\n",
    "        index: The mention.\n",
    "        asList: Whether to return the words as a list, otherwise just a string.\n",
    "    Return:\n",
    "        The sentence of the mention, minus the mention.\n",
    "    \"\"\"\n",
    "    \n",
    "    # the start and end indexes of the sentence\n",
    "    sStart = 0\n",
    "    sEnd = 0\n",
    "    \n",
    "    # get sentences using nltk\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # find sentence that mention is in\n",
    "    curLen = 0\n",
    "    for s in sents:\n",
    "        curLen += len(s)\n",
    "        # if greater than begin of mention\n",
    "        if curLen > mention[1]:\n",
    "            # remove mention from string to not get bias from self referencing article\n",
    "            if asList == True:\n",
    "                sentence = (s.replace(text[mention[1]:mention[2]],\"\")).split(\" \")\n",
    "            else:\n",
    "                sentence = s.replace(text[mention[1]:mention[2]],\"\")\n",
    "            \n",
    "            return sentence\n",
    "        \n",
    "    # in case it missed\n",
    "    if asList == True:\n",
    "        return []\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def escapeStringSolr(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Escapes a given string for use in Solr.\n",
    "    Args:\n",
    "        text: The string to escape.\n",
    "    Return:\n",
    "        The escaped text.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = text.replace(\"\\\\\", \"\\\\\\\\\\\\\")\n",
    "    text = text.replace('+', r'\\+')\n",
    "    text = text.replace(\"-\", \"\\-\")\n",
    "    text = text.replace(\"&&\", \"\\&&\")\n",
    "    text = text.replace(\"||\", \"\\||\")\n",
    "    text = text.replace(\"!\", \"\\!\")\n",
    "    text = text.replace(\"(\", \"\\(\")\n",
    "    text = text.replace(\")\", \"\\)\")\n",
    "    text = text.replace(\"{\", \"\\{\")\n",
    "    text = text.replace(\"}\", \"\\}\")\n",
    "    text = text.replace(\"[\", \"\\[\")\n",
    "    text = text.replace(\"]\", \"\\]\")\n",
    "    text = text.replace(\"^\", \"\\^\")\n",
    "    text = text.replace(\"\\\"\", \"\\\\\\\"\")\n",
    "    text = text.replace(\"~\", \"\\~\")\n",
    "    text = text.replace(\"*\", \"\\*\")\n",
    "    text = text.replace(\"?\", \"\\?\")\n",
    "    text = text.replace(\":\", \"\\:\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "def bestRelevancy1Match(mentionStr, context, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the candidate that gives the highest relevance when given the context.\n",
    "    Args:\n",
    "        mentionStr: The mention as it appears in the text\n",
    "        context: The words that surround the target word.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The index of the candidate with the best relevance score from the context.\n",
    "    \"\"\"\n",
    "    \n",
    "    # put text in right format\n",
    "    context = escapeStringSolr(context)\n",
    "    mentionStr = escapeStringSolr(mentionStr)\n",
    "    \n",
    "    strIds = ['id:' +  str(strId[0]) for strId in candidates]\n",
    "    \n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    addr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'fl':'id score', 'fq':\" \".join(strIds), 'indent':'on',\n",
    "            'q':'text:('+context.encode('utf-8')+')^1 title:(' + mentionStr.encode('utf-8')+')^1.35',\n",
    "            'wt':'json'}\n",
    "    r = requests.get(addr, params = params)\n",
    "    \n",
    "    if 'response' not in r.json():\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    if 'docs' not in r.json()['response']:\n",
    "        return 0\n",
    "    \n",
    "    results = r.json()['response']['docs']\n",
    "    if len(results) == 0:\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    bestId = long(r.json()['response']['docs'][0]['id'])\n",
    "    \n",
    "    #for doc in r.json()['response']['docs']:\n",
    "        #print '[' + id2title(doc['id']) + '] -> ' + str(doc['score'])\n",
    "    \n",
    "    # find which index has bestId\n",
    "    bestIndex = 0\n",
    "    for cand in candidates:\n",
    "        if cand[0] == bestId:\n",
    "            return bestIndex\n",
    "        else:\n",
    "            bestIndex += 1\n",
    "            \n",
    "    return bestIndex # in case it was missed\n",
    "\n",
    "def bestRelevancy2Match(context, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the candidate that gives the highest relevance when given the context.\n",
    "    Args:\n",
    "        context: The words that surround the target word.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The index of the candidate with the best relevance score from the context.\n",
    "    \"\"\"\n",
    "    \n",
    "    # put text in right format\n",
    "    context = escapeStringSolr(context)\n",
    "    strIds = ['entityid:' +  str(strId[0]) for strId in candidates]\n",
    "    \n",
    "    # dictionary to hold scores for each id\n",
    "    scoreDict = {}\n",
    "    for cand in candidates:\n",
    "        scoreDict[str(cand[0])] = 0\n",
    "    \n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    addr = 'http://localhost:8983/solr/enwiki20160305_context/select'\n",
    "    params={'fl':'entityid', 'fq':\" \".join(strIds), 'indent':'on',\n",
    "            'q':'_context_:('+context.encode('utf-8')+')',\n",
    "            'wt':'json'}\n",
    "    r = requests.get(addr, params = params)\n",
    "    \n",
    "    if 'response' not in r.json():\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    if 'docs' not in r.json()['response']:\n",
    "        return 0\n",
    "    \n",
    "    results = r.json()['response']['docs']\n",
    "    if len(results) == 0:\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    for doc in r.json()['response']['docs']:\n",
    "        scoreDict[str(doc['entityid'])] += 1\n",
    "    \n",
    "    # get the index that has the best score\n",
    "    bestScore = 0\n",
    "    bestIndex = 0\n",
    "    curIndex = 0\n",
    "    for cand in candidates:\n",
    "        if scoreDict[str(cand[0])] > bestScore:\n",
    "            bestScore = scoreDict[str(cand[0])]\n",
    "            bestIndex = curIndex\n",
    "        curIndex += 1\n",
    "            \n",
    "    return bestIndex\n",
    "\n",
    "def bestWord2VecMatch(context, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses word2vec to find the candidate with the best similarity to the context.\n",
    "    Args:\n",
    "        context: The words that surround the target word as a list.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The index of the candidate with the best similarity score with the context.\n",
    "    \"\"\"\n",
    "    \n",
    "    ctxVec = pd.Series(sp.zeros(300)) # default zero vector\n",
    "    # add all context words together\n",
    "    for word in context:\n",
    "        ctxVec += getword2vector(word)\n",
    "        \n",
    "    # compare context vector to each of the candidates\n",
    "    bestIndex = 0\n",
    "    bestScore = 0\n",
    "    i = 0\n",
    "    for cand in candidates:\n",
    "        eVec = getentity2vector(str(cand[0]))\n",
    "        score = 1-sp.spatial.distance.cosine(ctxVec, eVec)\n",
    "        #print '[' + id2title(cand[0]) + ']' + ' -> ' + str(score)\n",
    "        # update score and index\n",
    "        if score > bestScore: \n",
    "            bestIndex = i\n",
    "            bestScore = score\n",
    "            \n",
    "        i += 1 # next index\n",
    "            \n",
    "    return bestIndex\n",
    "    \n",
    "def wikifyPopular(textData, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the most popular candidate for each mention.\n",
    "    Args:\n",
    "        textData: A text in split form along with its suspected mentions.\n",
    "        candidates: A list of list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidates = []\n",
    "    i = 0 # track which mention's candidates we are looking at\n",
    "    # for each mention choose the top candidate\n",
    "    for mention in textData['mentions']:\n",
    "        if len(candidates[i]) > 0:\n",
    "            topCandidates.append([mention[1], mention[2], candidates[i][0][0]])\n",
    "        i += 1 # move to list of candidates for next mention\n",
    "            \n",
    "    return topCandidates\n",
    "\n",
    "def wikifyRelevancy(textData, candidates, oText, useSentence = False, window = 7, method2 = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the candidate that has the highest relevance with the surrounding window words.\n",
    "    Args:\n",
    "        textData: A textData in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "        oText: The original text to be used for getting sentence.\n",
    "        useSentence: Whether to set use whole sentence as context, or just windowsize.\n",
    "        window: How many words on both sides of a mention to search for context.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidates = []\n",
    "    i = 0 # track which mention's candidates we are looking at\n",
    "    # for each mention choose the top candidate\n",
    "    for mention in textData['mentions']:\n",
    "        if len(candidates[i]) > 0:\n",
    "            if not useSentence:\n",
    "                context = getSurroundingWords(textData['text'], mention[0], window)\n",
    "            else:\n",
    "                context = getMentionSentence(oText, mention)\n",
    "            #print '\\nMention: ' + textData['text'][mention[0]]\n",
    "            #print 'Context: ' + context\n",
    "            if method2 == False:\n",
    "                bestIndex = bestRelevancy1Match(textData['text'][mention[0]], context, candidates[i])\n",
    "            else:\n",
    "                bestIndex = bestRelevancy2Match(context, candidates[i])\n",
    "            topCandidates.append([mention[1], mention[2], candidates[i][bestIndex][0]])\n",
    "        i += 1 # move to list of candidates for next mention\n",
    "        \n",
    "    return topCandidates\n",
    "\n",
    "def wikifyWord2Vec(textData, candidates, oText, useSentence = False, window = 5):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the candidates that have the highest similarity to the context.\n",
    "    Args:\n",
    "        textData: A textData in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "        oText: The original text to be used for getting sentence.\n",
    "        useSentence: Whether to set use whole sentence as context, or just windowsize.\n",
    "        window: How many words on both sides of a mention to search for context.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidates = []\n",
    "    i = 0 # track which mention's candidates we are looking at\n",
    "    # for each mention choose the top candidate\n",
    "    for mention in textData['mentions']:\n",
    "        if len(candidates[i]) > 0:\n",
    "            if not useSentence:\n",
    "                context = getSurroundingWords(textData['text'], mention[0], window, asList = True)\n",
    "            else:\n",
    "                context = getMentionSentence(oText, mention, asList = True)\n",
    "            #print '\\nMention: ' + textData['text'][mention[0]]\n",
    "            #print 'Context: ' + \" \".join(context)\n",
    "            bestIndex = bestWord2VecMatch(context, candidates[i])\n",
    "            topCandidates.append([mention[1], mention[2], candidates[i][bestIndex][0]])\n",
    "        i += 1 # move to list of candidates for next mention\n",
    "        \n",
    "    return topCandidates\n",
    "\n",
    "def wikifyCoherence(textData, candidates, ws = 5):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the candidates that have the highest coherence according to rvs pagerank method.\n",
    "    Args:\n",
    "        textData: A textData in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "        ws: How many words on both sides of a mention to search for context.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    topCands = [] # the top candidate from each candidate list\n",
    "    candsScores = coherence_scores_driver(candidates, ws, method='rvspagerank', direction=DIR_BOTH, op_method=\"keydisamb\")\n",
    "    i = -1 # track what mention we are on\n",
    "    for cScores in candsScores:\n",
    "        i += 1\n",
    "        \n",
    "        if len(cScores) == 0:\n",
    "            continue # nothing to do with this one\n",
    "            \n",
    "        bestScore = sorted(cScores, reverse = True)[0]\n",
    "        curIndex = 0\n",
    "        for score in cScores:\n",
    "            if score == bestScore:\n",
    "                topCands.append([textData['mentions'][i][1], textData['mentions'][i][2], candidates[i][curIndex][0]])\n",
    "                break\n",
    "            curIndex += 1\n",
    "            \n",
    "    return topCands\n",
    "\n",
    "def wikifyEval(text, mentionsGiven, maxC = 20, method='popular', strict = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes the text (maybe text data), and wikifies it for evaluation purposes using the desired method.\n",
    "    Args:\n",
    "        text: The string to wikify. Either as just the original string to be modified, or in the \n",
    "            form of: [[w1,w2,...], [[wid,entityId],...] if the mentions are given.\n",
    "        mentionsGiven: Whether the mentions are given to us and the text is already split.\n",
    "        maxC: The max amount of candidates to extract.\n",
    "        method: The method used to wikify.\n",
    "        strict: Whether to use such rules as minimum metion length, or minimum frequency of concept.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    if not(mentionsGiven): # if words are not in pre-split form\n",
    "        textData = mentionExtract(text) # extract mentions from text\n",
    "        oText = text # the original text\n",
    "    else: # if they are\n",
    "        textData = text\n",
    "        textData['mentions'] = mentionStartsAndEnds(textData) # put mentions in right form\n",
    "        oText = \" \".join(text['text'])\n",
    "    \n",
    "    # get rid of small mentions\n",
    "    if strict:\n",
    "        textData['mentions'] = [item for item in textData['mentions']\n",
    "                    if  len(textData['text'][item[0]]) >= MIN_MENTION_LENGTH]\n",
    "    \n",
    "    candidates = generateCandidates(textData, maxC)\n",
    "    \n",
    "    if method == 'popular':\n",
    "        wikified = wikifyPopular(textData, candidates)\n",
    "    elif method == 'relevancy1':\n",
    "        wikified = wikifyRelevancy(textData, candidates, oText, useSentence = True, window = 7)\n",
    "    elif method == 'relevancy2':\n",
    "        wikified = wikifyRelevancy(textData, candidates, oText, useSentence = True, window = 7, method2 = True)\n",
    "    elif method == 'word2vec':\n",
    "        wikified = wikifyWord2Vec(textData, candidates, oText, useSentence = False, window = 5)\n",
    "    elif method == 'coherence':\n",
    "        wikified = wikifyCoherence(textData, candidates, ws = 5)\n",
    "    \n",
    "    # get rid of very unpopular mentions\n",
    "    if strict:\n",
    "        wikified = [item for item in wikified\n",
    "                    if item[3] >= MIN_FREQUENCY]\n",
    "        \n",
    "    return wikified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wikipedia import *\n",
    "from operator import itemgetter\n",
    "import requests\n",
    "anchor2concept('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id2title(48324759L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing the Solr splitting\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from wikipedia import *\n",
    "from __future__ import division\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode\n",
    "import tagme\n",
    "\n",
    "tagme.GCUBE_TOKEN = \"f6c2ba6c-751b-4977-a94c-c140c30e9b92-843339462\"\n",
    "\n",
    "def mentionProb(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the probability that the text is a mention in Wikipedia.\n",
    "    Args:\n",
    "        text: \n",
    "    Return:\n",
    "        The probability that the text is a mention in Wikipedia.\n",
    "    \"\"\"\n",
    "    \n",
    "    totalMentions = get_mention_count(text)\n",
    "    totalAppearances = get_solr_count(text.replace(\".\", \"\"))\n",
    "    \n",
    "    if totalAppearances == 0:\n",
    "        return 0 # a mention never used probably is not a good link\n",
    "    else:\n",
    "        return totalMentions/totalAppearances\n",
    "\n",
    "def get_solr_count(s):\n",
    "    \"\"\" Gets the number of documents the string occurs \n",
    "        NOTE: Multi words should be quoted\n",
    "    Arg:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Returns:\n",
    "        The number of documents\n",
    "    \"\"\"\n",
    "\n",
    "    q='+text:(\\\"%s\\\")'%(s,)\n",
    "    qstr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'indent':'on', 'wt':'json', 'q':q, 'rows':0}\n",
    "    r = requests.get(qstr, params=params)\n",
    "    try:\n",
    "        if 'response' not in r.json():\n",
    "            return 0\n",
    "        else:\n",
    "            return r.json()['response']['numFound']\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_mention_count(s):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the amount of times that the given string appears as a mention in wikipedia.\n",
    "    Args:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Return:\n",
    "        The amount of times the given string appears as a mention in wikipedia\n",
    "    \"\"\"\n",
    "    \n",
    "    result = anchor2concept(s)\n",
    "    rSum = 0\n",
    "    for item in result:\n",
    "        rSum += item[1]\n",
    "        \n",
    "    return rSum\n",
    "\n",
    "def getTextMentions(line):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A helper method to get the mentions in an evaluable format, includes the mentions'\n",
    "        start and end.\n",
    "    Args:\n",
    "        line: The json data that has info that needs to be converted.\n",
    "    Return:\n",
    "        The mentions in the form [[start, end, text],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    mentions = []\n",
    "    curWord = 0 \n",
    "    curStart = 0\n",
    "    for mention in line['mentions']:\n",
    "        while curWord < mention[0]:\n",
    "            curStart += len(line['text'][curWord]) + 1\n",
    "            curWord += 1\n",
    "        mentions.append([curStart, curStart + len(line['text'][curWord]), line['text'][curWord]])\n",
    "        \n",
    "    return mentions\n",
    "\n",
    "def destroyExclusiveOverlaps(textData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Removes all overlaps from text data, so that only the best mention in an\n",
    "        overlap set is left.\n",
    "    Args:\n",
    "        textData: [[start, end, text, anchProb],...]\n",
    "    Return:\n",
    "        textData minus the unesescary parts of the overlapping\n",
    "    \"\"\"\n",
    "    \n",
    "    newTextData = [] # textData minus the unesescary parts of the overlapping\n",
    "    overlappingSets = [] # stores arrays of the indexes of overlapping items from textData\n",
    "    \n",
    "    # creates the overlappingSets array\n",
    "    i = 0\n",
    "    while i < len(textData)-1:\n",
    "        # even single elements considered overlapping set\n",
    "        # this is root of overlapping set\n",
    "        overlappingSets.append([i])\n",
    "        overlapIndex = len(overlappingSets) - 1\n",
    "        theBegin = textData[i][0]\n",
    "        \n",
    "        # look at next words untill not overlap\n",
    "        for j in range(i+1, len(textData)):\n",
    "            # if next word starts before endiest one ends\n",
    "            if textData[j][0] == theBegin:\n",
    "                overlappingSets[overlapIndex].append(j)\n",
    "                i = j # make sure not to repeat overlap set\n",
    "            else:\n",
    "                # add final word\n",
    "                if j == len(textData) - 1:\n",
    "                    overlappingSets.append([j])\n",
    "                break\n",
    "        i += 1\n",
    "                    \n",
    "    # get only the best overlapping element of each set\n",
    "    for oSet in overlappingSets:\n",
    "        bestIndex = 0\n",
    "        bestScore = -1\n",
    "        for i in oSet:\n",
    "            totalMentions = get_mention_count(textData[i][2])\n",
    "            totalAppearances = get_solr_count(textData[i][2].replace(\".\", \"\"))\n",
    "            if totalAppearances == 0:\n",
    "                score = 0\n",
    "            else:\n",
    "                score = totalMentions/totalAppearances\n",
    "            \n",
    "            if score > bestScore:\n",
    "                bestScore = score\n",
    "                bestIndex = i\n",
    "        \n",
    "        # put right item in new textData\n",
    "        newTextData.append(textData[bestIndex])\n",
    "        \n",
    "    return newTextData\n",
    "\n",
    "def destroyResidualOverlaps(textData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Removes all overlaps from text data, so that only the best mention in an\n",
    "        overlap set is left.\n",
    "    Args:\n",
    "        textData: [[start, end, text, anchProb],...]\n",
    "    Return:\n",
    "        textData minus the unesescary elements that overlap.\n",
    "    \"\"\"\n",
    "    \n",
    "    newTextData = [] # to be returned\n",
    "    oSet = [] # the set of current overlaps\n",
    "    rootWIndex = 0 # the word to start looking from for finding root word\n",
    "    rEnd = 0 # the end index of the root word\n",
    "    \n",
    "    # keep looping as long as overlaps\n",
    "    while True:\n",
    "        oSet = []\n",
    "        oSet.append(textData[rootWIndex])\n",
    "        for i in range(rootWIndex + 1, len(textData)):\n",
    "            # if cur start before root end\n",
    "            if textData[i][0] < textData[rootWIndex][1]:\n",
    "                oSet.append(textData[i])\n",
    "            else:\n",
    "                break # have all overlap words\n",
    "\n",
    "        \n",
    "        bestIndex = 0\n",
    "        # deal with the overlaps\n",
    "        if len(oSet) > 1:\n",
    "            bestProb = 0\n",
    "            \n",
    "            # choose the most probable\n",
    "            i = 0\n",
    "            for mention in oSet:\n",
    "                prob = mentionProb(mention[2])\n",
    "                if prob > bestProb:\n",
    "                    bestProb = prob\n",
    "                    bestIndex = i\n",
    "                i += 1\n",
    "        else:\n",
    "            rootWIndex += 1 # move up one if no overlaps\n",
    "                \n",
    "        # remove from old text data all that is not best\n",
    "        for i in range(0, len(oSet)):\n",
    "            if i <> bestIndex:\n",
    "                textData.remove(oSet[i])\n",
    "                \n",
    "        # add the best to new\n",
    "        if not (oSet[bestIndex] in newTextData):\n",
    "            newTextData.append(oSet[bestIndex])\n",
    "            \n",
    "        if rootWIndex >= len(textData):\n",
    "            break\n",
    "    \n",
    "    return newTextData\n",
    "\n",
    "def destroyInclusiveOverlaps(text, textData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Removes all overlaps from text data, so that only the best mention in an\n",
    "        overlap set is left.\n",
    "    Args:\n",
    "        text: The source text.\n",
    "        textData: [[_, start, _, end, _, alts, pop],...]\n",
    "    Return:\n",
    "        textData minus the unesescary parts of the overlapping\n",
    "    \"\"\"\n",
    "    \n",
    "    newTextData = [] # textData minus the unesescary parts of the overlapping\n",
    "    overlappingSets = [] # stores arrays of the indexes of overlapping items from textData\n",
    "    \n",
    "    # creates the overlappingSets array\n",
    "    i = 0\n",
    "    while i < len(textData)-1:\n",
    "        # even single elements considered overlapping set\n",
    "        # this is root of overlapping set\n",
    "        overlappingSets.append([i])\n",
    "        overlapIndex = len(overlappingSets) - 1\n",
    "        endiestEnd = textData[i][1]\n",
    "        \n",
    "        # look at next words untill not overlap\n",
    "        for j in range(i+1, len(textData)):\n",
    "            # if next word starts before endiest one ends\n",
    "            if textData[j][0] < endiestEnd:\n",
    "                overlappingSets[overlapIndex].append(j)\n",
    "                i = j # make sure not to repeat overlap set\n",
    "                # update endiness if needed\n",
    "                if textData[j][1] > endiestEnd:\n",
    "                    endiestEnd = textData[j][1]\n",
    "            else:\n",
    "                # add final word\n",
    "                if j == len(textData) - 1:\n",
    "                    overlappingSets.append([j])\n",
    "                break\n",
    "        i += 1\n",
    "                    \n",
    "    # get only the best overlapping element of each set\n",
    "    for oSet in overlappingSets:\n",
    "        bestIndex = 0\n",
    "        bestScore = -1\n",
    "        for i in oSet:\n",
    "            totalMentions = get_mention_count(textData[i][2])\n",
    "            totalAppearances = get_solr_count(textData[i][2].replace(\".\", \"\"))\n",
    "            if totalAppearances == 0:\n",
    "                score = 0\n",
    "            else:\n",
    "                score = totalMentions/totalAppearances\n",
    "            \n",
    "            if score > bestScore:\n",
    "                bestScore = score\n",
    "                bestIndex = i\n",
    "        \n",
    "        # put right item in new textData\n",
    "        newTextData.append(textData[bestIndex])\n",
    "        \n",
    "    return newTextData\n",
    "                    \n",
    "def getSolrMentions(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A method to split the text and try to extract mentions using Solr.\n",
    "    Args:\n",
    "        text: The text to find mentions in.\n",
    "    Return:\n",
    "        The mentions as found from our method using Solr.\n",
    "    \"\"\"\n",
    "    \n",
    "    addr = 'http://localhost:8983/solr/enwikianchors20160305/tag'\n",
    "    params={'overlaps':'ALL', 'tagsLimit':'5000', 'fl':'id','wt':'json','indent':'on'}\n",
    "    r = requests.post(addr, params=params, data=text.encode('utf-8'))\n",
    "    textData0 = r.json()['tags']\n",
    "    \n",
    "    textData = []\n",
    "    # get rid of extra un-needed Solr data, and add in anchor probability\n",
    "    for item in textData0:\n",
    "        totalMentions = get_mention_count(text[item[1]:item[3]])\n",
    "        totalAppearances = get_solr_count(text[item[1]:item[3]].replace(\".\", \"\"))\n",
    "        if totalAppearances == 0:\n",
    "            anchorProb = 0\n",
    "        else:\n",
    "            anchorProb = totalMentions/totalAppearances\n",
    "        \n",
    "        # put in the new clean textData\n",
    "        textData.append([item[1], item[3], text[item[1]:item[3]], anchorProb])\n",
    "    \n",
    "    textData = destroyExclusiveOverlaps(textData)\n",
    "    textData = destroyResidualOverlaps(textData)\n",
    "    \n",
    "    # gets the POS labels for the words\n",
    "    postrs = []\n",
    "    for item in textData:\n",
    "        postrs.append(item[2])\n",
    "    postrs = nltk.pos_tag(postrs)\n",
    "    for i in range(0,len(textData)):\n",
    "        textData[i].append(postrs[i]) # [4][1] is index of type of word\n",
    "    \n",
    "    mentions = []\n",
    "    mentionPThrsh = 0.001\n",
    "    \n",
    "    for item in textData:\n",
    "        if (item[3] >= mentionPThrsh\n",
    "                and (item[4][1][0:2] == 'NN' or item[4][1] == 'JJ')):\n",
    "            mentions.append([item[0], item[1], item[2]])\n",
    "    \n",
    "    return mentions\n",
    "\n",
    "def precision(trueMentions, otherMentions):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the precision of otherSet against the trueMentions.\n",
    "    Args:\n",
    "        trueMentions: The 'right' answers for what the mentions are.\n",
    "        otherMentions: Our mentions obtained through some means.\n",
    "    Return:\n",
    "        The precision: (# of correct mentions)/(# of found mentions)\n",
    "    \"\"\"\n",
    "    \n",
    "    numFound = len(otherMentions)\n",
    "    numCorrect = 0 # incremented in for loop\n",
    "    \n",
    "    trueIndex = 0\n",
    "    otherIndex = 0\n",
    "    \n",
    "    while trueIndex < len(trueMentions) and otherIndex < len(otherMentions):\n",
    "        # if mentions start and end on the same\n",
    "        if (trueMentions[trueIndex][0] == otherMentions[otherIndex][0]\n",
    "               and trueMentions[trueIndex][1] == otherMentions[otherIndex][1]):\n",
    "            #print ('MATCH: [' + str(trueMentions[trueIndex][0]) + ',' + str(trueMentions[trueIndex][1]) + ']' + trueMentions[trueIndex][2] \n",
    "            #       + ' <===> [' + str(otherMentions[otherIndex][0]) + ',' + str(otherMentions[otherIndex][1]) + ']' + otherMentions[otherIndex][2])\n",
    "            numCorrect += 1\n",
    "            trueIndex += 1\n",
    "            otherIndex += 1\n",
    "        # if true mention starts before the other starts\n",
    "        elif trueMentions[trueIndex][0] < otherMentions[otherIndex][0]:\n",
    "            #print ('FAIL: [' + str(trueMentions[trueIndex][0]) + ',' + str(trueMentions[trueIndex][1]) + ']' + trueMentions[trueIndex][2] \n",
    "            #       + ' <XXX> [' + str(otherMentions[otherIndex][0]) + ',' + str(otherMentions[otherIndex][1]) + ']' + otherMentions[otherIndex][2])\n",
    "            trueIndex += 1\n",
    "        # if other mention starts before the true starts (same doesnt matter)\n",
    "        elif trueMentions[trueIndex][0] >= otherMentions[otherIndex][0]:\n",
    "            #print ('FAIL: [' + str(trueMentions[trueIndex][0]) + ',' + str(trueMentions[trueIndex][1]) + ']' + trueMentions[trueIndex][2] \n",
    "            #       + ' <XXX> [' + str(otherMentions[otherIndex][0]) + ',' + str(otherMentions[otherIndex][1]) + ']' + otherMentions[otherIndex][2])\n",
    "            otherIndex += 1\n",
    "        else:\n",
    "            print 'AAAAAAAHHHHHHHHHHHHHHHHHHHHHHHHHHHHH!!!!!!!!!!!!!!!!!!!'\n",
    "\n",
    "    #print 'correct: ' + str(numCorrect) + '\\nfound: ' + str(numFound)\n",
    "    if numFound == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numFound)\n",
    "\n",
    "def recall(trueMentions, otherMentions):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the recall of otherSet against the trueMentions.\n",
    "    Args:\n",
    "        trueMentions: The 'right' answers for what the mentions are.\n",
    "        otherMentions: Our mentions obtained through some means.\n",
    "    Return:\n",
    "        The recall: (# of correct entities)/(# of actual entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numActual = len(trueMentions)\n",
    "    numCorrect = 0 # incremented in for loop)\n",
    "    \n",
    "    trueIndex = 0\n",
    "    otherIndex = 0\n",
    "    \n",
    "    while trueIndex < len(trueMentions) and otherIndex < len(otherMentions):\n",
    "        # if mentions start and end on the same\n",
    "        if (trueMentions[trueIndex][0] == otherMentions[otherIndex][0]\n",
    "               and trueMentions[trueIndex][1] == otherMentions[otherIndex][1]):\n",
    "            numCorrect += 1\n",
    "            trueIndex += 1\n",
    "            otherIndex += 1\n",
    "        # if true mention starts before the other starts\n",
    "        elif trueMentions[trueIndex][0] < otherMentions[otherIndex][0]:\n",
    "            trueIndex += 1\n",
    "        # if other mention starts before the true starts (same doesnt matter)\n",
    "        elif trueMentions[trueIndex][0] >= otherMentions[otherIndex][0]:\n",
    "            otherIndex += 1\n",
    "        \n",
    "    print 'correct: ' + str(numCorrect) + '\\nactual: ' + str(numActual)\n",
    "    if numActual == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numActual)\n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "\n",
    "performances = {}\n",
    "\n",
    "# for each dataset, run all methods\n",
    "for dataset in datasets:\n",
    "    performances[dataset['name']] = {}\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "    \n",
    "    # reset counters\n",
    "    totalPrec = 0\n",
    "    totalRec = 0\n",
    "    totalLines = 0\n",
    "\n",
    "    # each method tests all lines\n",
    "    for line in dataLines:\n",
    "\n",
    "        print str(totalLines + 1)\n",
    "\n",
    "        trueMentions = getTextMentions(line)\n",
    "        solrMentions = getSolrMentions(\" \".join(line['text']))\n",
    "        \n",
    "        \"\"\"solrMentions0 = tagme.mentions(\" \".join(line['text']))\n",
    "        solrMentions = []\n",
    "        for item in solrMentions0.mentions:\n",
    "            solrMentions.append([item.begin, item.end, item.mention])\"\"\"\n",
    "\n",
    "        prec = precision(trueMentions, solrMentions)\n",
    "        rec = recall(trueMentions, solrMentions)\n",
    "        print str(prec) + ' ' + str(rec) + '\\n'\n",
    "\n",
    "        # track results\n",
    "        totalPrec += prec\n",
    "        totalRec += rec\n",
    "        totalLines += 1\n",
    "\n",
    "    # record results for this method on this dataset\n",
    "    performances[dataset['name']] = {'Precision':totalPrec/totalLines, \n",
    "                                     'Recall':totalRec/totalLines}\n",
    "            \n",
    "print performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = \"Dalhousie University\"\n",
    "\n",
    "print get_mention_count(text)\n",
    "print get_solr_count(text)\n",
    "\n",
    "print get_mention_count(text)/get_solr_count(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```curl -X POST \\\n",
    "  'http://localhost:8983/solr/geonames/tag?overlaps=NO_SUB&tagsLimit=5000&fl=id,name,countrycode&wt=json&indent=on' \\\n",
    "  -H 'Content-Type:text/plain' -d 'Hello New York City'```\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is for testing performance of different wikification methods.\n",
    "\"\"\"\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import tagme\n",
    "\n",
    "tagme.GCUBE_TOKEN = \"f6c2ba6c-751b-4977-a94c-c140c30e9b92-843339462\"\n",
    "    \n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "#datasets = [{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')},{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')},{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "datasets = [{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "\n",
    "methods = ['popular', 'relevancy1', 'relevancy2', 'word2vec', 'coherence']\n",
    "\n",
    "#if 'word2vec' in methods: # run in different cell\n",
    "    #word2vec = gensim_loadmodel('/users/cs/amaral/cgmdir/word2vecfiles/word2vec.enwiki-20160305-replace_surface.1.0.500.5.5.15.5.5')\n",
    "\n",
    "performances = {}\n",
    "\n",
    "# for each dataset, run all methods\n",
    "for dataset in datasets:\n",
    "    performances[dataset['name']] = {}\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    \n",
    "    # put in all lines that contain proper ascii\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        \n",
    "    print dataset['name'] + '\\n'\n",
    "    \n",
    "    # run each method on the data set\n",
    "    for mthd in methods:\n",
    "        print mthd\n",
    "        print str(datetime.now()) + '\\n'\n",
    "        \n",
    "        # reset counters\n",
    "        totalPrecS = 0\n",
    "        totalPrecM = 0\n",
    "        totalRecS = 0\n",
    "        totalRecM = 0\n",
    "        totalLines = 0\n",
    "        \n",
    "        # each method tests all lines\n",
    "        for line in dataLines:\n",
    "            \n",
    "            print str(totalLines + 1)\n",
    "            \n",
    "            # original split string with mentions given\n",
    "            #resultS = wikifyEval(copy.deepcopy(line), True, maxC = 50, method = mthd)\n",
    "            resultS = []\n",
    "            # unsplit string to be manually split and mentions found\n",
    "            resultM = wikifyEval(\" \".join(line['text']), False, maxC = 50, method = mthd)\n",
    "            # get absolute text indexes and entity id of each given mention\n",
    "            trueEntities = mentionStartsAndEnds(copy.deepcopy(line), forTruth = True) # the ground truth\n",
    "                \n",
    "            ## get statistical results from true entities and results S and M\n",
    "            #precS = precision(trueEntities, resultS) # precision of pre-split\n",
    "            precS = 0\n",
    "            precM = precision(trueEntities, resultM) # precision of manual split\n",
    "            #recS = recall(trueEntities, resultS) # recall of pre-split\n",
    "            recS = 0\n",
    "            recM = recall(trueEntities, resultM) # recall of manual split\n",
    "            \n",
    "            #clear_output() # delete this after\n",
    "            #print str(precS) + ' ' + str(precM) + ' ' + str(recS) + ' ' + str(recM) + '\\n'\n",
    "            print str(precM) + ' ' + str(recM) + '\\n'\n",
    "            #print str(precS) + ' ' + str(recS) + '\\n'\n",
    "            \n",
    "            # track results\n",
    "            totalPrecS += precS\n",
    "            totalPrecM += precM\n",
    "            totalRecS += recS\n",
    "            totalRecM += recM\n",
    "            totalLines += 1\n",
    "        \n",
    "        # record results for this method on this dataset\n",
    "        # [avg precision split, avg precision manual, avg recall split, avg recall manual]\n",
    "        performances[dataset['name']][mthd] = {'S Prec':totalPrecS/totalLines, \n",
    "                                               'M Prec':totalPrecM/totalLines,\n",
    "                                              'S Rec':totalRecS/totalLines, \n",
    "                                               'M Rec':totalRecM/totalLines\n",
    "                                              }\n",
    "        \n",
    "clear_output()\n",
    "print performances\n",
    "for dataset in datasets:\n",
    "    print dataset['name']\n",
    "    for mthd in methods:\n",
    "        print (mthd + ':'\n",
    "               + '\\n    S Prec:' + str(performances[dataset['name']][mthd]['S Prec'])\n",
    "               + '\\n    S Rec:' + str(performances[dataset['name']][mthd]['S Rec'])\n",
    "               + '\\n    M Prec:' + str(performances[dataset['name']][mthd]['M Prec'])\n",
    "               + '\\n    M Rec:' + str(performances[dataset['name']][mthd]['M Rec']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2vec = gensim_loadmodel('/users/cs/amaral/cgmdir/WikipediaClean5Negative300Skip10.Ehsan/WikipediaClean5Negative300Skip10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is for testing performance of TagMe wikification method.\n",
    "\"\"\"\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import tagme\n",
    "\n",
    "tagme.GCUBE_TOKEN = \"f6c2ba6c-751b-4977-a94c-c140c30e9b92-843339462\"\n",
    "    \n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "datasets = [{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "\n",
    "performances = {}\n",
    "\n",
    "# for each dataset, run all methods\n",
    "for dataset in datasets:\n",
    "    performances[dataset['name']] = {}\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    \n",
    "    # put in all lines that contain proper ascii\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        \n",
    "    print dataset['name'] + '\\n'\n",
    "\n",
    "    print str(datetime.now()) + '\\n'\n",
    "\n",
    "    # reset counters\n",
    "    totalPrecM = 0\n",
    "    totalRecM = 0\n",
    "    totalLines = 0\n",
    "\n",
    "    # each method tests all lines\n",
    "    for line in dataLines:\n",
    "\n",
    "        print str(totalLines + 1)\n",
    "        \n",
    "        antns = tagme.annotate(\" \".join(line['text']))\n",
    "        resultM = []\n",
    "        for an in antns.get_annotations(0.005):\n",
    "            resultM.append([an.begin,an.end,title2id(an.entity_title)])\n",
    "        trueEntities = mentionStartsAndEnds(copy.deepcopy(line), forTruth = True) # the ground truth\n",
    "\n",
    "        ## get statistical results from true entities and results\n",
    "        precM = precision(trueEntities, resultM)\n",
    "        recM = recall(trueEntities, resultM)\n",
    "\n",
    "        #clear_output() # delete this after\n",
    "        print str(precM) + ' ' + str(recM) + '\\n'\n",
    "        #print str(precS) + ' ' + str(recS)\n",
    "\n",
    "        # track results\n",
    "        totalPrecM += precM\n",
    "        totalRecM += recM\n",
    "        totalLines += 1\n",
    "\n",
    "    # record results for this method on this dataset\n",
    "    # [avg precision split, avg precision manual, avg recall split, avg recall manual]\n",
    "    performances[dataset['name']] = {'Prec':totalPrecM/totalLines,\n",
    "                                          'Rec':totalRecM/totalLines}\n",
    "            \n",
    "clear_output()\n",
    "print performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Try to compare the difference between mentions where the entity is the most popular and when it is not\"\"\"\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import tagme\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generateCandidates(textData, maxC):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates up to maxC candidates for each possible mention word in phrase (most frequent).\n",
    "    Args:\n",
    "        textData: A text in split form along with its suspected mentions.\n",
    "        maxC: The max amount of candidates to accept.\n",
    "    Return:\n",
    "        The top maxC candidates for each possible mention word in textData.\n",
    "    \"\"\"\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    for mention in textData['mentions']:\n",
    "        results = sorted(anchor2concept(textData['text'][mention[0]]), key = itemgetter(1), \n",
    "                          reverse = True)\n",
    "        candidates.append(results[:maxC]) # take up to maxC of the results\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "\n",
    "maxC = 1\n",
    "corCandPosDict = {} # dictionary containing the amount of correct entities found at each index\n",
    "correctPositions = []\n",
    "incorrectPositions = []\n",
    "correctRelevancies = []\n",
    "incorrectRelevancies = []\n",
    "for i in range(0, maxC):\n",
    "    corCandPosDict[str(i)] = 0\n",
    "\n",
    "totalMentions = 0\n",
    "\n",
    "jdata = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    \n",
    "    # put in all lines\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        \n",
    "    print dataset['name'] + '\\n'\n",
    "    print str(datetime.now()) + '\\n'\n",
    "    totalLines = 0\n",
    "    \n",
    "    for line in dataLines: \n",
    "        print str(totalLines + 1)\n",
    "        \n",
    "        totalMentions += len(line['mentions'])\n",
    "        \n",
    "        oLine = copy.deepcopy(line)\n",
    "        \n",
    "        oMentions = mentionStartsAndEnds(copy.deepcopy(line), forTruth = True)\n",
    "        \n",
    "        # get in right format\n",
    "        line['mentions'] = mentionStartsAndEnds(line) # put mentions in right form\n",
    "        oText = \" \".join(line['text'])\n",
    "            \n",
    "        candss = generateCandidates(line, maxC)\n",
    "        \n",
    "        copyCands = []\n",
    "        i = 0\n",
    "        for cands in candss:\n",
    "            if len(cands) > 0 and cands[0][0] != oMentions[i][2]:\n",
    "                copyCands.append(i)\n",
    "            i += 1\n",
    "            \n",
    "        # put in file\n",
    "        if len(copyCands) > 0:\n",
    "            jdata.append({'text':oLine['text'], 'mentions':[\n",
    "                oLine['mentions'][i] for i in copyCands]})\n",
    "        \n",
    "        totalLines += 1\n",
    "        \n",
    "        #print jdata\n",
    "        \n",
    "clear_output()\n",
    "with open('/users/cs/amaral/wsd-datasets/nopop.json', 'w') as f:\n",
    "    json.dump(jdata, f)\n",
    "    \n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with open('/users/cs/amaral/wsd-datasets/nopop.json', 'w') as f:\n",
    "    for item in jdata:\n",
    "        json.dump(item, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posRels = []\n",
    "for i in range(0, len(correctPositions)):\n",
    "    posRels.append([correctPositions[i], relevancies[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "##### https://stackoverflow.com/questions/470690/how-to-automatically-generate-n-distinct-colors\n",
    "##### Uri Cohen\n",
    "import colorsys\n",
    "def _get_colors(num_colors):\n",
    "    colors=[]\n",
    "    for i in np.arange(0., 360., 360. / num_colors):\n",
    "        hue = i/360.\n",
    "        lightness = (50 + np.random.rand() * 10)/100.\n",
    "        saturation = (90 + np.random.rand() * 10)/100.\n",
    "        colors.append(colorsys.hls_to_rgb(hue, lightness, saturation))\n",
    "    return colors\n",
    "#####\n",
    "#####\n",
    "\n",
    "maxC = 50\n",
    "df = pickle.load(open('/users/cs/amaral/wikisim/storage/correct-pos-frequencies.pkl', 'rb'))\n",
    "\n",
    "# https://chrisalbon.com/python/matplotlib_percentage_stacked_bar_plot.html\n",
    "f, ax = plt.subplots(1, figsize =(20,10))\n",
    "bar_width = 1\n",
    "bar_l = [i for i in range(len(df['freq_lvl']))]\n",
    "tick_pos = [i+(bar_width/2) for i in bar_l]\n",
    "\n",
    "# get the total at each level\n",
    "totals = []\n",
    "totalIndex = -1\n",
    "for i in range(0,len(df['freq_lvl'])):\n",
    "    totals.append(0)\n",
    "    totalIndex += 1\n",
    "    for j in range(0,maxC):\n",
    "        totals[totalIndex] += df['rank: ' + str(j)][i]\n",
    "        \n",
    "# get the percentage of each rank at each level\n",
    "data_rel = []\n",
    "drIndex = -1\n",
    "for i in range(0,maxC): # for each rank\n",
    "    data_rel.append([])\n",
    "    drIndex += 1\n",
    "    for j in range(0,len(df['freq_lvl'])): # for each level\n",
    "        data_rel[drIndex].append((df['rank: ' + str(i)][j] / totals[j]) * 100)\n",
    "        \n",
    "colors = _get_colors(maxC)\n",
    "\n",
    "# get numbers for stacking bars\n",
    "stackNums = []\n",
    "stackNums.append([0]*len(df['freq_lvl'])) # last all zero\n",
    "for i in range(1,maxC):\n",
    "    stackNums.append(copy.deepcopy(stackNums[i - 1]))\n",
    "    for j in range(0,len(df['freq_lvl'])):\n",
    "        # add all of current column\n",
    "        stackNums[i][j] += (df['rank: '+str(maxC - i)][j] / totals[j]) * 100\n",
    "\n",
    "for i in range(0, maxC):\n",
    "    ax.bar(bar_l,\n",
    "          data_rel[i],\n",
    "          bottom=stackNums[maxC - i - 1],\n",
    "          label='Rank: ' + str(i),\n",
    "          alpha=1,\n",
    "          color=colors[i],\n",
    "          width=bar_width,\n",
    "          edgecolor='white')\n",
    "    \n",
    "plt.xticks(tick_pos, df['freq_lvl'])\n",
    "ax.set_ylabel('Percentage')\n",
    "ax.set_xlabel('Relevancy Threshold')\n",
    "plt.xlim([min(tick_pos)-bar_width, max(tick_pos)])\n",
    "plt.ylim(-5, 105)\n",
    "\n",
    "red_patch = mpatches.Patch(color='red', label='The red data')\n",
    "\n",
    "\n",
    "plt.legend(handles=[mpatches.Patch(color=colors[i], label='Rank: '+str(i)) for i in range(0,10)])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://chrisalbon.com/python/matplotlib_percentage_stacked_bar_plot.html\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "raw_data = {'freq_lvl': [i for i in range(300,-1,-10)]} # all the freq levels\n",
    "\n",
    "for i in range(0, maxC):\n",
    "    raw_data['rank: ' + str(i)] = []\n",
    "\n",
    "# get the frequency of each rank at each freq level\n",
    "for lvl in raw_data['freq_lvl']:\n",
    "    for i in range(0, maxC):\n",
    "        raw_data['rank: ' + str(i)].append(len([1 for posRel in posRels \n",
    "                                       if posRel[0] == i and posRel[1] > lvl]))\n",
    "        \n",
    "df = pd.DataFrame(raw_data, columns = ['first_name']\n",
    "                  .extend(['rank: ' + str(i) for i in range(0, maxC)]))\n",
    "with open('/users/cs/amaral/wikisim/storage/correct-pos-frequencies.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)\n",
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "fig.suptitle('Relevancy Score for Candidates at given rank', fontsize=32)\n",
    "plt.xlabel('Candidate Popularity Rank (zero based)', fontsize=18)\n",
    "plt.ylabel('Relevancy Score (from containing sentence)', fontsize=18)\n",
    "plt.plot(incorrectPositions, incorrectRelevancies, 'ro')\n",
    "plt.plot(correctPositions, correctRelevancies, 'bo')\n",
    "plt.legend(handles=[mpatches.Patch(color='red', label='Incorrect'), mpatches.Patch(color='blue', label='Correct')])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"This is for seeing how often correct entity shows up in candidates\"\"\"\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import tagme\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def generateCandidates1(textData, maxC, oText):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates up to maxC candidates for each possible mention word in phrase.\n",
    "    Args:\n",
    "        textData: A text in split form along with its suspected mentions.\n",
    "        maxC: The max amount of candidates to accept.\n",
    "    Return:\n",
    "        The top maxC candidates for each possible mention word in textData.\n",
    "    \"\"\"\n",
    "    \n",
    "    candidates = []\n",
    "    #print textData['text']\n",
    "    for mention in textData['mentions']:\n",
    "        \n",
    "        # get all concepts for the anchor\n",
    "        concepts = anchor2concept(textData['text'][mention[0]])\n",
    "        \n",
    "        # get the ids as string for solr query\n",
    "        strIds = ['id:' +  str(strId[0]) for strId in concepts]\n",
    "        \n",
    "        context = getMentionSentence(oText, mention)\n",
    "        context = escapeStringSolr(context)\n",
    "        mentionStr = escapeStringSolr(textData['text'][mention[0]])\n",
    "        \n",
    "        # gets the relevancy scores of all of the given potential concepts\n",
    "        addr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "        params={'fl':'id score', 'indent':'on', 'start': '0', 'rows': str(maxC),\n",
    "                'fq':\" \".join(strIds),\n",
    "                'q':'text:('+context.encode('utf-8')+')^1 title:(' + mentionStr.encode('utf-8')+')^1.35',\n",
    "                'wt':'json'}\n",
    "        r = requests.get(addr, params = params)\n",
    "        \n",
    "        solrRes = []\n",
    "        try:\n",
    "            if not ('response' not in r.json()\n",
    "                   or 'docs' not in r.json()['response']\n",
    "                   or len(r.json()['response']['docs']) == 0):\n",
    "                for doc in r.json()['response']['docs'][:maxC]:\n",
    "                    freq = 0\n",
    "                    for concept in concepts:\n",
    "                        # find concept frequency\n",
    "                        if concept[0] == int(doc['id']):\n",
    "                            freq = concept[1]\n",
    "                    solrRes.append([long(doc['id']), freq, doc['score']])\n",
    "        except:\n",
    "            solrRes = []\n",
    "                \n",
    "        # sort by frequency\n",
    "        solrRes = sorted(solrRes, key = itemgetter(1), reverse = True)\n",
    "        \n",
    "        #print '\\nMention: ' + textData['text'][mention[0]]\n",
    "        #for res in solrRes:\n",
    "        #    print '[' + id2title(res[0]) + '] -> freq: ' + str(res[1]) + ', rel: ' + str(res[2])\n",
    "        \n",
    "        candidates.append(solrRes) # take up to maxC of the results\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def generateCandidates2(textData, maxC):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates up to maxC candidates for each possible mention word in phrase (most frequent).\n",
    "    Args:\n",
    "        textData: A text in split form along with its suspected mentions.\n",
    "        maxC: The max amount of candidates to accept.\n",
    "    Return:\n",
    "        The top maxC candidates for each possible mention word in textData.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    \n",
    "    for mention in textData['mentions']:\n",
    "        results = sorted(anchor2concept(textData['text'][mention[0]]), key = itemgetter(1), \n",
    "                          reverse = True)\n",
    "        candidates.append(results[:maxC]) # take up to maxC of the results\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def generateCandidates3(textData, maxC):\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    for mention in textData['mentions']:\n",
    "        anchors = anchor2concept(textData['text'][mention[0]])\n",
    "        entities = []\n",
    "        \n",
    "        for anchor in anchors:\n",
    "            wanchors = id2anchor(anchor[0]) # get all anchors of the id in this anchor\n",
    "            totalFreq = 0\n",
    "            for wanchor in wanchors:\n",
    "                totalFreq += wanchor[1]\n",
    "            \n",
    "            entities.append([anchor[0], totalFreq])\n",
    "        \n",
    "        results = sorted(entities, key = itemgetter(1), reverse = True)\n",
    "        \n",
    "        candidates.append(results[:maxC]) # take up to maxC of the results\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def generateCandidates5(textData, maxC):\n",
    "    \n",
    "    candidates = []\n",
    "    #print textData['text']\n",
    "    for mention in textData['mentions']:\n",
    "        \n",
    "        # get all concepts for the anchor\n",
    "        concepts = anchor2concept(textData['text'][mention[0]])\n",
    "        \n",
    "        # get the ids as string for solr query\n",
    "        strIds = ['id:' +  str(strId[0]) for strId in concepts]\n",
    "        \n",
    "        context = []\n",
    "        \n",
    "        for mention2 in textData['mentions']:\n",
    "            if mention2 <> mention:\n",
    "                context += escapeStringSolr(textData['text'][mention2[0]])\n",
    "        context = \" \".join(context)\n",
    "        mentionStr = escapeStringSolr(textData['text'][mention[0]])\n",
    "        \n",
    "        # gets the relevancy scores of all of the given potential concepts\n",
    "        addr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "        params={'fl':'id score', 'indent':'on', 'start': '0', 'rows': str(maxC),\n",
    "                'fq':\" \".join(strIds),\n",
    "                'q':'text:('+context.encode('utf-8')+')^1 title:(' + mentionStr.encode('utf-8')+')^1.35',\n",
    "                'wt':'json'}\n",
    "        r = requests.get(addr, params = params)\n",
    "        \n",
    "        solrRes = []\n",
    "        try:\n",
    "            if not ('response' not in r.json()\n",
    "                   or 'docs' not in r.json()['response']\n",
    "                   or len(r.json()['response']['docs']) == 0):\n",
    "                for doc in r.json()['response']['docs'][:maxC]:\n",
    "                    freq = 0\n",
    "                    for concept in concepts:\n",
    "                        # find concept frequency\n",
    "                        if concept[0] == int(doc['id']):\n",
    "                            freq = concept[1]\n",
    "                    solrRes.append([long(doc['id']), freq, doc['score']])\n",
    "        except:\n",
    "            solrRes = []\n",
    "        \n",
    "        #print '\\nMention: ' + textData['text'][mention[0]]\n",
    "        #for res in solrRes:\n",
    "        #    print '[' + id2title(res[0]) + '] -> freq: ' + str(res[1]) + ', rel: ' + str(res[2])\n",
    "        \n",
    "        candidates.append(solrRes) # take up to maxC of the results\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "\n",
    "maxC = 20\n",
    "correctCands1 = {} # dictionary containing the amount of correct entities found at each index\n",
    "correctCands2 = {} # dictionary containing the amount of correct entities found at each index\n",
    "correctCands3 = {} # dictionary containing the amount of correct entities found at each index\n",
    "correctCands4 = {} # dictionary containing the amount of correct entities found at each index\n",
    "correctCands5 = {} # dictionary containing the amount of correct entities found at each index\n",
    "for i in range(-1, maxC):\n",
    "    correctCands1[str(i)] = 0\n",
    "    correctCands2[str(i)] = 0\n",
    "    correctCands3[str(i)] = 0\n",
    "    correctCands4[str(i)] = 0\n",
    "    correctCands5[str(i)] = 0\n",
    "\n",
    "totalMentions = 0\n",
    "\n",
    "for dataset in datasets:\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    \n",
    "    # put in all lines\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        \n",
    "    print dataset['name'] + '\\n'\n",
    "    print str(datetime.now()) + '\\n'\n",
    "    totalLines = 0\n",
    "    \n",
    "    for line in dataLines: \n",
    "        print str(totalLines + 1)\n",
    "        \n",
    "        totalMentions += len(line['mentions'])\n",
    "        \n",
    "        oMentions = mentionStartsAndEnds(copy.deepcopy(line), forTruth = True)\n",
    "        \n",
    "        # get in right format\n",
    "        line['mentions'] = mentionStartsAndEnds(line) # put mentions in right form\n",
    "        oText = \" \".join(line['text'])\n",
    "            \n",
    "        cands1 = generateCandidates1(line, maxC, oText)\n",
    "        cands2 = generateCandidates2(line, maxC)\n",
    "        cands3 = generateCandidates3(line, maxC)\n",
    "        cands4 = []\n",
    "        for cands in cands1:\n",
    "            cands4.append(sorted(cands, key = itemgetter(2), reverse = True))\n",
    "        cands5 = generateCandidates5(line, maxC)\n",
    "        \n",
    "        i = 0\n",
    "        for cand in cands1:\n",
    "            corCand = False\n",
    "            for j in range(0, len(cand)):\n",
    "                if cand[j][0] == oMentions[i][2]:\n",
    "                    correctCands1[str(j)] += 1\n",
    "                    corCand = True\n",
    "                    break\n",
    "            if corCand == False:\n",
    "                correctCands1['-1'] += 1\n",
    "            i += 1\n",
    "            \n",
    "        i = 0\n",
    "        for cand in cands2:\n",
    "            corCand = False\n",
    "            for j in range(0, len(cand)):\n",
    "                if cand[j][0] == oMentions[i][2]:\n",
    "                    correctCands2[str(j)] += 1\n",
    "                    corCand = True\n",
    "                    break\n",
    "            if corCand == False:\n",
    "                correctCands2['-1'] += 1\n",
    "            i += 1\n",
    "            \n",
    "        i = 0\n",
    "        for cand in cands3:\n",
    "            corCand = False\n",
    "            for j in range(0, len(cand)):\n",
    "                if cand[j][0] == oMentions[i][2]:\n",
    "                    correctCands3[str(j)] += 1\n",
    "                    corCand = True\n",
    "                    break\n",
    "            if corCand == False:\n",
    "                correctCands3['-1'] += 1\n",
    "            i += 1\n",
    "            \n",
    "        i = 0\n",
    "        for cand in cands4:\n",
    "            corCand = False\n",
    "            for j in range(0, len(cand)):\n",
    "                if cand[j][0] == oMentions[i][2]:\n",
    "                    correctCands4[str(j)] += 1\n",
    "                    corCand = True\n",
    "                    break\n",
    "            if corCand == False:\n",
    "                correctCands4['-1'] += 1\n",
    "            i += 1\n",
    "            \n",
    "        i = 0\n",
    "        for cand in cands5:\n",
    "            corCand = False\n",
    "            for j in range(0, len(cand)):\n",
    "                if cand[j][0] == oMentions[i][2]:\n",
    "                    correctCands5[str(j)] += 1\n",
    "                    corCand = True\n",
    "                    break\n",
    "            if corCand == False:\n",
    "                correctCands5['-1'] += 1\n",
    "            i += 1\n",
    "            \n",
    "            \n",
    "        totalLines += 1\n",
    "        \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "candsPopRel = []\n",
    "for i in range(-1,maxC):\n",
    "    candsPopRel.append(correctCands1[str(i)])\n",
    "    \n",
    "candsPop = []\n",
    "for i in range(-1,maxC):\n",
    "    candsPop.append(correctCands2[str(i)])\n",
    "    \n",
    "candsPopPop = []\n",
    "for i in range(-1,maxC):\n",
    "    candsPopPop.append(correctCands3[str(i)])\n",
    "    \n",
    "candsRelSentence = []\n",
    "for i in range(-1,maxC):\n",
    "    candsRelSentence.append(correctCands4[str(i)])\n",
    "    \n",
    "candsRelMentions = []\n",
    "for i in range(-1,maxC):\n",
    "    candsRelMentions.append(correctCands5[str(i)])\n",
    "    \n",
    "x = range(-1, maxC)\n",
    "\n",
    "print 'Total Mentions: ' + str(totalMentions)\n",
    "\n",
    "\n",
    "plt.bar(x, candsPopRel, 0.5, color='red')\n",
    "plt.xlabel('Most Popular of most Relevant Rank')\n",
    "plt.ylabel('Correct Entities')\n",
    "plt.title('Correct Entities at given Rank')\n",
    "plt.show()\n",
    "print 'Misplaced: ' + str(correctCands1[str(-1)])\n",
    "print str(candsPopRel) + '\\n\\n'\n",
    "\n",
    "\n",
    "plt.bar(x, candsPop, 0.5, color='orange')\n",
    "plt.xlabel('Popularity v1 Rank')\n",
    "plt.ylabel('Correct Entities')\n",
    "plt.title('Correct Entities at given Rank')\n",
    "plt.show()\n",
    "print 'Misplaced: ' + str(correctCands2[str(-1)])\n",
    "print str(candsPop) + '\\n\\n'\n",
    "\n",
    "\n",
    "plt.bar(x, candsPopPop, 0.5, color='green')\n",
    "plt.xlabel('Popularity v2 Rank')\n",
    "plt.ylabel('Correct Entities')\n",
    "plt.title('Correct Entities at given Rank')\n",
    "plt.show()\n",
    "print 'Misplaced: ' + str(correctCands3[str(-1)])\n",
    "print str(candsPopPop) + '\\n\\n'\n",
    "\n",
    "\n",
    "plt.bar(x, candsRelSentence, 0.5, color='blue')\n",
    "plt.xlabel('Sentence Relevancy Rank')\n",
    "plt.ylabel('Correct Entities')\n",
    "plt.title('Correct Entities at given Rank')\n",
    "plt.show()\n",
    "print 'Misplaced: ' + str(correctCands4[str(-1)])\n",
    "print str(candsRelSentence) + '\\n\\n'\n",
    "\n",
    "\n",
    "plt.bar(x, candsRelMentions, 0.5, color='purple')\n",
    "plt.xlabel('Mentions Relevancy Rank')\n",
    "plt.ylabel('Correct Entities')\n",
    "plt.title('Correct Entities at given Rank')\n",
    "plt.show()\n",
    "print 'Misplaced: ' + str(correctCands5[str(-1)])\n",
    "print str(candsRelMentions) + '\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "concepts = anchor2concept('Tiger')\n",
    "concepts = sorted(concepts, key = itemgetter(1), reverse = True)[:50]\n",
    "for concept in concepts:\n",
    "    print 'id:' + str(concept[0]) + ' ' + id2title(concept[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PRP$': 1, 'VBG': 26, 'VBD': 17, 'VBN': 60, 'VBP': 7, 'WDT': 1, 'JJ': 1084, 'VBZ': 17, 'DT': 2, '$': 1, 'NN': 1946, 'FW': 2, 'POS': 1, '.': 2, 'PRP': 2, 'RB': 15, 'NNS': 569, 'NNP': 22726, 'VB': 31, 'CC': 1, 'CD': 347, 'IN': 4, 'MD': 2, 'NNPS': 212, 'JJS': 11, 'JJR': 2, 'SYM': 3}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Test some of the POS stuff\"\"\"\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import tagme\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "\n",
    "totalMentions = 0\n",
    "\n",
    "posDict = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    \n",
    "    # put in all lines\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        \n",
    "    print dataset['name'] + '\\n'\n",
    "    print str(datetime.now()) + '\\n'\n",
    "    \n",
    "    totalLines = 0\n",
    "    for line in dataLines: \n",
    "        print str(totalLines + 1)\n",
    "        totalMentions += len(line['mentions'])\n",
    "        pos = nltk.pos_tag(line['text'])\n",
    "        for mnt in line['mentions']:\n",
    "            if str(pos[mnt[0]][1]) not in posDict:\n",
    "                posDict[str(pos[mnt[0]][1])] = 1\n",
    "            else:\n",
    "                posDict[str(pos[mnt[0]][1])] += 1\n",
    "                \n",
    "        totalLines += 1\n",
    "        \n",
    "clear_output()\n",
    "print posDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
