{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wikipedia import *\n",
    "from operator import itemgetter\n",
    "import requests\n",
    "anchor2concept('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id2title(48324759L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```curl -X POST \\\n",
    "  'http://localhost:8983/solr/geonames/tag?overlaps=NO_SUB&tagsLimit=5000&fl=id,name,countrycode&wt=json&indent=on' \\\n",
    "  -H 'Content-Type:text/plain' -d 'Hello New York City'```\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wikipedia import *\n",
    "from operator import itemgetter\n",
    "import requests\n",
    "\n",
    "MIN_MENTION_LENGTH = 3 # mentions must be at least this long\n",
    "MIN_FREQUENCY = 20 # anchor with frequency below is ignored\n",
    "\n",
    "def stripSmallMentions(potAnchors):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Removes potential anchors with mentions that are too small from the list.\n",
    "    Args:\n",
    "        potAnchors: The list of potential anchors, along with some additional information.\n",
    "            [{'start', 'end', 'mention', 'mention variations'},...]\n",
    "    Return:\n",
    "        A new list of potential anchors.\n",
    "    \"\"\"\n",
    "    newPotAnchors = [] # the new list\n",
    "    for potAnchor in potAnchors:\n",
    "        if potAnchor['end'] - potAnchor['start'] >= MIN_MENTION_LENGTH:\n",
    "            newPotAnchors.append(potAnchor)\n",
    "    \n",
    "    return newPotAnchors\n",
    "\n",
    "def getMostFrequentConcept(mentions):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Finds the mention with the candidate concept with the most frequency.\n",
    "    Args:\n",
    "        mentions: A list of mentions to look for the most popular in.\n",
    "    Return:\n",
    "        A dictionary of the form {'mention', 'conceptId', 'freq'}.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The inputted mentions along with the frequency of thier most popular concept\n",
    "    mentionConceptFreqs = []\n",
    "    for mention in mentions:\n",
    "        # gets the most frequent concept from the current asr\n",
    "        mostFrequent = sorted(anchor2concept(mention), key = itemgetter(1), reverse = True)[0]\n",
    "        # mostFrequent[0] is conceptId, mostFrequent[1] is frequency of that concept\n",
    "        mentionConceptFreqs.append((mention, mostFrequent[0], mostFrequent[1]))\n",
    "        \n",
    "    # get the mention with the highest freqency\n",
    "    bestMention = sorted(mentionConceptFreqs, key = itemgetter(2), reverse = True)[0]\n",
    "    \n",
    "    bestMentionDict = {}\n",
    "    bestMentionDict['mention'] = bestMention[0]\n",
    "    bestMentionDict['conceptId'] = bestMention[1]\n",
    "    bestMentionDict['freq'] = bestMention[2]\n",
    "    \n",
    "    return bestMentionDict\n",
    "\n",
    "def wikifyPopular(potAnchors, useOriginalMention):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes in a list potential anchors and returns the resulting anchors.\n",
    "    Args:\n",
    "        potAnchors: The list of potential anchors, along with some additional information.\n",
    "            [{'start', 'end', 'mention', 'mentionVars'},...]\n",
    "        useOriginalMention: Whether to use the mention from the original or one of the word forms.\n",
    "    Return:\n",
    "        The potential anchors along with the corresponding concept and frequency of that concept.\n",
    "    \"\"\"\n",
    "    \n",
    "    # if not using original mention, use the mention variation with the most frequent results\n",
    "    # either way adds 'conceptId', and 'freq'\n",
    "    if useOriginalMention == False:\n",
    "        for potAnchor in potAnchors:\n",
    "            bestMention = getMostFrequentConcept(potAnchor['mentionVars'])\n",
    "            potAnchor['conceptId'] = bestMention['conceptId']\n",
    "            potAnchor['freq'] = bestMention['freq']\n",
    "    else:\n",
    "        for potAnchor in potAnchors:\n",
    "            mentionData = sorted(anchor2concept(potAnchor['mention']), \n",
    "                                 key = itemgetter(1), reverse = True)[0]\n",
    "            potAnchor['conceptId'] = mentionData[0]\n",
    "            potAnchor['freq'] = mentionData[1]\n",
    "            \n",
    "    return potAnchors\n",
    "\n",
    "def wikify(query, useOriginalMention, method='popular'):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes the query string, and wikifies it using the desired method.\n",
    "    Args:\n",
    "        query: The string to wikify.\n",
    "        mehtod: The method used to wikify.\n",
    "        useOriginalMention: Whether to use mention from the original or a potential variation.\n",
    "    Return:\n",
    "        The anchors along with their best matched concept from wikipedia.\n",
    "    \"\"\"\n",
    "    \n",
    "    # first get the potential anchors from solr\n",
    "    addr = 'http://localhost:8983/solr/enwikianchors20160305/tag'\n",
    "    params={'overlaps':'NO_SUB', 'tagsLimit':'5000', 'fl':'id','wt':'json','indent':'on'}\n",
    "    r = requests.post(addr, params=params, data=query)\n",
    "    queryResult = r.json()['tags']\n",
    "    \n",
    "    # an array of dictionaries to hold the data of each potential anchor\n",
    "    potAnchors = [] \n",
    "    # convert queryResult to potAnchors (much cleaner)\n",
    "    for record in queryResult:\n",
    "        potAnchors.append({'start':record[1], 'end':record[3], \n",
    "                            'mention':query[record[1]:record[3]],\n",
    "                          'mentionVars':record[5]})\n",
    "        \n",
    "    # don't use any potential anchors below size threshold\n",
    "    potAnchors = stripSmallMentions(potAnchors)\n",
    "    \n",
    "    if method == 'popular':\n",
    "        potAnchors = wikifyPopular(potAnchors, useOriginalMention)\n",
    "    \n",
    "    # deal with overlap on anchors here\n",
    "    \n",
    "    # return anchors in finalized form\n",
    "    anchors = []\n",
    "    for potAnchor in potAnchors:\n",
    "        if potAnchor['freq'] >= MIN_FREQUENCY: \n",
    "            anchors.append({'start':potAnchor['start'], 'end':potAnchor['end'],\n",
    "                            'mention':potAnchor['mention'],\n",
    "                           'wikiTitle':id2title(potAnchor['conceptId'])})\n",
    "    \n",
    "    return anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluation of wikifications\n",
    "\"\"\"\n",
    "\n",
    "from wikipedia import *\n",
    "from operator import itemgetter\n",
    "import requests\n",
    "import json\n",
    "from __future__ import division\n",
    "\n",
    "MIN_MENTION_LENGTH = 3 # mentions must be at least this long\n",
    "MIN_FREQUENCY = 20 # anchor with frequency below is ignored\n",
    "     \n",
    "def splitWords(phrase):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes in a phrase and splits it into the different word / possible entities.\n",
    "    Args:\n",
    "        phrase: The text to be split.\n",
    "    Return:\n",
    "        The text split it into the different word / possible entities.\n",
    "    \"\"\"\n",
    "    \n",
    "    addr = 'http://localhost:8983/solr/enwikianchors20160305/tag'\n",
    "    params={'overlaps':'NO_SUB', 'tagsLimit':'5000', 'fl':'id','wt':'json','indent':'on'}\n",
    "    r = requests.post(addr, params=params, data=phrase)\n",
    "    textData = r.json()['tags']\n",
    "    \n",
    "    splitPhrase = []\n",
    "    \n",
    "    for item in textData:\n",
    "        splitPhrase.append(phrase[item[1]:item[3]])\n",
    "    \n",
    "    return splitPhrase\n",
    "\n",
    "def precision(truthSet, mySet):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the precision of mySet against the truthSet.\n",
    "    Args:\n",
    "        truthSet: The 'right' answers for what the entities are.\n",
    "        mySet: My code's output for what it thinks the right entities are.\n",
    "    Return:\n",
    "        The precision: (# of correct entities)/(# of found entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numFound = len(mySet)\n",
    "    numCorrect = 0 # incremented in for loop\n",
    "    \n",
    "    # find all correct\n",
    "    for entity1 in mySet:\n",
    "        for entity2 in truthSet:\n",
    "            if entity1[1] == title2id(entity2[1]):\n",
    "                numCorrect += 1\n",
    "                break\n",
    "    print 'correct: ' + str(numCorrect) + '\\nfound: ' + str(numFound)\n",
    "    if numFound == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numFound)\n",
    "\n",
    "def recall(truthSet, mySet):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the recall of mySet against the truthSet.\n",
    "    Args:\n",
    "        truthSet: The 'right' answers for what the entities are.\n",
    "        mySet: My code's output for what it thinks the right entities are.\n",
    "    Return:\n",
    "        The recall: (# of correct entities)/(# of actual entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numActual = len(truthSet)\n",
    "    numCorrect = 0 # incremented in for loop\n",
    "    \n",
    "    # find all correct\n",
    "    for entity1 in mySet:\n",
    "        for entity2 in truthSet:\n",
    "            if entity1[1] == title2id(entity2[1]):\n",
    "                numCorrect += 1\n",
    "                break\n",
    "                \n",
    "    if numActual == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numActual)\n",
    "    \n",
    "def wikifyPopular(phrase):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes in a phrase split into different words or anchors and returns the most likely entities\n",
    "        in the phrase based on popularity method.\n",
    "    Args:\n",
    "        phrase: The original phrase in split form: [w1, w2, ...]\n",
    "    Return:\n",
    "        The anchors/word ids along with the corresponding entity id. Of the form: \n",
    "        [[wid, entityId, entityFreq],...]\n",
    "    \"\"\"\n",
    "    \n",
    "    proposedEntities = []\n",
    "\n",
    "    i = 0 # counter to see what word this is on\n",
    "    for word in phrase:\n",
    "        results = sorted(anchor2concept(word), key = itemgetter(1), \n",
    "                          reverse = True)\n",
    "        if len(results) > 0: # some results\n",
    "            entity = results[0]\n",
    "            proposedEntities.append([i, entity[0], entity[1]])\n",
    "            \n",
    "        i += 1 # on to the next word\n",
    "            \n",
    "    return proposedEntities\n",
    "\n",
    "def wikifyPopularMentionsGiven(data):\n",
    "    \n",
    "    proposedEntities = []\n",
    "\n",
    "    for mention in data['mentions']:\n",
    "        results = sorted(anchor2concept(data['text'][mention[0]]), key = itemgetter(1), \n",
    "                          reverse = True)\n",
    "        if len(results) > 0: # some results\n",
    "            entity = results[0]\n",
    "            proposedEntities.append([mention[0], entity[0], entity[1]])\n",
    "            \n",
    "    return proposedEntities\n",
    "\n",
    "def wikifyEval(phrase, preSplitWords, method='popular', strict = True):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes the phrase string, and wikifies it for evaluation purposes using the desired method.\n",
    "    Args:\n",
    "        phrase: The string to wikify. Either as just the original string to be modified, or in the \n",
    "            form of: [[w1,w2,...], [[wid,entityId,freq],...], with the second part blank and filled in by \n",
    "            this method.\n",
    "        preSplitWords: Whether to use pre-split words from the dataset, or use our own method of\n",
    "            splitting words.\n",
    "        mentionsGiven: Whether the mentions are given to us.\n",
    "        method: The method used to wikify.\n",
    "        strict: Whether to use such rules as minimum metion length, or minimum frequency of concept.\n",
    "    Return:\n",
    "        The original split text and the anchors along with their best matched concept from wikipedia.\n",
    "        Of the form: [[w1,w2,...], [[wid,entityId,freq],...]]\n",
    "    \"\"\"\n",
    "    \n",
    "    # words are not in pre-split form\n",
    "    if not(preSplitWords):\n",
    "        phrase = splitWords(phrase) # modify phrase into split form\n",
    "    \n",
    "    #wikified has form: [[w1,w2,...], [[wid,entityId,freq],...], with the second part blank and filled in by \n",
    "    #one of the methods below.\n",
    "    wikified = [phrase]\n",
    "    \n",
    "    if method == 'popular':\n",
    "        wikified.append(wikifyPopular(phrase))\n",
    "    \n",
    "    # small clean-up to do\n",
    "    if strict:\n",
    "        wikified[1] = [item for item in wikified[1]\n",
    "                    if not (item[2] < MIN_FREQUENCY or len(phrase[item[0]]) < MIN_MENTION_LENGTH)]\n",
    "    \n",
    "    # remove duplicates\n",
    "    idsHad = [] # a list of entities to check for duplicates\n",
    "    newWikified1 = [] # to replace old wikified[1]\n",
    "    for item in wikified[1]:\n",
    "        if item[1] not in idsHad:\n",
    "            newWikified1.append(item)\n",
    "            idsHad.append(item[1])\n",
    "    wikified[1] = newWikified1\n",
    "        \n",
    "    return wikified\n",
    "\n",
    "def wikifyEvalMentionsGiven(data, method='popular', strict = True):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes the data and returns the most likely entity based on the given method.\n",
    "    Args:\n",
    "        phrase: The split phrase along with the mentions.\n",
    "        method: The method used to wikify.\n",
    "        strict: Whether to use such rules as minimum metion length, or minimum frequency of concept.\n",
    "    Return:\n",
    "        The data with its most likely entities.\n",
    "    \"\"\"\n",
    "    wikified = [data['text']]\n",
    "    \n",
    "    if method == 'popular':\n",
    "        wikified.append(wikifyPopularMentionsGiven(data))\n",
    "    \n",
    "    # small clean-up to do\n",
    "    if strict:\n",
    "        wikified[1] = [item for item in wikified[1]\n",
    "                    if not (item[2] < MIN_FREQUENCY or len(phrase[item[0]]) < MIN_MENTION_LENGTH)]\n",
    "        \n",
    "    return wikified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kore\n",
      "correct: 0\n",
      "found: 2\n",
      "0.0 0.0\n",
      "1\n",
      "\n",
      "correct: 0\n",
      "found: 2\n",
      "0.0 0.0\n",
      "2\n",
      "\n",
      "correct: 1\n",
      "found: 2\n",
      "0.5 0.5\n",
      "3\n",
      "\n",
      "correct: 0\n",
      "found: 2\n",
      "0.0 0.0\n",
      "4\n",
      "\n",
      "correct: 1\n",
      "found: 1\n",
      "1.0 1.0\n",
      "5\n",
      "\n",
      "correct: 1\n",
      "found: 2\n",
      "0.5 0.5\n",
      "6\n",
      "\n",
      "correct: 0\n",
      "found: 3\n",
      "0.0 0.0\n",
      "7\n",
      "\n",
      "correct: 1\n",
      "found: 3\n",
      "0.333333333333 0.333333333333\n",
      "8\n",
      "\n",
      "correct: 0\n",
      "found: 2\n",
      "0.0 0.0\n",
      "9\n",
      "\n",
      "correct: 2\n",
      "found: 5\n",
      "0.4 0.4\n",
      "10\n",
      "\n",
      "correct: 2\n",
      "found: 4\n",
      "0.5 0.5\n",
      "11\n",
      "\n",
      "correct: 1\n",
      "found: 3\n",
      "0.333333333333 0.333333333333\n",
      "12\n",
      "\n",
      "correct: 0\n",
      "found: 3\n",
      "0.0 0.0\n",
      "13\n",
      "\n",
      "correct: 3\n",
      "found: 5\n",
      "0.6 0.6\n",
      "14\n",
      "\n",
      "correct: 3\n",
      "found: 6\n",
      "0.5 0.5\n",
      "15\n",
      "\n",
      "correct: 0\n",
      "found: 3\n",
      "0.0 0.0\n",
      "16\n",
      "\n",
      "correct: 1\n",
      "found: 3\n",
      "0.333333333333 0.333333333333\n",
      "17\n",
      "\n",
      "correct: 1\n",
      "found: 3\n",
      "0.333333333333 0.333333333333\n",
      "18\n",
      "\n",
      "correct: 1\n",
      "found: 5\n",
      "0.2 0.2\n",
      "19\n",
      "\n",
      "correct: 2\n",
      "found: 4\n",
      "0.5 0.5\n",
      "20\n",
      "\n",
      "correct: 2\n",
      "found: 3\n",
      "0.666666666667 0.666666666667\n",
      "21\n",
      "\n",
      "correct: 3\n",
      "found: 4\n",
      "0.75 0.75\n",
      "22\n",
      "\n",
      "correct: 1\n",
      "found: 3\n",
      "0.333333333333 0.333333333333\n",
      "23\n",
      "\n",
      "correct: 0\n",
      "found: 4\n",
      "0.0 0.0\n",
      "24\n",
      "\n",
      "correct: 0\n",
      "found: 2\n",
      "0.0 0.0\n",
      "25\n",
      "\n",
      "correct: 2\n",
      "found: 3\n",
      "0.666666666667 0.666666666667\n",
      "26\n",
      "\n",
      "correct: 1\n",
      "found: 2\n",
      "0.5 0.5\n",
      "27\n",
      "\n",
      "correct: 2\n",
      "found: 3\n",
      "0.666666666667 0.666666666667\n",
      "28\n",
      "\n",
      "correct: 0\n",
      "found: 3\n",
      "0.0 0.0\n",
      "29\n",
      "\n",
      "correct: 2\n",
      "found: 2\n",
      "1.0 1.0\n",
      "30\n",
      "\n",
      "correct: 1\n",
      "found: 2\n",
      "0.5 0.5\n",
      "31\n",
      "\n",
      "correct: 1\n",
      "found: 2\n",
      "0.5 0.5\n",
      "32\n",
      "\n",
      "correct: 0\n",
      "found: 2\n",
      "0.0 0.0\n",
      "33\n",
      "\n",
      "correct: 0\n",
      "found: 1\n",
      "0.0 0.0\n",
      "34\n",
      "\n",
      "correct: 0\n",
      "found: 3\n",
      "0.0 0.0\n",
      "35\n",
      "\n",
      "correct: 0\n",
      "found: 2\n",
      "0.0 0.0\n",
      "36\n",
      "\n",
      "correct: 1\n",
      "found: 2\n",
      "0.5 0.333333333333\n",
      "37\n",
      "\n",
      "correct: 0\n",
      "found: 3\n",
      "0.0 0.0\n",
      "38\n",
      "\n",
      "correct: 0\n",
      "found: 2\n",
      "0.0 0.0\n",
      "39\n",
      "\n",
      "correct: 1\n",
      "found: 2\n",
      "0.5 0.5\n",
      "40\n",
      "\n",
      "correct: 4\n",
      "found: 4\n",
      "1.0 1.0\n",
      "41\n",
      "\n",
      "correct: 3\n",
      "found: 4\n",
      "0.75 0.75\n",
      "42\n",
      "\n",
      "correct: 3\n",
      "found: 3\n",
      "1.0 1.0\n",
      "43\n",
      "\n",
      "correct: 1\n",
      "found: 4\n",
      "0.25 0.25\n",
      "44\n",
      "\n",
      "correct: 1\n",
      "found: 2\n",
      "0.5 0.5\n",
      "45\n",
      "\n",
      "correct: 1\n",
      "found: 2\n",
      "0.5 0.5\n",
      "46\n",
      "\n",
      "correct: 1\n",
      "found: 2\n",
      "0.5 0.5\n",
      "47\n",
      "\n",
      "correct: 1\n",
      "found: 2\n",
      "0.5 0.5\n",
      "48\n",
      "\n",
      "correct: 2\n",
      "found: 3\n",
      "0.666666666667 0.666666666667\n",
      "49\n",
      "\n",
      "correct: 0\n",
      "found: 1\n",
      "0.0 0.0\n",
      "50\n",
      "\n",
      "AQUAINT\n",
      "correct: 10\n",
      "found: 10\n",
      "1.0 0.909090909091\n",
      "1\n",
      "\n",
      "correct: 9\n",
      "found: 10\n",
      "0.9 0.75\n",
      "2\n",
      "\n",
      "correct: 12\n",
      "found: 14\n",
      "0.857142857143 0.857142857143\n",
      "3\n",
      "\n",
      "correct: 13\n",
      "found: 15\n",
      "0.866666666667 0.866666666667\n",
      "4\n",
      "\n",
      "correct: 11\n",
      "found: 11\n",
      "1.0 0.916666666667\n",
      "5\n",
      "\n",
      "correct: 6\n",
      "found: 7\n",
      "0.857142857143 0.666666666667\n",
      "6\n",
      "\n",
      "correct: 12\n",
      "found: 15\n",
      "0.8 0.8\n",
      "7\n",
      "\n",
      "correct: 4\n",
      "found: 5\n",
      "0.8 0.666666666667\n",
      "8\n",
      "\n",
      "correct: 15\n",
      "found: 16\n",
      "0.9375 0.9375\n",
      "9\n",
      "\n",
      "correct: 17\n",
      "found: 18\n",
      "0.944444444444 0.944444444444\n",
      "10\n",
      "\n",
      "correct: 12\n",
      "found: 18\n",
      "0.666666666667 0.666666666667\n",
      "11\n",
      "\n",
      "correct: 13\n",
      "found: 14\n",
      "0.928571428571 0.866666666667\n",
      "12\n",
      "\n",
      "correct: 17\n",
      "found: 22\n",
      "0.772727272727 0.772727272727\n",
      "13\n",
      "\n",
      "correct: 10\n",
      "found: 12\n",
      "0.833333333333 0.769230769231\n",
      "14\n",
      "\n",
      "correct: 14\n",
      "found: 15\n",
      "0.933333333333 0.875\n",
      "15\n",
      "\n",
      "correct: 7\n",
      "found: 8\n",
      "0.875 0.875\n",
      "16\n",
      "\n",
      "correct: 10\n",
      "found: 10\n",
      "1.0 0.833333333333\n",
      "17\n",
      "\n",
      "correct: 14\n",
      "found: 20\n",
      "0.7 0.7\n",
      "18\n",
      "\n",
      "correct: 11\n",
      "found: 13\n",
      "0.846153846154 0.846153846154\n",
      "19\n",
      "\n",
      "correct: 14\n",
      "found: 15\n",
      "0.933333333333 0.875\n",
      "20\n",
      "\n",
      "correct: 24\n",
      "found: 31\n",
      "0.774193548387 0.705882352941\n",
      "21\n",
      "\n",
      "correct: 13\n",
      "found: 16\n",
      "0.8125 0.8125\n",
      "22\n",
      "\n",
      "correct: 10\n",
      "found: 12\n",
      "0.833333333333 0.769230769231\n",
      "23\n",
      "\n",
      "correct: 4\n",
      "found: 5\n",
      "0.8 0.8\n",
      "24\n",
      "\n",
      "correct: 9\n",
      "found: 11\n",
      "0.818181818182 0.818181818182\n",
      "25\n",
      "\n",
      "correct: 8\n",
      "found: 8\n",
      "1.0 1.0\n",
      "26\n",
      "\n",
      "correct: 16\n",
      "found: 16\n",
      "1.0 0.941176470588\n",
      "27\n",
      "\n",
      "correct: 13\n",
      "found: 14\n",
      "0.928571428571 0.866666666667\n",
      "28\n",
      "\n",
      "correct: 8\n",
      "found: 12\n",
      "0.666666666667 0.666666666667\n",
      "29\n",
      "\n",
      "correct: 12\n",
      "found: 14\n",
      "0.857142857143 0.857142857143\n",
      "30\n",
      "\n",
      "correct: 8\n",
      "found: 8\n",
      "1.0 1.0\n",
      "31\n",
      "\n",
      "correct: 9\n",
      "found: 11\n",
      "0.818181818182 0.818181818182\n",
      "32\n",
      "\n",
      "correct: 8\n",
      "found: 9\n",
      "0.888888888889 0.888888888889\n",
      "33\n",
      "\n",
      "correct: 8\n",
      "found: 12\n",
      "0.666666666667 0.666666666667\n",
      "34\n",
      "\n",
      "correct: 8\n",
      "found: 10\n",
      "0.8 0.727272727273\n",
      "35\n",
      "\n",
      "correct: 10\n",
      "found: 11\n",
      "0.909090909091 0.909090909091\n",
      "36\n",
      "\n",
      "correct: 11\n",
      "found: 14\n",
      "0.785714285714 0.733333333333\n",
      "37\n",
      "\n",
      "correct: 5\n",
      "found: 6\n",
      "0.833333333333 0.714285714286\n",
      "38\n",
      "\n",
      "correct: 15\n",
      "found: 18\n",
      "0.833333333333 0.833333333333\n",
      "39\n",
      "\n",
      "correct: 10\n",
      "found: 14\n",
      "0.714285714286 0.625\n",
      "40\n",
      "\n",
      "correct: 18\n",
      "found: 19\n",
      "0.947368421053 0.947368421053\n",
      "41\n",
      "\n",
      "correct: 15\n",
      "found: 15\n",
      "1.0 1.0\n",
      "42\n",
      "\n",
      "correct: 14\n",
      "found: 14\n",
      "1.0 1.0\n",
      "43\n",
      "\n",
      "correct: 12\n",
      "found: 13\n",
      "0.923076923077 0.8\n",
      "44\n",
      "\n",
      "correct: 4\n",
      "found: 5\n",
      "0.8 0.666666666667\n",
      "45\n",
      "\n",
      "correct: 8\n",
      "found: 10\n",
      "0.8 0.727272727273\n",
      "46\n",
      "\n",
      "correct: 6\n",
      "found: 7\n",
      "0.857142857143 0.666666666667\n",
      "47\n",
      "\n",
      "correct: 21\n",
      "found: 23\n",
      "0.913043478261 0.913043478261\n",
      "48\n",
      "\n",
      "correct: 8\n",
      "found: 10\n",
      "0.8 0.727272727273\n",
      "49\n",
      "\n",
      "correct: 10\n",
      "found: 17\n",
      "0.588235294118 0.555555555556\n",
      "50\n",
      "\n",
      "MSNBC\n",
      "correct: 49\n",
      "found: 54\n",
      "0.907407407407 0.859649122807\n",
      "1\n",
      "\n",
      "correct: 19\n",
      "found: 24\n",
      "0.791666666667 0.791666666667\n",
      "2\n",
      "\n",
      "correct: 33\n",
      "found: 41\n",
      "0.80487804878 0.733333333333\n",
      "3\n",
      "\n",
      "correct: 14\n",
      "found: 36\n",
      "0.388888888889 0.388888888889\n",
      "4\n",
      "\n",
      "correct: 34\n",
      "found: 92\n",
      "0.369565217391 0.36170212766\n",
      "5\n",
      "\n",
      "correct: 8\n",
      "found: 12\n",
      "0.666666666667 0.666666666667\n",
      "6\n",
      "\n",
      "correct: 41\n",
      "found: 67\n",
      "0.611940298507 0.577464788732\n",
      "7\n",
      "\n",
      "correct: 11\n",
      "found: 15\n",
      "0.733333333333 0.733333333333\n",
      "8\n",
      "\n",
      "correct: 24\n",
      "found: 48\n",
      "0.5 0.489795918367\n",
      "9\n",
      "\n",
      "correct: 31\n",
      "found: 45\n",
      "0.688888888889 0.688888888889\n",
      "10\n",
      "\n",
      "correct: 11\n",
      "found: 11\n",
      "1.0 0.916666666667\n",
      "11\n",
      "\n",
      "correct: 30\n",
      "found: 33\n",
      "0.909090909091 0.882352941176\n",
      "12\n",
      "\n",
      "correct: 19\n",
      "found: 33\n",
      "0.575757575758 0.575757575758\n",
      "13\n",
      "\n",
      "correct: 34\n",
      "found: 36\n",
      "0.944444444444 0.918918918919\n",
      "14\n",
      "\n",
      "correct: 6\n",
      "found: 8\n",
      "0.75 0.666666666667\n",
      "15\n",
      "\n",
      "correct: 22\n",
      "found: 23\n",
      "0.95652173913 0.916666666667\n",
      "16\n",
      "\n",
      "correct: 11\n",
      "found: 13\n",
      "0.846153846154 0.6875\n",
      "17\n",
      "\n",
      "correct: 8\n",
      "found: 8\n",
      "1.0 0.8\n",
      "18\n",
      "\n",
      "correct: 11\n",
      "found: 23\n",
      "0.478260869565 0.478260869565\n",
      "19\n",
      "\n",
      "correct: 10\n",
      "found: 11\n",
      "0.909090909091 0.909090909091\n",
      "20\n",
      "\n",
      "{'kore': {'popular': [0.3656666666666667, 0.0, 0.36233333333333334, 0.0]}, 'MSNBC': {'popular': [0.7416277854882222, 0.0, 0.7021635474926601, 0.0]}, 'AQUAINT': {'popular': [0.856419352298902, 0.0, 0.811039413346857, 0.0]}}\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output \n",
    "\n",
    "\"\"\"\n",
    "This is for testing performance of different wikification methods.\n",
    "\"\"\"\n",
    "\n",
    "def getWiki5000Entities(annotationData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A helper method to get the entities of wiki5000 into the right form.\n",
    "    Args:\n",
    "        annotationData: The json data that has info that needs to be converted.\n",
    "    Return:\n",
    "        The entities in the usual format of [[something, entity],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    entities = []\n",
    "    for item in json.loads(annotationData):\n",
    "        entities.append([None, item['url'].replace(' ', '_')])\n",
    "    \n",
    "    return entities\n",
    "\n",
    "#pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki.5000.json')}]\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "\n",
    "methods = ['popular']\n",
    "\n",
    "performances = {}\n",
    "\n",
    "# for each dataset, run all methods\n",
    "for dataset in datasets:\n",
    "    performances[dataset['name']] = {}\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        \n",
    "    print dataset['name']\n",
    "    \n",
    "    # run each method on the data set\n",
    "    for mthd in methods:\n",
    "        # reset counters\n",
    "        totalPrecS = 0\n",
    "        totalPrecM = 0\n",
    "        totalRecS = 0\n",
    "        totalRecM = 0\n",
    "        totalLines = 0\n",
    "        \n",
    "        # each method tests all lines\n",
    "        for line in dataLines:\n",
    "            # different structure for wiki\n",
    "            if dataset['name'] == 'wiki5000':\n",
    "                resultS = None # no pre-split text\n",
    "                resultM = wikifyEval(line['opening_text'].encode('utf-8').strip(), False, mthd, True)\n",
    "                \n",
    "                # for unification of format for statistical testing\n",
    "                trueEntities = getWiki5000Entities(line['opening_annotation'])\n",
    "            else:\n",
    "                # original split string\n",
    "                #resultS = wikifyEval(line['text'], True, mthd, True)\n",
    "                # unsplit string\n",
    "                #resultM = wikifyEval((\" \".join(line['text'])).encode('utf-8').strip(), False, mthd, True)\n",
    "                \n",
    "                \n",
    "                #### With mentions pre given\n",
    "                \n",
    "                resultS = wikifyEvalMentionsGiven(line, method='popular', strict = False)\n",
    "                \n",
    "                #### end with mentions pre givin\n",
    "                \n",
    "                \n",
    "                trueEntities = line['mentions']\n",
    "                \n",
    "            ## get statistical results from true entities and results S and M\n",
    "            \n",
    "            # wiki5000 exception\n",
    "            if resultS <> None:\n",
    "                precS = precision(trueEntities, resultS[1]) # precision of pre-split\n",
    "            else:\n",
    "                precS = 0\n",
    "                \n",
    "            #precM = precision(trueEntities, resultM[1]) # precision of manual split\n",
    "            \n",
    "            # wiki5000 exception\n",
    "            if resultS <> None:\n",
    "                recS = recall(trueEntities, resultS[1]) # recall of pre-split\n",
    "            else:\n",
    "                recS = 0\n",
    "                \n",
    "            #recM = recall(trueEntities, resultM[1]) # recall of manual split\n",
    "            \n",
    "            #clear_output() # delete this after\n",
    "            #print str(precS) + ' ' + str(precM) + ' ' + str(recS) + ' ' + str(recM)\n",
    "            print str(precS) + ' ' + str(recS)\n",
    "            \n",
    "            # track results\n",
    "            totalPrecS += precS\n",
    "            #totalPrecM += precM\n",
    "            totalRecS += recS\n",
    "            #totalRecM += recM\n",
    "            totalLines += 1\n",
    "            \n",
    "            print str(totalLines) + '\\n'\n",
    "        \n",
    "        # record results for this method on this dataset\n",
    "        # [avg precision split, avg precision manual, avg recall split, avg recall manual]\n",
    "        performances[dataset['name']][mthd] = [totalPrecS/totalLines, totalPrecM/totalLines,\n",
    "                                              totalRecS/totalLines, totalRecM/totalLines,]\n",
    "            \n",
    "print performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test individual text on wikification.\n",
    "\"\"\"\n",
    "\n",
    "data = json.loads(\"\"\"{\"text\": [\"Three\", \"of\", \"the\", \"greatest\", \"guitarists\", \"started\", \"their\", \"career\", \"in\", \"a\", \"single\", \"band\", \":\", \"Clapton\", \",\", \"Beck\", \",\", \"and\", \"Page\", \".\"], \"mentions\": [[13, \"Eric_Clapton\"], [15, \"Jeff_Beck\"], [18, \"Jimmy_Page\"]]}\n",
    "\"\"\".decode('utf-8').strip())\n",
    "\n",
    "print str(data) + '\\n'\n",
    "\n",
    "print \" \".join(data['text']).encode('utf-8').strip()\n",
    "\n",
    "#results = wikifyEval(data['text'], True, 'popular', True)\n",
    "results = wikifyEval(\" \".join(data['text']).encode('utf-8').strip(), False, 'popular', True)\n",
    "print results[0]\n",
    "for result in results[1]:\n",
    "    print id2title(result[1])\n",
    "\n",
    "prec = precision(data['mentions'], results[1])\n",
    "rec = recall(data['mentions'], results[1])\n",
    "\n",
    "print '\\nprecision: ' + str(prec) + ', rec: ' + str(rec) + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test stuff from Armin.\n",
    "\"\"\"\n",
    "from wsd import *\n",
    "import time\n",
    "S=[\"Carlos\", \"met\", \"David\", \"and\" , \"Victoria\", \"in\", \"Madrid\"]\n",
    "M=[[0, \"Roberto_Carlos\"], [2, \"David_Beckham\"], [4, \"Victoria_Beckham\"], [6, \"Madrid\"]]\n",
    "\n",
    "S=[\"Carlos\", \"met\", \"David\", \"and\" , \"Victoria\", \"in\", \"Madrid\"]\n",
    "M=[[2, \"David_Beckham\"], [4, \"Victoria_Beckham\"], [6, \"Madrid\"]]\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "C = generate_candidates(S, M, max_t=20, enforce=False)\n",
    "print \"Candidates: \", C, \"\\n\"\n",
    "candslist_scores = keyentity_candidate_scores (C, 2, 'rvspagerank')\n",
    "print \"Key Scores: \", candslist_scores, \"\\n\"\n",
    "_, _, cands_score_list = entity_to_context_scores(C, 2, 'rvspagerank');\n",
    "print \"context Scores: \", cands_score_list,\"\\n\"\n",
    "\n",
    "ids, titles = disambiguate_driver(C, 2, 'rvspagerank', 2, 'keydisamb')\n",
    "print \"ids: \", ids\n",
    "print \"titles: \", titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is for testing if the wikification works.\n",
    "\"\"\"\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "phrase = 'Three of the greatest guitarists started their career in a single band : Clapton , Beck , and Page'\n",
    "print phrase + \"\\n\"\n",
    "\n",
    "anchors = wikify(phrase, False)\n",
    "for anchor in anchors:\n",
    "    print anchor['mention'] + '-->' + anchor['wikiTitle']\n",
    "    \n",
    "print\n",
    "\n",
    "anchors = wikify(phrase, True)\n",
    "for anchor in anchors:\n",
    "    print anchor['mention'] + '-->' + anchor['wikiTitle']\n",
    "    \n",
    "print\n",
    "    \n",
    "newText = \"\"\n",
    "\n",
    "anchors = sorted(anchors, key=itemgetter('start')) # make sure anchors are sorted\n",
    "anchorIndex = 0 # keep track of current anchor added\n",
    "i = 0 \n",
    "while i < len(phrase):\n",
    "    if anchorIndex < len(anchors) and i == anchors[anchorIndex]['start']:\n",
    "        anchor = anchors[anchorIndex]\n",
    "        newText += (\"<a href=\\\"https://en.wikipedia.org/wiki/\" + anchor['wikiTitle']\n",
    "                   + \"\\\" target=\\\"_blank\\\">\" + anchor['mention'] + \"</a>\")\n",
    "        i = anchors[anchorIndex]['end']\n",
    "        anchorIndex += 1\n",
    "    else:\n",
    "        newText += phrase[i]\n",
    "        i += 1\n",
    "    \n",
    "display(HTML(newText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ideas:\n",
    "    -anchor frequency adjuster\n",
    "    -use similarity with other anchors\n",
    "\n",
    "Sample Querries:\n",
    "    'I walked down to the park and found a duck and a pebble'\n",
    "    'I walked into an electronic store and bought a pebble'\n",
    "    'I walked down to the park and found a duck studying quantum mechanics'\n",
    "    'I walked down to the park and found a duck studying quantum mechanical systems'\n",
    "    'I met David in Spain'\n",
    "    'An entomologist spots what might be a rare subspecies of beetle, due to the pattern on its back'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(q)\n",
    "\n",
    "minAcceptedFrequency = 20 # do not use entities with less frequencies than this\n",
    "minWordLength = 3 # for efficiency do not even look at words with less size than this\n",
    "\n",
    "potentialMentionsAmount = len(queryResult) # the total amount of potential mentions including overlaps\n",
    "\n",
    "# records appear to be sorted by start index\n",
    "\n",
    "# get each word and the corresponding variations of that potential anchor\n",
    "anchorsPossibilities = getAnchorPossibilities(queryResult)\n",
    "#print anchorsPossibilities\n",
    "\n",
    "# get the most popular concept for the variations of each potential anchor\n",
    "anchorPossibilityFrequencies = []\n",
    "for i in range(len(anchorsPossibilities)):\n",
    "    anchorPossibilityFrequencies.append((\n",
    "        anchorsPossibilities[i][0], anchorsPossibilities[i][1], \n",
    "        anchorsPossibilities[i][2], getMostFrequentConcept(anchorsPossibilities[i][3])))\n",
    "    \n",
    "#print anchorPossibilityFrequencies\n",
    "\n",
    "prunedPotentialAnchors = [] # store all with high enough frequency here\n",
    "# ditch all with frequency under threshold\n",
    "for potentialAnchor in anchorPossibilityFrequencies:\n",
    "    if(potentialAnchor[3][2] >= minAcceptedFrequency):\n",
    "        prunedPotentialAnchors.append(potentialAnchor)\n",
    "        \n",
    "#print prunedPotentialAnchors\n",
    "\n",
    "# display final results\n",
    "for anchor in prunedPotentialAnchors:\n",
    "    print(anchor[2] + \"-->\" + \"https://en.wikipedia.org/wiki/\" \n",
    "          + id2title(anchor[3][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getAnchorPossibilities(data):\n",
    "    \"\"\"\n",
    "    Description: \n",
    "        Extracts all potential anchors from each record in the data.\n",
    "    Args:\n",
    "        data: The data to get potential anchors from.\n",
    "    Return: \n",
    "        An array of possible anchors for the source word in the following format:\n",
    "        (start index, end index, source word, possible anchors)\n",
    "    \"\"\"\n",
    "    \n",
    "    # array of (start index, end index, source word, possible anchors)\n",
    "    anchorsPossibilities = [] \n",
    "    for record in data:\n",
    "        # don't take words below the threshold\n",
    "        if record[3] - record[1] >= minWordLength:\n",
    "            anchorsPossibilities.append((record[1], record[3], \n",
    "                                        q[record[1]:record[3]], record[5]))\n",
    "            \n",
    "    return anchorsPossibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMostFrequentConcept(anchorSearchRepresentations):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Finds the anchor-search representation (asr) with the most frequency in the list \n",
    "        of anchorSeachRepresentations.\n",
    "    Args:\n",
    "        anchorSearchRepresentations: a list of possible representations of an anchor\n",
    "        for searching purposes.\n",
    "    Return:\n",
    "        The anchor-search representation that gives the concept with the most\n",
    "        frequencies along with the concept and its frequency.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The inputted asrs along with the frequency of thier most popular concept\n",
    "    asrFrequencies = []\n",
    "    for asr in anchorSearchRepresentations:\n",
    "        # gets the most frequent concept from the current asr\n",
    "        mostFrequent = sorted(anchor2concept(asr), key = itemgetter(1), reverse = True)[0]\n",
    "        # (asr, concept, concept frequency)\n",
    "        asrFrequencies.append((asr, mostFrequent[0], mostFrequent[1]))\n",
    "        \n",
    "    # get and return the asr with the highest freqency\n",
    "    return sorted(asrFrequencies, key = itemgetter(2), reverse = True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -X POST \\\n",
    "  'http://localhost:8983/solr/enwikianchors20160305/tag?overlaps=NO_SUB&tagsLimit=5000&fl=id&wt=json&indent=on' \\\n",
    "  -H 'Content-Type:text/plain' -d 'I met David in Spain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" ORIGINAL\n",
    "Evaluation of wikifications\n",
    "\"\"\"\n",
    "\n",
    "from wikipedia import *\n",
    "from operator import itemgetter\n",
    "import requests\n",
    "import json\n",
    "from __future__ import division\n",
    "\n",
    "MIN_MENTION_LENGTH = 3 # mentions must be at least this long\n",
    "MIN_FREQUENCY = 20 # anchor with frequency below is ignored\n",
    "     \n",
    "def splitWords(phrase):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes in a phrase and splits it into the different word / possible entities.\n",
    "    Args:\n",
    "        phrase: The text to be split.\n",
    "    Return:\n",
    "        The text split it into the different word / possible entities.\n",
    "    \"\"\"\n",
    "    \n",
    "    addr = 'http://localhost:8983/solr/enwikianchors20160305/tag'\n",
    "    params={'overlaps':'ALL', 'tagsLimit':'5000', 'fl':'id','wt':'json','indent':'on'}\n",
    "    r = requests.post(addr, params=params, data=phrase)\n",
    "    textData = r.json()['tags']\n",
    "    \n",
    "    splitPhrase = []\n",
    "    \n",
    "    for item in textData:\n",
    "        splitPhrase.append(phrase[item[1]:item[3]])\n",
    "    \n",
    "    return splitPhrase\n",
    "\n",
    "def precision(truthSet, mySet):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the precision of mySet against the truthSet.\n",
    "    Args:\n",
    "        truthSet: The 'right' answers for what the entities are.\n",
    "        mySet: My code's output for what it thinks the right entities are.\n",
    "    Return:\n",
    "        The precision: (# of correct entities)/(# of found entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numFound = len(mySet)\n",
    "    numCorrect = 0 # incremented in for loop\n",
    "    \n",
    "    # find all correct\n",
    "    for entity1 in mySet:\n",
    "        for entity2 in truthSet:\n",
    "            if entity1[1] == title2id(entity2[1]):\n",
    "                numCorrect += 1\n",
    "                break\n",
    "                \n",
    "    if numFound == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numFound)\n",
    "\n",
    "def recall(truthSet, mySet):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the recall of mySet against the truthSet.\n",
    "    Args:\n",
    "        truthSet: The 'right' answers for what the entities are.\n",
    "        mySet: My code's output for what it thinks the right entities are.\n",
    "    Return:\n",
    "        The recall: (# of correct entities)/(# of actual entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numActual = len(truthSet)\n",
    "    numCorrect = 0 # incremented in for loop\n",
    "    \n",
    "    # find all correct\n",
    "    for entity1 in mySet:\n",
    "        for entity2 in truthSet:\n",
    "            if entity1[1] == title2id(entity2[1]):\n",
    "                numCorrect += 1\n",
    "                break\n",
    "                \n",
    "    if numActual == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numActual)\n",
    "    \n",
    "def wikifyPopular(phrase):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes in a phrase split into different words or anchors and returns the most likely entities\n",
    "        in the phrase based on popularity method.\n",
    "    Args:\n",
    "        phrase: The original phrase in split form: [w1, w2, ...]\n",
    "    Return:\n",
    "        The anchors/word ids along with the corresponding entity id. Of the form: \n",
    "        [[wid, entityId, entityFreq],...]\n",
    "    \"\"\"\n",
    "    \n",
    "    proposedEntities = []\n",
    "\n",
    "    i = 0 # counter to see what word this is on\n",
    "    for word in phrase:\n",
    "        results = sorted(anchor2concept(word), key = itemgetter(1), \n",
    "                          reverse = True)\n",
    "        if len(results) > 0: # some results\n",
    "            entity = results[0]\n",
    "            proposedEntities.append([i, entity[0], entity[1]])\n",
    "            i += 1\n",
    "            \n",
    "    return proposedEntities\n",
    "\n",
    "def wikifyEval(phrase, preSplitWords, method='popular', strict = True):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes the phrase string, and wikifies it for evaluation purposes using the desired method.\n",
    "    Args:\n",
    "        phrase: The string to wikify. Either as just the original string to be modified, or in the \n",
    "            form of: [[w1,w2,...], [[wid,entityId,freq],...], with the second part blank and filled in by \n",
    "            this method.\n",
    "        preSplitWords: Whether to use pre-split words from the dataset, or use our own method of\n",
    "            splitting words.\n",
    "        method: The method used to wikify.\n",
    "        strict: Whether to use such rules as minimum metion length, or minimum frequency of concept.\n",
    "    Return:\n",
    "        The original split text and the anchors along with their best matched concept from wikipedia.\n",
    "        Of the form: [[w1,w2,...], [[wid,entityId,freq],...]]\n",
    "    \"\"\"\n",
    "    \n",
    "    # words are not in pre-split form\n",
    "    if not(preSplitWords):\n",
    "        phrase = splitWords(phrase) # modify phrase into split form\n",
    "    \n",
    "    #wikified has form: [[w1,w2,...], [[wid,entityId,freq],...], with the second part blank and filled in by \n",
    "    #one of the methods below.\n",
    "    wikified = [phrase]\n",
    "    \n",
    "    if method == 'popular':\n",
    "        wikified.append(wikifyPopular(phrase))\n",
    "    \n",
    "    # small clean-up to do\n",
    "    if strict:\n",
    "        wikified[1] = [item for item in wikified[1]\n",
    "                    if not (item[2] < MIN_FREQUENCY or len(phrase[item[0]]) < MIN_MENTION_LENGTH)]\n",
    "    \n",
    "    # remove duplicates\n",
    "    idsHad = [] # a list of entities to check for duplicates\n",
    "    newWikified1 = [] # to replace old wikified[1]\n",
    "    for item in wikified[1]:\n",
    "        if item[1] not in idsHad:\n",
    "            newWikified1.append(item)\n",
    "            idsHad.append(item[1])\n",
    "    wikified[1] = newWikified1\n",
    "        \n",
    "    return wikified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output \n",
    "\n",
    "\"\"\" ORGINAL\n",
    "This is for testing performance of different wikification methods.\n",
    "\"\"\"\n",
    "\n",
    "def getWiki5000Entities(annotationData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A helper method to get the entities of wiki5000 into the right form.\n",
    "    Args:\n",
    "        annotationData: The json data that has info that needs to be converted.\n",
    "    Return:\n",
    "        The entities in the usual format of [[something, entity],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    entities = []\n",
    "    for item in json.loads(annotationData):\n",
    "        entities.append([None, item['url'].replace(' ', '_')])\n",
    "    \n",
    "    return entities\n",
    "\n",
    "#pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki.5000.json')}]\n",
    "\n",
    "methods = ['popular']\n",
    "\n",
    "performances = {}\n",
    "\n",
    "# for each dataset, run all methods\n",
    "for dataset in datasets:\n",
    "    performances[dataset['name']] = {}\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        \n",
    "    print dataset['name']\n",
    "    \n",
    "    # run each method on the data set\n",
    "    for mthd in methods:\n",
    "        # reset counters\n",
    "        totalPrecS = 0\n",
    "        totalPrecM = 0\n",
    "        totalRecS = 0\n",
    "        totalRecM = 0\n",
    "        totalLines = 0\n",
    "        \n",
    "        # each method tests all lines\n",
    "        for line in dataLines:\n",
    "            # different structure for wiki\n",
    "            if dataset['name'] == 'wiki5000':\n",
    "                resultS = None # no pre-split text\n",
    "                resultM = wikifyEval(line['opening_text'].encode('utf-8').strip(), False, mthd)\n",
    "                \n",
    "                # for unification of format for statistical testing\n",
    "                trueEntities = getWiki5000Entities(line['opening_annotation'])\n",
    "            else:\n",
    "                # original split string\n",
    "                resultS = wikifyEval(line['text'], True, mthd)\n",
    "                # unsplit string\n",
    "                resultM = wikifyEval((\" \".join(line['text'])).encode('utf-8').strip(), False, mthd)\n",
    "                \n",
    "                trueEntities = line['mentions']\n",
    "                \n",
    "            ## get statistical results from true entities and results S and M\n",
    "            \n",
    "            # wiki5000 exception\n",
    "            if resultS <> None:\n",
    "                precS = precision(trueEntities, resultS[1]) # precision of pre-split\n",
    "            else:\n",
    "                precS = 0\n",
    "                \n",
    "            precM = precision(trueEntities, resultM[1]) # precision of manual split\n",
    "            \n",
    "            # wiki5000 exception\n",
    "            if resultS <> None:\n",
    "                recS = recall(trueEntities, resultS[1]) # recall of pre-split\n",
    "            else:\n",
    "                recS = 0\n",
    "                \n",
    "            recM = recall(trueEntities, resultM[1]) # recall of manual split\n",
    "            \n",
    "            clear_output() # delete this after\n",
    "            print str(precS) + ' ' + str(precM) + ' ' + str(recS) + ' ' + str(recM)\n",
    "            \n",
    "            # track results\n",
    "            totalPrecS += precS\n",
    "            totalPrecM += precM\n",
    "            totalRecS += recS\n",
    "            totalRecM += recM\n",
    "            totalLines += 1\n",
    "            \n",
    "            print str(totalLines) + '\\n'\n",
    "        \n",
    "        # record results for this method on this dataset\n",
    "        # [avg precision split, avg precision manual, avg recall split, avg recall manual]\n",
    "        performances[dataset['name']][mthd] = [totalPrecS/totalLines, totalPrecM/totalLines,\n",
    "                                              totalRecS/totalLines, totalRecM/totalLines,]\n",
    "            \n",
    "print performances"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
